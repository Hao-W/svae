{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_marginals(num_dims, CUDA, device=None, optimized=False):\n",
    "    target = {'loc': torch.zeros(num_dims), \n",
    "             'log_scale': torch.zeros(num_dims)}\n",
    "    proposal = {'loc': 8.0 * torch.ones(num_dims),\n",
    "           'log_scale': torch.log(2.0 * torch.ones(num_dims))}    \n",
    "    if CUDA:\n",
    "        with torch.cuda.device(device):\n",
    "            for key in target.keys():\n",
    "                target[key] = target[key].cuda()\n",
    "            for key in proposal.keys():\n",
    "                proposal[key] = proposal[key].cuda()         \n",
    "    if optimized:\n",
    "        for key in target.keys():\n",
    "            target[key] = torch.nn.Parameter(target[key])\n",
    "        for key in proposal.keys():\n",
    "            proposal[key] = torch.nn.Parameter(proposal[key])\n",
    "            \n",
    "    return target, proposal\n",
    "\n",
    "from kernel_rep  import Kernel\n",
    "def init_kernels(num_hiddens, CUDA, device=None):\n",
    "    fk = Kernel(num_hiddens=num_hiddens)\n",
    "    bk = Kernel(num_hiddens=num_hiddens)\n",
    "    if CUDA:\n",
    "        with torch.cuda.device(device):\n",
    "            fk.cuda()\n",
    "            bk.cuda()\n",
    "    return fk, bk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.normal import Normal\n",
    "def loss_1(target, proposal, fk, bk, num_samples):\n",
    "    \"\"\"\n",
    "    this loss function employs the following gradient estimators:\n",
    "    For phi\n",
    "    - \\nabla_\\phi KL (\\pi_2(x_2) r_\\theta(x_1 | x2) || \\pi_1(x1) q_\\phi(x_2 | x_1))\n",
    "    For theta\n",
    "    - \\nabla_\\theta KL (\\pi_1(x1) q_\\phi(x_2 | x_1) || \\pi_2(x_2) r_\\theta(x_1 | x2))\n",
    "    so that both of them do not require reparameterization or score function\n",
    "    \"\"\"\n",
    "    p = Normal(target['loc'], target['log_scale'].exp())\n",
    "    q = Normal(proposal['loc'], proposal['log_scale'].exp())\n",
    "    x1 = q.sample((num_samples,))\n",
    "    log_q = q.log_prob(x1)\n",
    "    x2, log_f, mu_x2, sigma_x2 = fk.forward(x=x1, sampled=True)\n",
    "    log_p = p.log_prob(x2)\n",
    "    log_b, mu_x1, sigma_x1 = bk.forward(x=x2, sampled=False, samples_old=x1)\n",
    "    log_w = (log_p + log_b - log_q - log_f).sum(-1).detach()\n",
    "    w = F.softmax(log_w, 0)\n",
    "    ess_f = 1.0 / (w**2).sum(0)\n",
    "    loss_f = (w * ( - log_f.sum(-1))).sum(0)\n",
    "    loss_b = - log_b.mean()\n",
    "    ex_kl = log_w.mean()\n",
    "    in_kl = (w * log_w).sum()\n",
    "    return loss_f, loss_b, ess_f, ex_kl, in_kl\n",
    "\n",
    "def loss_2(target, proposal, fk, bk, num_samples):\n",
    "    \"\"\"\n",
    "    this loss function only targets the inclusive KL divergence:\n",
    "    - \\nabla_\\phi KL (\\pi_2(x_2) r_\\theta(x_1 | x2) || \\pi_1(x1) q_\\phi(x_2 | x_1))\n",
    "    \"\"\"\n",
    "    p = Normal(target['loc'], target['log_scale'].exp())\n",
    "    q = Normal(proposal['loc'], proposal['log_scale'].exp())\n",
    "    x1 = q.sample((num_samples,))\n",
    "    log_q = q.log_prob(x1)\n",
    "    x2, log_f, mu_x2, sigma_x2 = fk.forward(x=x1, sampled=True)\n",
    "    log_p = p.log_prob(x2)\n",
    "    log_b, mu_x1, sigma_x1 = bk.forward(x=x2, sampled=False, samples_old=x1)\n",
    "    log_w = (log_p + log_b - log_q - log_f).sum(-1).detach()\n",
    "    w = F.softmax(log_w, 0)\n",
    "    ess_f = 1.0 / (w**2).sum(0)\n",
    "    loss_f = (w * ( - log_f.sum(-1))).sum(0)\n",
    "    assert log_b.shape == (num_samples, 1), \"ERROR\"\n",
    "    loss_b = (w * (log_w) * log_b.squeeze(-1)).sum()\n",
    "    ex_kl = log_w.mean()\n",
    "    in_kl = (w * log_w).sum()\n",
    "    return loss_f, loss_b, ess_f, ex_kl, in_kl\n",
    "\n",
    "\n",
    "def loss_3(target, proposal, fk, bk, num_samples):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    p = Normal(target['loc'], target['log_scale'].exp())\n",
    "    q = Normal(proposal['loc'], proposal['log_scale'].exp())\n",
    "    x1 = q.sample((num_samples,))\n",
    "    log_q = q.log_prob(x1)\n",
    "    x2, log_f, mu_x2, sigma_x2 = fk.forward(x=x1, sampled=True)\n",
    "    log_p = p.log_prob(x2)\n",
    "    log_b, mu_x1, sigma_x1 = bk.forward(x=x2, sampled=False, samples_old=x1)\n",
    "    log_w = (log_p + log_b - log_q - log_f).sum(-1).detach()\n",
    "    w = F.softmax(log_w, 0)\n",
    "    ess_f = 1.0 / (w**2).sum(0)\n",
    "    loss_f = (w * ( - log_f.sum(-1))).sum(0)\n",
    "    assert log_b.shape == (num_samples, 1), \"ERROR\"\n",
    "    log_w_t = (log_p + log_b - log_q - log_f.detach()).sum(-1)\n",
    "    loss_b = (log_w_t.exp() * (log_w_t)).mean()\n",
    "    ex_kl = log_w.mean()\n",
    "    in_kl = (w * log_w).sum()\n",
    "    return loss_f, loss_b, ess_f, ex_kl, in_kl\n",
    "\n",
    "def loss_4(target, proposal, fk, bk, num_samples):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    p = Normal(target['loc'], target['log_scale'].exp())\n",
    "    q = Normal(proposal['loc'], proposal['log_scale'].exp())\n",
    "    x1 = q.sample((num_samples,))\n",
    "    log_q = q.log_prob(x1)\n",
    "    x2, log_f, mu_x2, sigma_x2 = fk.forward(x=x1, sampled=True)\n",
    "    log_p = p.log_prob(x2)\n",
    "    log_b, mu_x1, sigma_x1 = bk.forward(x=x2, sampled=False, samples_old=x1)\n",
    "    log_w = (log_p + log_b - log_q - log_f).sum(-1).detach()\n",
    "    w = F.softmax(log_w, 0)\n",
    "    ess_f = 1.0 / (w**2).sum(0)\n",
    "    loss_f = (w * ( - log_f.sum(-1))).sum(0)\n",
    "    loss_b = (w * (- log_b.sum(-1))).sum(0)\n",
    "    ex_kl = log_w.mean()\n",
    "    in_kl = (w * log_w).sum()\n",
    "    return loss_f, loss_b, ess_f, ex_kl, in_kl\n",
    "\n",
    "def loss_5(target, proposal, fk, bk, num_samples):\n",
    "    \"\"\"\n",
    "    Doubly Reparameterized Gradient Estimator\n",
    "    \"\"\"\n",
    "    p = Normal(target['loc'], target['log_scale'].exp())\n",
    "    q = Normal(proposal['loc'], proposal['log_scale'].exp())\n",
    "    x1 = q.sample((num_samples,))\n",
    "    log_q = q.log_prob(x1)\n",
    "    x2, log_f, mu_x2, sigma_x2 = fk.forward(x=x1, sampled=True)\n",
    "    log_p = p.log_prob(x2)\n",
    "    log_b, mu_x1, sigma_x1 = bk.forward(x=x2, sampled=False, samples_old=x1)\n",
    "    log_w = (log_p + log_b - log_q - log_f).sum(-1).detach()\n",
    "    w = F.softmax(log_w, 0)\n",
    "    ess_f = 1.0 / (w**2).sum(0)\n",
    "    loss_f = (w * ( - log_f.sum(-1))).sum(0)\n",
    "    loss_b = (w * (- log_b.sum(-1))).sum(0)\n",
    "    ex_kl = log_w.mean()\n",
    "    in_kl = (w * log_w).sum()\n",
    "    return loss_f, loss_b, ess_f, ex_kl, in_kl\n",
    "\n",
    "import pandas as pd\n",
    "def train(grad_steps, loss, optimizer, target, proposal, fk, bk, num_samples):\n",
    "    time_start = time.time()\n",
    "    output = pd.DataFrame(index=range(grad_steps), columns=['loss_f', 'loss_b', 'ess_f', 'ex_kl', 'in_kl'])\n",
    "    for n in range(grad_steps):\n",
    "        loss_f, loss_b, ess_f, ex_kl, in_kl = loss(target=target, proposal=proposal, fk=fk, bk=bk, num_samples=num_samples)\n",
    "        loss_f.backward(retain_graph=True)\n",
    "        loss_b.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        output['loss_f'][n] = loss_f.detach().cpu().data.numpy()\n",
    "        output['loss_b'][n] = loss_b.detach().cpu().data.numpy()\n",
    "        output['ess_f'][n] = ess_f.cpu().data.numpy()\n",
    "        output['ex_kl'][n] = ex_kl.cpu().data.numpy()\n",
    "        output['in_kl'][n] = in_kl.cpu().data.numpy()  \n",
    "    time_end = time.time()\n",
    "    print('%d gradient steps in %ds' % (grad_steps, time_end - time_start))\n",
    "    return output\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def plot_convergence(output, obj, num_samples, grad_steps):\n",
    "    fs = 8\n",
    "    fig = plt.figure(figsize=(fs*2.5,fs))\n",
    "    ax1 = fig.add_subplot(1, 2, 1)\n",
    "    ax1.plot(output['ex_kl'], label=r'$\\mathrm{KL} (\\pi_1(x_1) \\: q_\\phi (x_2 | x_1) \\: || \\: \\pi_2(x_2) \\: r_\\theta(x_1 | x_2))$')\n",
    "    ax1.plot(output['in_kl'], label=r'$\\mathrm{KL} (\\pi_2(x_2) \\: r_\\theta(x_1 | x_2) \\: || \\: \\pi_1(x_1) \\: q_\\phi (x_2 | x_1))$')\n",
    "    ax1.plot(np.zeros(grad_steps), label='zero constant')\n",
    "    ax1.set_ylim(-10, 10)\n",
    "    ax1.set_xlabel('Gradient Steps')\n",
    "    ax1.legend(fontsize=16)\n",
    "\n",
    "    ax2 = fig.add_subplot(1, 2, 2)\n",
    "    ax2.plot(output['ess_f'], label='ESS with %d samples' % num_samples)\n",
    "    ax2.legend(fontsize=16)\n",
    "    ax2.set_xlabel('Gradient Steps')\n",
    "    plt.savefig('results-loss%d-%dsamples-%dsteps.png' % (obj, num_samples, grad_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_plot(obj, num_dims, num_hiddens, num_samples, grad_steps, CUDA, device):\n",
    "    print(\"Training with Loss %d, Marginal Dim=%d, Sample Size=%d, Gradient Steps=%d\" % (obj, num_dims, num_samples, grad_steps))\n",
    "    target, proposal = init_marginals(num_dims=num_dims, CUDA=CUDA, device=device, optimized=False)\n",
    "    fk, bk = init_kernels(num_hiddens=num_hiddens, CUDA=CUDA, device=device)\n",
    "    optimizer = torch.optim.Adam(list(fk.parameters())+list(bk.parameters()), lr=1e-3, betas=(0.9, 0.99))\n",
    "    \n",
    "    if obj == 1:\n",
    "        output = train(grad_steps=grad_steps, \n",
    "                       loss=loss_1, \n",
    "                       optimizer=optimizer, \n",
    "                       target=target, \n",
    "                       proposal=proposal, \n",
    "                       fk=fk,\n",
    "                       bk=bk,\n",
    "                       num_samples=num_samples)\n",
    "    elif obj == 2:\n",
    "        output = train(grad_steps=grad_steps, \n",
    "                       loss=loss_2, \n",
    "                       optimizer=optimizer, \n",
    "                       target=target, \n",
    "                       proposal=proposal,\n",
    "                       fk=fk,\n",
    "                       bk=bk,\n",
    "                       num_samples=num_samples)\n",
    "    elif obj == 3:\n",
    "        output = train(grad_steps=grad_steps, \n",
    "                       loss=loss_3, \n",
    "                       optimizer=optimizer, \n",
    "                       target=target, \n",
    "                       proposal=proposal, \n",
    "                       fk=fk,\n",
    "                       bk=bk,\n",
    "                       num_samples=num_samples)\n",
    "    elif obj == 4:\n",
    "        output = train(grad_steps=grad_steps, \n",
    "                   loss=loss_4, \n",
    "                   optimizer=optimizer, \n",
    "                   target=target, \n",
    "                   proposal=proposal, \n",
    "                   fk=fk,\n",
    "                   bk=bk,\n",
    "                   num_samples=num_samples)      \n",
    "    plot_convergence(output=output, obj=obj, num_samples=num_samples, grad_steps=grad_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_DIMS = 1\n",
    "NUM_HIDDENS = 8\n",
    "GRAD_STEPS = 2000\n",
    "NUM_SAMPLES = 100\n",
    "\n",
    "train_and_plot(obj=1, \n",
    "               num_dims=NUM_DIMS, \n",
    "               num_hiddens=NUM_HIDDENS, \n",
    "               num_samples=NUM_SAMPLES, \n",
    "               grad_steps=GRAD_STEPS, \n",
    "               CUDA=CUDA, \n",
    "               device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_and_plot(obj=2, \n",
    "               num_dims=NUM_DIMS, \n",
    "               num_hiddens=NUM_HIDDENS, \n",
    "               num_samples=NUM_SAMPLES, \n",
    "               grad_steps=4000, \n",
    "               CUDA=CUDA, \n",
    "               device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
