{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal as mvn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bi_Gaussian():\n",
    "    def __init__(self, mu1, mu2, sigma1, sigma2, rho, CUDA, device):\n",
    "        super().__init__()\n",
    "        self.mu1 = mu1\n",
    "        self.mu2 = mu2\n",
    "        self.sigma1 = sigma1\n",
    "        self.sigma2 = sigma2\n",
    "        self.rho = rho\n",
    "        if CUDA:\n",
    "            with torch.cuda.device(device):\n",
    "                self.mu1 = self.mu1.cuda()\n",
    "                self.mu2 = self.mu2.cuda()\n",
    "                self.sigma1 = self.sigma1.cuda()\n",
    "                self.sigma2 = self.sigma2.cuda()\n",
    "                self.rho = self.rho.cuda()\n",
    "\n",
    "    def conditional(self, x, cond):\n",
    "        \"\"\"\n",
    "        return parameters of conditional distribution X1 | X2 or X2 | X1\n",
    "        based on the cond flag\n",
    "        \"\"\"\n",
    "        if cond == 'x2': ## X1 | X2\n",
    "            cond_mu = self.mu1 + (self.sigma1 / self.sigma2) * (x - self.mu2) * self.rho\n",
    "            cond_sigma = ((1 - self.rho ** 2) * (self.sigma1**2)).sqrt()\n",
    "        else: ## X2 | X1\n",
    "            cond_mu = self.mu2 + (self.sigma2 / self.sigma1) * (x - self.mu1) * self.rho\n",
    "            cond_sigma = ((1 - self.rho ** 2) * (self.sigma2**2)).sqrt()\n",
    "        return cond_mu, cond_sigma\n",
    "\n",
    "    def log_pdf_gamma(self, x, mu, sigma):\n",
    "        \"\"\"\n",
    "        return log unormalized density of a univariate Gaussian\n",
    "        \"\"\"\n",
    "        return Normal(mu, sigma).log_prob(x)\n",
    "        # return ((x - mu)**2) / (-2 * sigma**2)\n",
    "\n",
    "    def Joint(self):\n",
    "        \"\"\"\n",
    "        return the paramters for the bivariate Gaussian given the indiviual parameters\n",
    "        \"\"\"\n",
    "        Mu = torch.cat((self.mu1.unsqueeze(-1), self.mu2.unsqueeze(-1)), -1)\n",
    "        cov = self.rho * self.sigma1 * self.sigma2\n",
    "        part1 = torch.cat((self.sigma1.unsqueeze(-1)**2, cov.unsqueeze(-1)), -1)\n",
    "        part2 = torch.cat((cov.unsqueeze(-1), self.sigma2.unsqueeze(-1)**2), -1)\n",
    "        Sigma = torch.cat((part1.unsqueeze(-1), part2.unsqueeze(-1)), -1)\n",
    "        return Mu, Sigma\n",
    "\n",
    "    def Marginal(self, x, name):\n",
    "        if name == 'x1':\n",
    "            return Normal(self.mu1, self.sigma1).log_prob(x)\n",
    "        else:\n",
    "            return Normal(self.mu2, self.sigma2).log_prob(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a target bivariate Gaussian for unit test\n",
    "mu1 = torch.ones(1) * 5.0\n",
    "mu2 = torch.ones(1) * 8.0\n",
    "sigma1 = torch.ones(1) * 1.0\n",
    "sigma2 = torch.ones(1) * 2.5\n",
    "rho = torch.ones(1) * 0.6\n",
    "bg = Bi_Gaussian(mu1, mu2, sigma1, sigma2, rho, CUDA=True, device=DEVICE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
