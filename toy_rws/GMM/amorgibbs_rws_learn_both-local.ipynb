{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probtorch: 0.0+5a2c637 torch: 1.0.0 cuda: True\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from plots import *\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal as mvn\n",
    "from torch.distributions.one_hot_categorical import OneHotCategorical as cat\n",
    "from torch.distributions.gamma import Gamma\n",
    "from torch import logsumexp\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "sys.path.append('/home/hao/Research/probtorch/')\n",
    "import probtorch\n",
    "print('probtorch:', probtorch.__version__, \n",
    "      'torch:', torch.__version__, \n",
    "      'cuda:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "K = 3\n",
    "D = 2\n",
    "\n",
    "## Model Parameters\n",
    "NUM_SAMPLES = 10\n",
    "NUM_HIDDEN = 64\n",
    "STEPS = 10\n",
    "NUM_STATS = K+D*K+D*K\n",
    "NUM_LATENTS = D * K\n",
    "NUM_OBS_GLOBAL = D + K\n",
    "NUM_OBS_LOCAL = D + K*D + K*D\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 2000\n",
    "LEARNING_RATE = 1e-3\n",
    "CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = torch.from_numpy(np.load('gmm_dataset/sequences.npy')).float()\n",
    "# Zs = torch.from_numpy(np.load('gmm_dataset/states.npy')).float()\n",
    "# mus_true = torch.from_numpy(np.load('gmm_dataset/means.npy')).float()\n",
    "# covs = torch.from_numpy(np.load('gmm_dataset2/covariances.npy')).float()\n",
    "Pi = torch.from_numpy(np.load('gmm_dataset/init.npy')).float()\n",
    "num_seqs = Xs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_global(nn.Module):\n",
    "    def __init__(self, num_obs=NUM_OBS_GLOBAL,\n",
    "                       num_stats=NUM_STATS,\n",
    "                       num_hidden=NUM_HIDDEN,\n",
    "                       num_latents=NUM_LATENTS):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.enc_stats = nn.Sequential(\n",
    "            nn.Linear(num_obs, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_stats))\n",
    "        self.enc_hidden = nn.Sequential(\n",
    "            nn.Linear(num_stats, num_hidden),\n",
    "            nn.ReLU())\n",
    "        self.sigmas_log_alpha = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        self.sigmas_log_beta = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        \n",
    "        self.enc_hidden2 = nn.Sequential(\n",
    "            nn.Linear(num_stats, num_hidden),\n",
    "            nn.ReLU())\n",
    "        self.mus_mean = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        self.mus_log_std = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        \n",
    "    def forward(self, obs, K, D, num_samples, batch_size):\n",
    "        stats = self.enc_stats(obs).view(batch_size, N, -1).sum(1)\n",
    "        hidden = self.enc_hidden(stats)\n",
    "        alpha = torch.exp(self.sigmas_log_alpha(hidden)).view(-1, K, D) ## B * K * D\n",
    "        beta = torch.exp(self.sigmas_log_beta(hidden)).view(-1, K, D) ## B * K * D\n",
    "        precisions = Gamma(alpha, beta).sample((num_samples,)) ## S * B * K * D\n",
    "        \n",
    "        hidden2 = self.enc_hidden2(stats)                 \n",
    "        mus_mean = self.mus_mean(hidden2).view(-1, K, D)\n",
    "        mus_sigma = torch.exp(self.mus_log_std(hidden2).view(-1, K, D))\n",
    "        mus = Normal(mus_mean, mus_sigma).sample((num_samples,))  \n",
    "        return alpha, beta, precisions, mus_mean, mus_sigma, mus\n",
    "    \n",
    "class Encoder_local(nn.Module):\n",
    "    def __init__(self, num_obs=NUM_OBS_LOCAL,\n",
    "                       num_hidden=NUM_HIDDEN,\n",
    "                       num_latents=K):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.enc_onehot = nn.Sequential(\n",
    "            nn.Linear(num_obs, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_latents),\n",
    "            nn.Softmax(-1))\n",
    "        \n",
    "    def forward(self, obs, N, K, D, num_samples, batch_size):\n",
    "        zs_pi = self.enc_onehot(obs).view(batch_size, N, K)\n",
    "        zs = cat(zs_pi).sample((num_samples,))\n",
    "        log_qz = cat(zs_pi).log_prob(zs).view(num_samples, batch_size, -1).sum(-1) ## S * B\n",
    "        zs = zs.view(num_samples, batch_size, -1, K) ## S * B * N * K\n",
    "        return zs_pi, zs, log_qz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    enc_global = Encoder_global()\n",
    "    enc_local = Encoder_local()\n",
    "    optimizer =  torch.optim.Adam(list(enc_global.parameters()) + list(enc_local.parameters()),lr=LEARNING_RATE, betas=(0.9, 0.99))    \n",
    "    return enc_global, enc_local, optimizer\n",
    "enc_global, enc_local, optimizer = initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_mean = torch.zeros((BATCH_SIZE, K, D))\n",
    "prior_sigma = torch.ones((BATCH_SIZE, K, D))\n",
    "prior_alpha = torch.ones((BATCH_SIZE, K, D)) * 2.0\n",
    "prior_beta = torch.ones((BATCH_SIZE, K, D)) * 2.0\n",
    "\n",
    "def log_joints_gmm(X, Z, Pi, mus, precisions, N, D, K, prior_mean, prior_sigma, prior_alpha, prior_beta, batch_size):\n",
    "    log_probs = torch.zeros(batch_size).float()\n",
    "    ## priors on mus and sigmas, S * B\n",
    "    log_probs = log_probs + Normal(prior_mean, prior_sigma).log_prob(mus).sum(-1).sum(-1)\n",
    "    log_probs = log_probs + Gamma(prior_alpha, prior_beta).log_prob(precisions).sum(-1).sum(-1)\n",
    "    ## Z B-by-T-by-K\n",
    "    log_probs = log_probs + cat(Pi).log_prob(Z).sum(-1)\n",
    "    labels = Z.nonzero()\n",
    "    sigmas = 1. / torch.sqrt(precisions)\n",
    "    log_probs = log_probs + Normal(mus[labels[:, 0], labels[:, -1], :].view(batch_size, N, D), \n",
    "                                   sigmas[labels[:, 0], labels[:, -1], :].view(batch_size, N, D)).log_prob(X).sum(-1).sum(-1)\n",
    "    return log_probs\n",
    "\n",
    "def inti_global(K, D, prior_mean, prior_sigma, prior_alpha, prior_beta, batch_size):\n",
    "    mus = Normal(prior_mean, prior_sigma).sample()\n",
    "    precisions = Gamma(prior_alpha, prior_beta).sample()\n",
    "    ## log prior size B\n",
    "    log_p =  Normal(prior_mean, prior_sigma).log_prob(mus).sum(-1).sum(-1) + Gamma(prior_alpha, prior_beta).log_prob(precisions).sum(-1).sum(-1)\n",
    "    return mus, precisions, log_p\n",
    "\n",
    "def E_step(X, mus, precisions, N, D, K, batch_size):\n",
    "    mus_flat = mus.view(-1, K*D).unsqueeze(1).repeat(1, N, 1)\n",
    "    covs = 1. / torch.sqrt(precisions)\n",
    "    covs_flat = covs.view(-1, K*D).unsqueeze(1).repeat(1, N, 1)\n",
    "    data = torch.cat((X, mus_flat, covs_flat), -1).view(batch_size*N, -1)\n",
    "    zs_pi, zs, log_q_z = enc_local(data, N, K, D, 1, batch_size)\n",
    "    return zs_pi, zs[0], log_q_z[0]\n",
    "\n",
    "def M_step(X, z, N, D, K, batch_size):\n",
    "    data = torch.cat((X, z), dim=-1).view(batch_size*N, -1)\n",
    "    alpha, beta, precisions, mus_mean, mus_sigma, mus = enc_global(data, K, D, 1, batch_size)            \n",
    "    log_q_eta =  Normal(mus_mean, mus_sigma).log_prob(mus[0]).sum(-1).sum(-1) + Gamma(alpha, beta).log_prob(precisions[0]).sum(-1).sum(-1)## B\n",
    "    return mus[0], precisions[0], log_q_eta\n",
    "    \n",
    "    \n",
    "def rws(Xs, Pi, N, K, D, num_samples, steps, batch_size):\n",
    "    \"\"\"\n",
    "    train both encoders\n",
    "    rws gradient estimator\n",
    "    sis sampling scheme\n",
    "    no resampling\n",
    "    \"\"\"\n",
    "    log_increment_weights = torch.zeros((steps, num_samples, batch_size))\n",
    "    log_uptonow_weights = torch.zeros((steps, num_samples, batch_size))\n",
    "    Z_samples = torch.zeros((num_samples, batch_size, N, K))\n",
    "    for m in range(steps):\n",
    "        if m == 0:\n",
    "            for l in range(num_samples):\n",
    "                mus, precisions, log_p_eta = inti_global(K, D, prior_mean, prior_sigma, prior_alpha, prior_beta, batch_size)\n",
    "                zs_pi, z, log_q_z = E_step(Xs, mus, precisions, N, D, K, batch_size)\n",
    "                Z_samples[l] = z\n",
    "                labels = z.nonzero()\n",
    "                log_p_z = cat(Pi).log_prob(z).sum(-1)\n",
    "                sigmas = 1. / torch.sqrt(precisions)\n",
    "                log_p_x = Normal(mus[labels[:, 0], labels[:, -1], :].view(batch_size, N, D), sigmas[labels[:, 0], labels[:, -1], :].view(batch_size, N, D)).log_prob(Xs).sum(-1).sum(-1)\n",
    "                log_increment_weights[m, l] = log_p_x + log_p_z - log_q_z     \n",
    "                log_uptonow_weights[m, l] = log_p_x + log_p_z - log_q_z       \n",
    "        else:\n",
    "            for l in range(num_samples):\n",
    "                z_prev = Z_samples[l]\n",
    "                mus, precisions, log_q_eta = M_step(Xs, z_prev, N, D, K, batch_size)\n",
    "                zs_pi, z, log_q_z = E_step(Xs, mus, precisions, N, D, K, batch_size)\n",
    "                Z_samples[l] = z\n",
    "                log_p_joint = log_joints_gmm(Xs, z, Pi, mus, precisions, N, D, K, prior_mean, prior_sigma, prior_alpha, prior_beta, batch_size)\n",
    "                log_increment_weights[m, l] = log_p_joint - log_q_z - log_q_eta\n",
    "                log_uptonow_weights[m ,l] = log_increment_weights[m, l] + log_uptonow_weights[m-1 ,l]\n",
    "#     log_final_weights = log_uptonow_weights[-1]\n",
    "    log_local_weights = log_increment_weights[-1]\n",
    "    \n",
    "    \n",
    "    local_weights = torch.exp(log_local_weights - logsumexp(log_local_weights, 0)).detach()\n",
    "#     weights = torch.exp(log_final_weights - logsumexp(log_final_weights, 0)).detach()\n",
    "#     overall_weights = torch.exp(log_uptonow_weights - logsumexp(log_uptonow_weights, 1).unsqueeze(1).repeat(1, num_samples, 1)).detach()\n",
    "    \n",
    "#     eubo = torch.mul(weights, log_final_weights).sum(0).mean()\n",
    "#     elbo = log_increment_weights.mean(0).mean()\n",
    "    \n",
    "    ess = (1./ (local_weights ** 2).sum(0)).mean()\n",
    "    \n",
    "    eubolocal = torch.mul(local_weights, log_local_weights).sum(0).mean()\n",
    "    elbolocal = log_local_weights.mean(0).mean()\n",
    "    \n",
    "#     euboave = torch.mul(overall_weights, log_increment_weights).sum(1).mean(0).mean()\n",
    "#     elboave = log_increment_weights.mean(1).mean(0).mean()\n",
    "    \n",
    "    return eubolocal, elbolocal, ess \n",
    "\n",
    "def shuffler(batch_Xs, N, K, D, batch_size):\n",
    "    indices = torch.cat([torch.randperm(N).unsqueeze(0) for b in range(batch_size)])\n",
    "    indices_Xs = indices.unsqueeze(-1).repeat(1, 1, D)\n",
    "    return torch.gather(batch_Xs, 1, indices_Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, EUBO=-76112.693961, ELBO=-117339491.602795, ESS=1.017 (6s)\n",
      "epoch=1, EUBO=-304.706500, ELBO=-752.773285, ESS=1.046 (5s)\n",
      "epoch=2, EUBO=-262.492047, ELBO=-596.000909, ESS=1.059 (5s)\n",
      "epoch=3, EUBO=-233.912759, ELBO=-398.359787, ESS=1.092 (5s)\n",
      "epoch=4, EUBO=-227.140559, ELBO=-379.837527, ESS=1.143 (5s)\n",
      "epoch=5, EUBO=-218.070753, ELBO=-300.568529, ESS=1.193 (5s)\n",
      "epoch=6, EUBO=-202.685020, ELBO=-243.535242, ESS=1.235 (5s)\n",
      "epoch=7, EUBO=-196.766631, ELBO=-224.755066, ESS=1.253 (5s)\n",
      "epoch=8, EUBO=-193.276785, ELBO=-214.975041, ESS=1.323 (5s)\n",
      "epoch=9, EUBO=-190.443312, ELBO=-207.809290, ESS=1.372 (5s)\n",
      "epoch=10, EUBO=-189.086642, ELBO=-203.207533, ESS=1.415 (5s)\n",
      "epoch=11, EUBO=-187.435193, ELBO=-200.942795, ESS=1.468 (5s)\n",
      "epoch=12, EUBO=-186.475723, ELBO=-198.657088, ESS=1.533 (5s)\n",
      "epoch=13, EUBO=-185.393480, ELBO=-197.693756, ESS=1.499 (5s)\n",
      "epoch=14, EUBO=-184.601242, ELBO=-196.252005, ESS=1.601 (5s)\n",
      "epoch=15, EUBO=-184.073622, ELBO=-194.273080, ESS=1.656 (5s)\n",
      "epoch=16, EUBO=-183.323985, ELBO=-195.548790, ESS=1.554 (5s)\n",
      "epoch=17, EUBO=-182.864845, ELBO=-193.054506, ESS=1.644 (5s)\n",
      "epoch=18, EUBO=-182.381989, ELBO=-191.916841, ESS=1.673 (5s)\n",
      "epoch=19, EUBO=-181.952583, ELBO=-191.022128, ESS=1.711 (5s)\n",
      "epoch=20, EUBO=-181.953748, ELBO=-190.904103, ESS=1.669 (5s)\n",
      "epoch=21, EUBO=-181.649548, ELBO=-190.417609, ESS=1.658 (5s)\n",
      "epoch=22, EUBO=-181.460999, ELBO=-189.757614, ESS=1.735 (5s)\n",
      "epoch=23, EUBO=-181.204634, ELBO=-189.479961, ESS=1.735 (5s)\n",
      "epoch=24, EUBO=-181.136604, ELBO=-189.640442, ESS=1.714 (5s)\n",
      "epoch=25, EUBO=-180.927290, ELBO=-189.089659, ESS=1.754 (5s)\n",
      "epoch=26, EUBO=-180.921002, ELBO=-188.847687, ESS=1.756 (5s)\n",
      "epoch=27, EUBO=-180.683704, ELBO=-188.548700, ESS=1.762 (5s)\n",
      "epoch=28, EUBO=-180.882401, ELBO=-188.412852, ESS=1.781 (5s)\n",
      "epoch=29, EUBO=-180.431593, ELBO=-188.100238, ESS=1.789 (5s)\n",
      "epoch=30, EUBO=-180.388029, ELBO=-187.887152, ESS=1.808 (5s)\n",
      "epoch=31, EUBO=-180.223810, ELBO=-187.632440, ESS=1.835 (5s)\n",
      "epoch=32, EUBO=-180.252921, ELBO=-187.625508, ESS=1.813 (5s)\n",
      "epoch=33, EUBO=-179.957365, ELBO=-187.398518, ESS=1.800 (5s)\n",
      "epoch=34, EUBO=-180.059052, ELBO=-187.432025, ESS=1.799 (5s)\n",
      "epoch=35, EUBO=-179.698273, ELBO=-187.830211, ESS=1.787 (5s)\n",
      "epoch=36, EUBO=-179.746249, ELBO=-187.606720, ESS=1.796 (5s)\n",
      "epoch=37, EUBO=-179.542355, ELBO=-186.968437, ESS=1.817 (5s)\n",
      "epoch=38, EUBO=-179.569504, ELBO=-187.233577, ESS=1.784 (5s)\n",
      "epoch=39, EUBO=-179.433226, ELBO=-187.230533, ESS=1.741 (5s)\n",
      "epoch=40, EUBO=-179.518309, ELBO=-187.065982, ESS=1.817 (5s)\n",
      "epoch=41, EUBO=-179.199358, ELBO=-186.517934, ESS=1.825 (5s)\n",
      "epoch=42, EUBO=-179.397809, ELBO=-186.540506, ESS=1.839 (5s)\n",
      "epoch=43, EUBO=-179.074269, ELBO=-186.605128, ESS=1.819 (5s)\n",
      "epoch=44, EUBO=-178.938269, ELBO=-186.366521, ESS=1.805 (5s)\n",
      "epoch=45, EUBO=-178.888147, ELBO=-186.356018, ESS=1.811 (5s)\n",
      "epoch=46, EUBO=-178.490996, ELBO=-186.839723, ESS=1.742 (5s)\n",
      "epoch=47, EUBO=-178.804340, ELBO=-186.653497, ESS=1.724 (5s)\n",
      "epoch=48, EUBO=-178.825728, ELBO=-185.956950, ESS=1.841 (5s)\n",
      "epoch=49, EUBO=-178.413051, ELBO=-186.833784, ESS=1.693 (5s)\n",
      "epoch=50, EUBO=-178.556897, ELBO=-186.050856, ESS=1.789 (5s)\n",
      "epoch=51, EUBO=-178.371158, ELBO=-186.248077, ESS=1.747 (5s)\n",
      "epoch=52, EUBO=-177.936201, ELBO=-186.885536, ESS=1.662 (5s)\n",
      "epoch=53, EUBO=-178.246693, ELBO=-186.707024, ESS=1.733 (5s)\n",
      "epoch=54, EUBO=-177.885724, ELBO=-186.111432, ESS=1.722 (5s)\n",
      "epoch=55, EUBO=-177.949071, ELBO=-185.677122, ESS=1.771 (5s)\n",
      "epoch=56, EUBO=-177.894862, ELBO=-186.036473, ESS=1.755 (5s)\n",
      "epoch=57, EUBO=-177.689340, ELBO=-186.082373, ESS=1.702 (5s)\n",
      "epoch=58, EUBO=-177.514485, ELBO=-186.142894, ESS=1.678 (5s)\n",
      "epoch=59, EUBO=-177.233482, ELBO=-186.227519, ESS=1.699 (5s)\n",
      "epoch=60, EUBO=-177.141884, ELBO=-186.063930, ESS=1.649 (5s)\n",
      "epoch=61, EUBO=-177.026910, ELBO=-186.011136, ESS=1.642 (5s)\n",
      "epoch=62, EUBO=-176.994670, ELBO=-186.620398, ESS=1.641 (5s)\n",
      "epoch=63, EUBO=-176.823003, ELBO=-186.036063, ESS=1.635 (5s)\n",
      "epoch=64, EUBO=-176.281796, ELBO=-186.470633, ESS=1.575 (5s)\n",
      "epoch=65, EUBO=-176.279678, ELBO=-186.309372, ESS=1.594 (5s)\n",
      "epoch=66, EUBO=-176.345343, ELBO=-185.852620, ESS=1.616 (5s)\n",
      "epoch=67, EUBO=-176.510617, ELBO=-186.803275, ESS=1.582 (5s)\n",
      "epoch=68, EUBO=-176.169096, ELBO=-186.849959, ESS=1.538 (5s)\n",
      "epoch=69, EUBO=-176.124750, ELBO=-186.649236, ESS=1.563 (5s)\n",
      "epoch=70, EUBO=-175.530344, ELBO=-186.556290, ESS=1.493 (5s)\n",
      "epoch=71, EUBO=-175.491188, ELBO=-186.368480, ESS=1.553 (5s)\n",
      "epoch=72, EUBO=-175.785168, ELBO=-186.439935, ESS=1.544 (5s)\n",
      "epoch=73, EUBO=-175.523073, ELBO=-185.878810, ESS=1.556 (5s)\n",
      "epoch=74, EUBO=-175.417636, ELBO=-185.874829, ESS=1.573 (5s)\n",
      "epoch=75, EUBO=-175.406496, ELBO=-186.507831, ESS=1.529 (5s)\n",
      "epoch=76, EUBO=-175.563918, ELBO=-185.922597, ESS=1.582 (5s)\n",
      "epoch=77, EUBO=-175.361847, ELBO=-186.101878, ESS=1.542 (5s)\n",
      "epoch=78, EUBO=-174.857300, ELBO=-186.140683, ESS=1.521 (5s)\n",
      "epoch=79, EUBO=-174.854935, ELBO=-185.355222, ESS=1.554 (5s)\n",
      "epoch=80, EUBO=-174.832329, ELBO=-185.964731, ESS=1.539 (5s)\n",
      "epoch=81, EUBO=-174.994452, ELBO=-186.158839, ESS=1.525 (5s)\n",
      "epoch=82, EUBO=-175.265736, ELBO=-185.142830, ESS=1.614 (5s)\n",
      "epoch=83, EUBO=-174.810576, ELBO=-185.544583, ESS=1.549 (5s)\n",
      "epoch=84, EUBO=-174.552809, ELBO=-185.980876, ESS=1.500 (5s)\n",
      "epoch=85, EUBO=-174.763776, ELBO=-185.532227, ESS=1.554 (5s)\n",
      "epoch=86, EUBO=-174.925073, ELBO=-186.379150, ESS=1.537 (5s)\n",
      "epoch=87, EUBO=-174.347452, ELBO=-185.235542, ESS=1.524 (5s)\n",
      "epoch=88, EUBO=-174.552422, ELBO=-186.253584, ESS=1.489 (5s)\n",
      "epoch=89, EUBO=-174.041884, ELBO=-184.972699, ESS=1.546 (5s)\n",
      "epoch=90, EUBO=-174.037387, ELBO=-184.772437, ESS=1.573 (5s)\n",
      "epoch=91, EUBO=-173.865303, ELBO=-185.381770, ESS=1.541 (5s)\n",
      "epoch=92, EUBO=-173.878044, ELBO=-185.254485, ESS=1.540 (5s)\n",
      "epoch=93, EUBO=-173.607747, ELBO=-184.435089, ESS=1.562 (5s)\n",
      "epoch=94, EUBO=-174.101411, ELBO=-185.494160, ESS=1.587 (5s)\n",
      "epoch=95, EUBO=-174.008208, ELBO=-185.505693, ESS=1.531 (5s)\n",
      "epoch=96, EUBO=-173.657741, ELBO=-184.795552, ESS=1.573 (5s)\n",
      "epoch=97, EUBO=-173.618625, ELBO=-184.804443, ESS=1.545 (5s)\n",
      "epoch=98, EUBO=-173.309183, ELBO=-184.152899, ESS=1.514 (5s)\n",
      "epoch=99, EUBO=-173.487163, ELBO=-185.064920, ESS=1.525 (5s)\n",
      "epoch=100, EUBO=-173.252007, ELBO=-184.865781, ESS=1.501 (5s)\n",
      "epoch=101, EUBO=-173.305295, ELBO=-184.191116, ESS=1.521 (5s)\n",
      "epoch=102, EUBO=-173.600157, ELBO=-184.409424, ESS=1.542 (5s)\n",
      "epoch=103, EUBO=-173.122797, ELBO=-185.038327, ESS=1.484 (5s)\n",
      "epoch=104, EUBO=-173.376955, ELBO=-185.814806, ESS=1.498 (5s)\n",
      "epoch=105, EUBO=-173.107129, ELBO=-184.444046, ESS=1.524 (5s)\n",
      "epoch=106, EUBO=-173.287076, ELBO=-184.845448, ESS=1.516 (5s)\n",
      "epoch=107, EUBO=-172.709772, ELBO=-185.061221, ESS=1.489 (5s)\n",
      "epoch=108, EUBO=-172.424232, ELBO=-183.803229, ESS=1.516 (5s)\n",
      "epoch=109, EUBO=-172.524498, ELBO=-183.122911, ESS=1.553 (5s)\n",
      "epoch=110, EUBO=-172.342506, ELBO=-184.002325, ESS=1.522 (5s)\n",
      "epoch=111, EUBO=-172.274933, ELBO=-183.518710, ESS=1.541 (5s)\n",
      "epoch=112, EUBO=-172.210513, ELBO=-183.818195, ESS=1.498 (5s)\n",
      "epoch=113, EUBO=-172.228847, ELBO=-184.258081, ESS=1.481 (5s)\n",
      "epoch=114, EUBO=-172.210367, ELBO=-183.989345, ESS=1.518 (5s)\n",
      "epoch=115, EUBO=-171.960968, ELBO=-184.028319, ESS=1.495 (5s)\n",
      "epoch=116, EUBO=-171.705386, ELBO=-183.407501, ESS=1.490 (5s)\n",
      "epoch=117, EUBO=-171.603299, ELBO=-183.481294, ESS=1.498 (5s)\n",
      "epoch=118, EUBO=-171.411607, ELBO=-183.638785, ESS=1.522 (5s)\n",
      "epoch=119, EUBO=-171.279819, ELBO=-182.563148, ESS=1.571 (5s)\n",
      "epoch=120, EUBO=-171.980641, ELBO=-182.874657, ESS=1.565 (5s)\n",
      "epoch=121, EUBO=-171.907584, ELBO=-184.067445, ESS=1.475 (5s)\n",
      "epoch=122, EUBO=-171.200597, ELBO=-183.822896, ESS=1.465 (5s)\n",
      "epoch=123, EUBO=-171.286526, ELBO=-182.769055, ESS=1.501 (5s)\n",
      "epoch=124, EUBO=-171.189041, ELBO=-182.950931, ESS=1.496 (5s)\n",
      "epoch=125, EUBO=-171.204955, ELBO=-184.133392, ESS=1.472 (5s)\n",
      "epoch=126, EUBO=-171.115416, ELBO=-182.670139, ESS=1.496 (5s)\n",
      "epoch=127, EUBO=-170.946864, ELBO=-182.336102, ESS=1.496 (5s)\n",
      "epoch=128, EUBO=-171.166614, ELBO=-182.874341, ESS=1.508 (5s)\n",
      "epoch=129, EUBO=-171.259576, ELBO=-182.722678, ESS=1.510 (5s)\n",
      "epoch=130, EUBO=-170.991495, ELBO=-183.264008, ESS=1.502 (5s)\n",
      "epoch=131, EUBO=-170.990269, ELBO=-183.114799, ESS=1.488 (5s)\n",
      "epoch=132, EUBO=-170.761624, ELBO=-182.654495, ESS=1.503 (5s)\n",
      "epoch=133, EUBO=-170.682881, ELBO=-181.880447, ESS=1.550 (5s)\n",
      "epoch=134, EUBO=-170.775124, ELBO=-183.090746, ESS=1.508 (5s)\n",
      "epoch=135, EUBO=-170.784497, ELBO=-182.506136, ESS=1.513 (5s)\n",
      "epoch=136, EUBO=-170.730615, ELBO=-181.716051, ESS=1.555 (5s)\n",
      "epoch=137, EUBO=-170.881223, ELBO=-182.700491, ESS=1.513 (5s)\n",
      "epoch=138, EUBO=-170.355836, ELBO=-181.667870, ESS=1.531 (5s)\n",
      "epoch=139, EUBO=-170.591382, ELBO=-181.705598, ESS=1.556 (5s)\n",
      "epoch=140, EUBO=-170.143665, ELBO=-181.254918, ESS=1.531 (5s)\n",
      "epoch=141, EUBO=-170.186951, ELBO=-181.390393, ESS=1.523 (5s)\n",
      "epoch=142, EUBO=-170.269843, ELBO=-181.875648, ESS=1.505 (5s)\n",
      "epoch=143, EUBO=-170.107675, ELBO=-181.622896, ESS=1.504 (5s)\n",
      "epoch=144, EUBO=-170.234933, ELBO=-181.291486, ESS=1.511 (5s)\n",
      "epoch=145, EUBO=-169.941000, ELBO=-180.560495, ESS=1.538 (5s)\n",
      "epoch=146, EUBO=-169.909634, ELBO=-181.753453, ESS=1.515 (5s)\n",
      "epoch=147, EUBO=-170.171806, ELBO=-181.439931, ESS=1.511 (5s)\n",
      "epoch=148, EUBO=-170.321460, ELBO=-181.638519, ESS=1.552 (5s)\n",
      "epoch=149, EUBO=-170.210081, ELBO=-181.407220, ESS=1.541 (5s)\n",
      "epoch=150, EUBO=-170.229846, ELBO=-181.496519, ESS=1.525 (5s)\n",
      "epoch=151, EUBO=-170.181328, ELBO=-181.394612, ESS=1.541 (5s)\n",
      "epoch=152, EUBO=-169.669675, ELBO=-180.409782, ESS=1.590 (5s)\n",
      "epoch=153, EUBO=-169.807410, ELBO=-180.722029, ESS=1.557 (5s)\n",
      "epoch=154, EUBO=-169.717599, ELBO=-180.595444, ESS=1.542 (5s)\n",
      "epoch=155, EUBO=-169.696405, ELBO=-181.421915, ESS=1.497 (5s)\n",
      "epoch=156, EUBO=-169.949651, ELBO=-180.727026, ESS=1.549 (5s)\n",
      "epoch=157, EUBO=-169.678395, ELBO=-181.554288, ESS=1.517 (5s)\n",
      "epoch=158, EUBO=-169.793948, ELBO=-181.719910, ESS=1.510 (5s)\n",
      "epoch=159, EUBO=-169.507378, ELBO=-180.408820, ESS=1.557 (5s)\n",
      "epoch=160, EUBO=-169.416266, ELBO=-180.116225, ESS=1.591 (5s)\n",
      "epoch=161, EUBO=-169.519394, ELBO=-180.026668, ESS=1.603 (5s)\n",
      "epoch=162, EUBO=-169.303607, ELBO=-179.794244, ESS=1.579 (5s)\n",
      "epoch=163, EUBO=-169.078471, ELBO=-180.419766, ESS=1.543 (5s)\n",
      "epoch=164, EUBO=-168.953783, ELBO=-180.415877, ESS=1.528 (5s)\n",
      "epoch=165, EUBO=-169.554704, ELBO=-180.220854, ESS=1.568 (5s)\n",
      "epoch=166, EUBO=-169.354753, ELBO=-179.640636, ESS=1.582 (5s)\n",
      "epoch=167, EUBO=-169.322208, ELBO=-180.345471, ESS=1.525 (5s)\n",
      "epoch=168, EUBO=-169.207373, ELBO=-180.421974, ESS=1.543 (5s)\n",
      "epoch=169, EUBO=-169.014328, ELBO=-180.824689, ESS=1.529 (5s)\n",
      "epoch=170, EUBO=-169.247746, ELBO=-180.541716, ESS=1.530 (5s)\n",
      "epoch=171, EUBO=-169.053694, ELBO=-179.541707, ESS=1.581 (5s)\n",
      "epoch=172, EUBO=-168.809189, ELBO=-180.150288, ESS=1.553 (5s)\n",
      "epoch=173, EUBO=-168.788632, ELBO=-179.546080, ESS=1.547 (5s)\n",
      "epoch=174, EUBO=-169.344131, ELBO=-179.755122, ESS=1.594 (5s)\n",
      "epoch=175, EUBO=-169.310037, ELBO=-180.118707, ESS=1.578 (5s)\n",
      "epoch=176, EUBO=-169.025124, ELBO=-179.409479, ESS=1.582 (5s)\n",
      "epoch=177, EUBO=-169.158766, ELBO=-179.389252, ESS=1.607 (5s)\n"
     ]
    }
   ],
   "source": [
    "EUBOs = []\n",
    "ELBOs = []\n",
    "ESSs = []\n",
    "num_batches = int((Xs.shape[0] / BATCH_SIZE))\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    time_start = time.time()\n",
    "    indices = torch.randperm(num_seqs)\n",
    "    EUBO = 0.0\n",
    "    ELBO = 0.0\n",
    "    ESS = 0.0\n",
    "    for step in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "        batch_indices = indices[step*BATCH_SIZE : (step+1)*BATCH_SIZE]\n",
    "        batch_Xs = Xs[batch_indices]\n",
    "        batch_Xs = shuffler(batch_Xs, N, K, D, BATCH_SIZE)\n",
    "        eubo, elbo, ess = rws(batch_Xs, Pi, N, K, D, NUM_SAMPLES, STEPS, BATCH_SIZE)\n",
    "        eubo.backward()\n",
    "        optimizer.step()\n",
    "        EUBO += eubo.item()\n",
    "        ELBO += elbo.item()\n",
    "        ESS += ess.item()\n",
    "    EUBO /= num_batches\n",
    "    ELBO /= num_batches\n",
    "    ESS /= num_batches\n",
    "    EUBOs.append(EUBO)\n",
    "    ELBOs.append(ELBO)\n",
    "    ESSs.append(ESS)\n",
    "\n",
    "    time_end = time.time()\n",
    "    print('epoch=%d, EUBO=%f, ELBO=%f, ESS=%.3f (%ds)' % (epoch, EUBO, ELBO, ESS, time_end - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(EUBOs, ELBOs, ESSs, NUM_SAMPLES, NUM_EPOCHS, LEARNING_RATE):\n",
    "    fout = open('local_amorgibbs-steps=%d-samples=%d-lr=%d.txt' % (STEPS, NUM_SAMPLES, LEARNING_RATE), 'w+')\n",
    "    fout.write('EUBOs, ELBOs, ESSs\\n')\n",
    "    for i in range(len(EUBOs)):\n",
    "        fout.write(str(EUBOs[i]) + ', ' + str(ELBOs[i]) + ', ' + str(ESSs[i]) + '\\n')\n",
    "    fout.close()\n",
    "# torch.save(enc.state_dict(), 'models/local_amorgibbs-steps=%d-samples=%d-lr=%d' % (STEPS, NUM_SAMPLES, LEARNING_RATE))\n",
    "save_results(EUBOs, ELBOs, ESSs, NUM_SAMPLES, NUM_EPOCHS, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(EUBOs, ELBOs, ESSs, num_samples, num_epochs, lr):\n",
    "    fig = plt.figure(figsize=(30, 30))\n",
    "    fig.tight_layout()\n",
    "    ax1 = fig.add_subplot(2, 1, 1)\n",
    "    ax2 = fig.add_subplot(2, 1, 2)\n",
    "    ax1.plot(EUBOs, 'r', label='EUBOs')\n",
    "    ax1.plot(ELBOs, 'b', label='ELBOs')\n",
    "    ax1.tick_params(labelsize=18)\n",
    "    ax2.plot(np.array(ESSs) / num_samples, 'm', label='ESS')\n",
    "    ax1.set_title('epoch=%d, batch_size=%d, lr=%.1E, samples=%d' % (num_epochs, BATCH_SIZE, lr, num_samples), fontsize=18)\n",
    "    ax1.set_ylim([-200, -150])\n",
    "    ax1.legend()\n",
    "    ax2.legend()\n",
    "    ax2.tick_params(labelsize=18)\n",
    "    plt.savefig('local_gibbs_results_learn_both_lr=%.1E_samples=%d.svg' % (lr, num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(EUBOs, ELBOs, ESSs, NUM_SAMPLES, NUM_EPOCHS, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.load(enc.state_dict(), 'models/local_amorgibbs-steps=%d-samples=%d-lr=%d' % (STEPS, NUM_SAMPLES, LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_mean = torch.zeros((BATCH_SIZE, K, D))\n",
    "prior_sigma = torch.ones((BATCH_SIZE, K, D))\n",
    "prior_alpha = torch.ones((BATCH_SIZE, K, D)) * 2.0\n",
    "prior_beta = torch.ones((BATCH_SIZE, K, D)) * 2.0\n",
    "\n",
    "\n",
    "def sample_single_batch(num_seqs, N, K, D, batch_size):\n",
    "    indices = torch.randperm(num_seqs)\n",
    "    batch_indices = indices[0*batch_size : (0+1)*batch_size]\n",
    "    batch_Xs = Xs[batch_indices]\n",
    "    batch_Xs = shuffler(batch_Xs, N, K, D, batch_size)\n",
    "    return batch_Xs\n",
    "\n",
    "def test(num_seqs, Pi, N, K, D, steps, batch_size):\n",
    "    x = sample_single_batch(num_seqs, N, K, D, batch_size)\n",
    "    for m in range(steps):\n",
    "        if m == 0:\n",
    "            for l in range(num_samples):\n",
    "                mus, precisions, log_p_eta = inti_global(K, D, prior_mean, prior_sigma, prior_alpha, prior_beta, batch_size)\n",
    "                zs_pi, z, log_q_z = E_step(x, mus, precisions, N, D, K, batch_size)\n",
    "                Z_samples[l] = z    \n",
    "        else:\n",
    "            for l in range(num_samples):\n",
    "                z_prev = Z_samples[l]\n",
    "                mus, precisions, log_q_eta = M_step(x, z_prev, N, D, K, batch_size)\n",
    "                zs_pi, z, log_q_z = E_step(x, mus, precisions, N, D, K, batch_size)\n",
    "    return x, z, mus, precisions\n",
    "\n",
    "def plot_final_samples(Xs, Zs, mus, precisions, steps, batch_size):\n",
    "    colors = ['r', 'b', 'gold']\n",
    "    fig = plt.figure(figsize=(25,50))\n",
    "    for b in range(batch_size):\n",
    "        ax = fig.add_subplot(int(batch_size / 5), 5, b+1)\n",
    "        x = Xs[b].data.numpy()\n",
    "        z = Zs[b].data.numpy()\n",
    "        mu = mus[b].data.numpy()\n",
    "        precision = precisions[b].data.numpy()\n",
    "\n",
    "        covs = np.zeros((K, D, D))\n",
    "        assignments = np.nonzero(z)[1]\n",
    "        for k in range(K):\n",
    "            covs[k] = np.diag(1. / precision[k])\n",
    "            xk = x[np.where(assignments == k)]\n",
    "            ax.scatter(xk[:, 0], xk[:, 1], c=colors[k])\n",
    "            plot_cov_ellipse(cov=covs[k], pos=mu[k], nstd=2, ax=ax, alpha=0.2, color=colors[k])\n",
    "        ax.set_ylim([-10, 10])\n",
    "        ax.set_xlim([-10, 10])\n",
    "    plt.savefig('amorgibbs_samples_steps=%d.svg' % (steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS = 1000\n",
    "BATCH_SIZE = 50\n",
    "\n",
    "x, z_samples, mus_samples, precisions_samples = test(num_seqs, Pi, N, K, D, STEPS, BATCH_SIZE)\n",
    "plot_final_samples(x, z_samples, mus_samples, precisions_samples, STEPS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
