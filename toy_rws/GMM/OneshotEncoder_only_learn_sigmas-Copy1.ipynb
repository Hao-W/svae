{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probtorch: 0.0+5a2c637 torch: 1.0.0 cuda: True\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from plots import *\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal as mvn\n",
    "from torch.distributions.one_hot_categorical import OneHotCategorical as cat\n",
    "from torch.distributions.gamma import Gamma\n",
    "from torch import logsumexp\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "sys.path.append('/home/hao/Research/probtorch/')\n",
    "import probtorch\n",
    "print('probtorch:', probtorch.__version__, \n",
    "      'torch:', torch.__version__, \n",
    "      'cuda:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "K = 3\n",
    "D = 2\n",
    "\n",
    "## Model Parameters\n",
    "NUM_SAMPLES = 10\n",
    "NUM_HIDDEN = 64\n",
    "NUM_STATS = K+D*K+D*K\n",
    "NUM_LATENTS = D * K\n",
    "NUM_OBS = D + K\n",
    "BATCH_SIZE = 10\n",
    "NUM_EPOCHS = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = torch.from_numpy(np.load('gmm_dataset/sequences.npy')).float()\n",
    "Zs = torch.from_numpy(np.load('gmm_dataset/states.npy')).float()\n",
    "mus_true = torch.from_numpy(np.load('gmm_dataset/means.npy')).float()\n",
    "# covs = torch.from_numpy(np.load('gmm_dataset2/covariances.npy')).float()\n",
    "Pi = torch.from_numpy(np.load('gmm_dataset/init.npy')).float()\n",
    "num_seqs = Zs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StatsGMM(Xs, Zs, K, D):\n",
    "    \"\"\"\n",
    "    Xs is B * N * D\n",
    "    Zs is B * N * K\n",
    "    stat1 corresponds I[z_n=1], ..., I[z_n=K]\n",
    "    stat2 corresponds I[z_n=1]x_n, ..., I[z_n=K]x_n\n",
    "    stat3 corresponds I[z_n=1]x_n**2, ..., I[z_n=K]x_n**2\n",
    "    return B * (K+D*K+D*K)\n",
    "    \"\"\"\n",
    "    stat1 = Zs.sum(1)\n",
    "    stat2 = torch.mul(Zs.unsqueeze(-1).repeat(1, 1, 1, D), Xs.unsqueeze(-1).repeat(1, 1, 1, K).transpose(-1, -2)).sum(1) \n",
    "    stat3 = torch.mul(Zs.unsqueeze(-1).repeat(1, 1, 1, D), torch.mul(Xs, Xs).unsqueeze(-1).repeat(1, 1, 1, K).transpose(-1, -2)).sum(1) \n",
    "    return stat1, stat2, stat3, torch.cat((stat1, stat2.view(-1, D*K), stat3.view(-1, D*K)), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_obs=NUM_OBS,\n",
    "                       num_stats=NUM_STATS,\n",
    "                       num_hidden=NUM_HIDDEN,\n",
    "                       num_latents=NUM_LATENTS):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.enc_stats = nn.Sequential(\n",
    "            nn.Linear(num_obs, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_stats))\n",
    "        self.enc_hidden = nn.Sequential(\n",
    "            nn.Linear(num_stats, num_hidden),\n",
    "            nn.ReLU())\n",
    "        self.sigmas_log_alpha = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        self.sigmas_log_beta = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        \n",
    "    def forward(self, obs, K, D, num_samples, batch_size):\n",
    "        stats = self.enc_stats(obs).view(batch_size, N, -1).sum(1)\n",
    "        hidden = self.enc_hidden(stats)\n",
    "        alpha = torch.exp(self.sigmas_log_alpha(hidden).view(-1, K, D))\n",
    "        beta = torch.exp(self.sigmas_log_beta(hidden).view(-1, K, D))\n",
    "        precisions = Gamma(alpha, beta).sample((num_samples,))\n",
    "        return alpha, beta, precisions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    enc = Encoder()\n",
    "    if CUDA:\n",
    "        enc.cuda()\n",
    "    optimizer =  torch.optim.Adam(list(enc.parameters()),lr=LEARNING_RATE)    \n",
    "    return enc, optimizer\n",
    "enc, optimizer = initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_joints_gmm(Z, Pi, mus, precisions, Xs, N, D, K, num_samples, batch_size):\n",
    "    log_probs = torch.zeros((num_samples, batch_size)).float()\n",
    "    ## priors on mus and sigmas, S * B\n",
    "#     log_probs = log_probs + Normal(torch.zeros((batch_size, K, D)), torch.ones((batch_size, K, D))).log_prob(mus).sum(-1).sum(-1)\n",
    "    log_probs = log_probs + Gamma(torch.ones((batch_size, K, D)) * 2.0, torch.ones((batch_size, K, D)) * 2.0).log_prob(precisions).sum(-1).sum(-1)\n",
    "    ## Z B-by-T-by-K\n",
    "#     log_probs = log_probs + cat(Pi).log_prob(Z).sum(-1)\n",
    "    labels = Z.nonzero()\n",
    "    sigmas = 1. / torch.sqrt(precisions)\n",
    "    mus_expand = mus.unsqueeze(0).repeat(num_samples, 1, 1, 1)\n",
    "    log_probs = log_probs + Normal(mus_expand[:, labels[:, 0], labels[:, -1], :].view(-1, batch_size, N, D), sigmas[:, labels[:, 0], labels[:, -1], :].view(-1, batch_size, N, D)).log_prob(Xs).sum(-1).sum(-1)\n",
    "    return log_probs\n",
    "\n",
    "def conjugate_posterior(stat1, stat2, stat3, mus, K, D, batch_size):\n",
    "    stat1_expand = stat1.unsqueeze(-1).repeat(1, 1, D)\n",
    "    posterior_alpha = torch.ones((batch_size, K, D)) * 2.0 + (stat1_expand / 2.)\n",
    "    posterior_beta = torch.ones((batch_size, K, D)) * 2.0 + (stat3 + (stat1_expand * (mus ** 2)) - 2 * mus * stat2) / 2.\n",
    "    return posterior_alpha, posterior_beta\n",
    "    \n",
    "def kl_gamma_gamma(p_alpha, p_beta, q_alpha, q_beta):\n",
    "    t1 = q_alpha * (p_beta / q_beta).log()\n",
    "    t2 = torch.lgamma(q_alpha) - torch.lgamma(p_alpha)\n",
    "    t3 = (p_alpha - q_alpha) * torch.digamma(p_alpha)\n",
    "    t4 = (q_beta - p_beta) * (p_alpha / p_beta)\n",
    "    return t1 + t2 + t3 + t4\n",
    "\n",
    "def kls_gammas(weights, tau, q_alpha, q_beta, p_alpha, p_beta, K, D):\n",
    "    log_q = Gamma(q_alpha, q_beta).log_prob(tau).sum(-1).sum(-1)\n",
    "    log_p = Gamma(p_alpha, p_beta).log_prob(tau).sum(-1).sum(-1)\n",
    "    MCKl_exclusive = (log_q - log_p).mean(0).mean()\n",
    "    TrueKl_exclusive = kl_gamma_gamma(q_alpha, q_beta, p_alpha, p_beta).mean()\n",
    "    \n",
    "    MCKl_inclusive = torch.mul(weights, log_p - log_q).sum(0).mean()\n",
    "    TrueKl_inclusive = kl_gamma_gamma(p_alpha, p_beta, q_alpha, q_beta).mean()\n",
    "    return MCKl_inclusive, TrueKl_inclusive, MCKl_exclusive, TrueKl_exclusive\n",
    "\n",
    "def rws(Xs, Zs, Pi, N, K, D, mus, num_samples, batch_size):\n",
    "    stat1, stat2, stat3, stats = StatsGMM(Xs, Zs, K, D)\n",
    "    data = torch.cat((Xs, Zs), dim=-1).view(batch_size*N, -1)\n",
    "    sigmas_alpha, sigmas_beta, precisions = enc(data, K, D, num_samples, batch_size)\n",
    "    log_q =  Gamma(sigmas_alpha, sigmas_beta).log_prob(precisions).sum(-1).sum(-1)## S * B\n",
    "\n",
    "    log_p = log_joints_gmm(Zs, Pi, mus, precisions, Xs, N, D, K, num_samples, batch_size)\n",
    "    log_weights = log_p - log_q\n",
    "    weights = torch.exp(log_weights - logsumexp(log_weights, dim=0)).detach()\n",
    "    eubo = torch.mul(weights, log_weights).sum(0).mean()\n",
    "    elbo = log_weights.mean(0).mean()\n",
    "    ess = (1. / (weights ** 2).sum(0)).mean()\n",
    "    posterior_alpha, posterior_beta = conjugate_posterior(stat1, stat2, stat3, mus, K, D, batch_size)\n",
    "    MCKl_inclusive, TrueKl_inclusive, MCKl_exclusive, TrueKl_exclusive = kls_gammas(weights, precisions, sigmas_alpha, sigmas_beta, posterior_alpha, posterior_beta, K, D)\n",
    "    return eubo, elbo, ess, MCKl_inclusive, TrueKl_inclusive, MCKl_exclusive, TrueKl_exclusive\n",
    "\n",
    "def shuffler(batch_Xs, batch_Zs, N, K, D, batch_size):\n",
    "    indices = torch.cat([torch.randperm(N).unsqueeze(0) for b in range(batch_size)])\n",
    "    indices_Xs = indices.unsqueeze(-1).repeat(1, 1, D)\n",
    "    indices_Zs = indices.unsqueeze(-1).repeat(1, 1, K)\n",
    "    return torch.gather(batch_Xs, 1, indices_Xs), torch.gather(batch_Zs, 1, indices_Zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, EUBO=-738596.025490, ELBO=-770770.786326, ESS=1.025, inc MCKL=-738511.703481, inc TKL=528.446882, exc MCKL=770686.560434, exc TKL=133475.820961 (2s)\n",
      "epoch=10, EUBO=-88.194787, ELBO=-100.103357, ESS=1.631, inc MCKL=-2.943656, inc TKL=1.674807, exc MCKL=14.852230, exc TKL=2.483812 (12s)\n",
      "epoch=20, EUBO=-86.998185, ELBO=-96.748324, ESS=1.818, inc MCKL=-1.747056, inc TKL=0.966432, exc MCKL=11.497199, exc TKL=1.935096 (12s)\n",
      "epoch=30, EUBO=-86.761644, ELBO=-96.243587, ESS=1.787, inc MCKL=-1.510520, inc TKL=0.836779, exc MCKL=10.992462, exc TKL=1.844722 (10s)\n",
      "epoch=40, EUBO=-86.683500, ELBO=-96.208616, ESS=1.803, inc MCKL=-1.432374, inc TKL=0.774485, exc MCKL=10.957489, exc TKL=1.847699 (11s)\n",
      "epoch=50, EUBO=-86.547169, ELBO=-96.050105, ESS=1.813, inc MCKL=-1.296043, inc TKL=0.760544, exc MCKL=10.798979, exc TKL=1.817793 (11s)\n",
      "epoch=60, EUBO=-86.335220, ELBO=-95.773883, ESS=1.839, inc MCKL=-1.084081, inc TKL=0.752072, exc MCKL=10.522755, exc TKL=1.753161 (12s)\n",
      "epoch=70, EUBO=-86.407134, ELBO=-95.522074, ESS=1.839, inc MCKL=-1.156003, inc TKL=0.755622, exc MCKL=10.270946, exc TKL=1.733302 (13s)\n",
      "epoch=80, EUBO=-86.462776, ELBO=-95.947152, ESS=1.831, inc MCKL=-1.211648, inc TKL=0.717447, exc MCKL=10.696026, exc TKL=1.777929 (13s)\n",
      "epoch=90, EUBO=-86.363741, ELBO=-95.837328, ESS=1.820, inc MCKL=-1.112619, inc TKL=0.714983, exc MCKL=10.586200, exc TKL=1.763425 (12s)\n"
     ]
    }
   ],
   "source": [
    "EUBOs = []\n",
    "ELBOs = []\n",
    "ESSs = []\n",
    "MCKls_inclusive = []\n",
    "TrueKls_inclusive = []\n",
    "MCKls_exclusive = []\n",
    "TrueKls_exclusive = []\n",
    "\n",
    "num_batches = int((Xs.shape[0] / BATCH_SIZE))\n",
    "time_start = time.time()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    indices = torch.randperm(num_seqs)\n",
    "\n",
    "    EUBO = 0.0\n",
    "    ELBO = 0.0\n",
    "    ESS = 0.0\n",
    "    MCKl_inclusive = 0.0\n",
    "    TrueKl_inclusive = 0.0\n",
    "    MCKl_exclusive = 0.0\n",
    "    TrueKl_exclusive = 0.0\n",
    "    for step in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "        batch_indices = indices[step*BATCH_SIZE : (step+1)*BATCH_SIZE]\n",
    "        batch_Xs = Xs[batch_indices]\n",
    "        batch_Zs = Zs[batch_indices]\n",
    "        batch_Mus = mus_true[batch_indices]\n",
    "        batch_Xs, batch_Zs = shuffler(batch_Xs, batch_Zs, N, K, D, BATCH_SIZE)\n",
    "        eubo, elbo, ess, mckl_inclusive, truekl_inclusive, mckl_exclusive, truekl_exclusive = rws(batch_Xs, batch_Zs, Pi, N, K, D, batch_Mus, NUM_SAMPLES, BATCH_SIZE)\n",
    "        eubo.backward()\n",
    "        optimizer.step()\n",
    "        EUBO += eubo.item()\n",
    "        ELBO += elbo.item()\n",
    "        ESS += ess.item()\n",
    "        MCKl_inclusive += mckl_inclusive.item()\n",
    "        MCKl_exclusive += mckl_exclusive.item()\n",
    "        TrueKl_inclusive += truekl_inclusive.item()\n",
    "        TrueKl_exclusive += truekl_exclusive.item()\n",
    "        \n",
    "    EUBO /= num_batches\n",
    "    ELBO /= num_batches\n",
    "    ESS /= num_batches\n",
    "    MCKl_inclusive /= num_batches\n",
    "    TrueKl_inclusive /= num_batches\n",
    "    MCKl_exclusive /= num_batches\n",
    "    TrueKl_exclusive /= num_batches\n",
    "    \n",
    "    EUBOs.append(EUBO)\n",
    "    ELBOs.append(ELBO)\n",
    "    ESSs.append(ESS)\n",
    "    MCKls_inclusive.append(MCKl_inclusive)\n",
    "    TrueKls_inclusive.append(TrueKl_inclusive)\n",
    "    MCKls_exclusive.append(MCKl_exclusive)\n",
    "    TrueKls_exclusive.append(TrueKl_exclusive)\n",
    "    \n",
    "#     time_end = time.time()\n",
    "    if epoch % 10 == 0:\n",
    "        time_end = time.time()\n",
    "        print('epoch=%d, EUBO=%f, ELBO=%f, ESS=%.3f, inc MCKL=%f, inc TKL=%f, exc MCKL=%f, exc TKL=%f (%ds)' % (epoch, EUBO, ELBO, ESS, MCKl_inclusive, TrueKl_inclusive, MCKl_exclusive, TrueKl_exclusive, time_end - time_start))\n",
    "        time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(EUBOs, ELBOs, ESSs, MCKls_exclusive, TrueKls_exclusive, MCKls_inclusive, TrueKls_inclusive, num_samples, num_epochs, lr):\n",
    "    fig = plt.figure(figsize=(30, 30))\n",
    "    fig.tight_layout()\n",
    "    ax1 = fig.add_subplot(3, 1, 1)\n",
    "    ax2 = fig.add_subplot(3, 1, 2)\n",
    "    ax3 = fig.add_subplot(3, 1, 3)\n",
    "    ax1.plot(EUBOs, 'r', label='EUBOs')\n",
    "    ax1.plot(ELBOs, 'b', label='ELBOs')\n",
    "    ax2.plot(TrueKls_exclusive, '#66b3ff', label='true exclusive KL')\n",
    "    ax2.plot(MCKls_exclusive, '#ff9999', label='est exclusive KL')\n",
    "    ax2.plot(TrueKls_inclusive, '#99ff99', label='true inclusive KL')\n",
    "    ax2.plot(MCKls_inclusive, 'gold', label='est inclusive KL')\n",
    "    \n",
    "    ax1.tick_params(labelsize=18)\n",
    "    ax3.plot(np.array(ESSs) / num_samples, 'm', label='ESS')\n",
    "    ax1.set_title('epoch=%d, batch_size=%d, lr=%.1E, samples=%d' % (num_epochs, BATCH_SIZE, lr, num_samples), fontsize=18)\n",
    "    ax1.set_ylim([-300, -80])\n",
    "    ax1.legend()\n",
    "    ax2.set_ylim([-50, 50])\n",
    "    ax2.legend()\n",
    "    ax3.legend()\n",
    "    ax2.tick_params(labelsize=18)\n",
    "    ax3.tick_params(labelsize=18)\n",
    "    plt.savefig('gmm_rws_datatodist_lr=%.1E_samples=%d.svg' % (lr, num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(EUBOs, ELBOs, ESSs, MCKls_exclusive, TrueKls_exclusive, MCKls_inclusive, TrueKls_inclusive, NUM_SAMPLES, NUM_EPOCHS, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
