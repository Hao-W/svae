{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from plots import *\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal as mvn\n",
    "from torch.distributions.one_hot_categorical import OneHotCategorical as cat\n",
    "from torch import logsumexp\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "sys.path.append('/home/hao/Research/probtorch/')\n",
    "import probtorch\n",
    "print('probtorch:', probtorch.__version__, \n",
    "      'torch:', torch.__version__, \n",
    "      'cuda:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "K = 3\n",
    "D = 2\n",
    "\n",
    "## Model Parameters\n",
    "NUM_SAMPLES = 10\n",
    "NUM_HIDDEN = 64\n",
    "NUM_STATS = K+D*K+D*K\n",
    "NUM_LATENTS = D * K\n",
    "NUM_OBS = D + K\n",
    "BATCH_SIZE = 10\n",
    "NUM_EPOCHS = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = torch.from_numpy(np.load('gmm_dataset/sequences.npy')).float()\n",
    "Zs = torch.from_numpy(np.load('gmm_dataset/states.npy')).float()\n",
    "mus_true = torch.from_numpy(np.load('gmm_dataset/means.npy')).float()\n",
    "covs = torch.from_numpy(np.load('gmm_dataset/covariances.npy')).float()\n",
    "Pi = torch.from_numpy(np.load('gmm_dataset/init.npy')).float()\n",
    "num_seqs = Zs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def StatsGMM(Xs, Zs, K, D):\n",
    "    \"\"\"\n",
    "    Xs is B * N * D\n",
    "    Zs is B * N * K\n",
    "    return B * (K+D*K)\n",
    "    \"\"\"\n",
    "    stat1 = Zs.sum(1)\n",
    "    stat2 = torch.mul(Zs.unsqueeze(-1).repeat(1, 1, 1, D), Xs.unsqueeze(-1).repeat(1, 1, 1, K).transpose(-1, -2)).sum(1) \n",
    "    return stat1, stat2, torch.cat((stat1, stat2.view(-1, D*K)), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_obs=NUM_OBS,\n",
    "                       num_stats=NUM_STATS,\n",
    "                       num_hidden=NUM_HIDDEN,\n",
    "                       num_latents=NUM_LATENTS):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.enc_stats = nn.Sequential(\n",
    "            nn.Linear(num_obs, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_stats))\n",
    "        self.enc_hidden = nn.Sequential(\n",
    "            nn.Linear(num_stats+K*D, num_hidden),\n",
    "            nn.ReLU())\n",
    "        self.mean = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        self.log_std = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        \n",
    "    def forward(self, obs, K, D, covs, num_samples, batch_size):\n",
    "        stats = self.enc_stats(obs).view(batch_size, N, -1).sum(1)\n",
    "        stats_covs = torch.cat((stats, covs.view(-1, K*D)), dim=-1)\n",
    "\n",
    "        hidden = self.enc_hidden(stats_covs)\n",
    "        mean = self.mean(hidden).view(-1, K, D)\n",
    "        std = torch.exp(self.log_std(hidden).view(-1, K, D))\n",
    "        mus = Normal(mean, std).sample((num_samples, )) ## S * B * K * D\n",
    "        return mean, std, mus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    enc = Encoder()\n",
    "    if CUDA:\n",
    "        enc.cuda()\n",
    "    optimizer =  torch.optim.Adam(list(enc.parameters()),lr=LEARNING_RATE)    \n",
    "    return enc, optimizer\n",
    "enc, optimizer = initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_joints_gmm(Z, Pi, mus, covs, Xs, N, D, K, num_samples, batch_size):\n",
    "    log_probs = torch.zeros((num_samples, batch_size)).float()\n",
    "    ## S * B\n",
    "    log_probs = log_probs + Normal(torch.zeros((batch_size, K, D)), torch.ones((batch_size, K, D))).log_prob(mus).sum(-1).sum(-1)\n",
    "    ## Z B-by-T-by-K\n",
    "    log_probs = log_probs + cat(Pi).log_prob(Z).sum(-1)\n",
    "    labels = Z.nonzero()\n",
    "    covs_expand = covs.unsqueeze(0).repeat(num_samples, 1, 1, 1)\n",
    "    log_probs = log_probs + Normal(mus[:, labels[:, 0], labels[:, -1], :].view(-1, batch_size, N, D), covs_expand[:, labels[:, 0], labels[:, -1], :].view(-1, batch_size, N, D)).log_prob(Xs).sum(-1).sum(-1)\n",
    "    return log_probs\n",
    "\n",
    "def conjugate_posterior(stat1, stat2, covs, K, D, batch_size):\n",
    "    prior_covs_inv = torch.ones(K, D)\n",
    "    posterior_covs = 1. / (prior_covs_inv + torch.mul(stat1.unsqueeze(-1).repeat(1, 1, D), 1. / covs))\n",
    "    posterior_mean = torch.mul(posterior_covs, torch.mul(stat2, 1. / covs))\n",
    "    return posterior_mean, posterior_covs\n",
    "\n",
    "def kl_normal_normal(p_mean, p_std, q_mean, q_std):\n",
    "    var_ratio = (p_std / q_std).pow(2)\n",
    "    t1 = ((p_mean - q_mean) / q_std).pow(2)\n",
    "    return 0.5 * (var_ratio + t1 - 1 - var_ratio.log())\n",
    "\n",
    "def kls_gaussians(weights, mus, mus_mean, mus_std, posterior_mean, posterior_covs, K, D):\n",
    "    log_q = Normal(mus_mean, mus_std).log_prob(mus).sum(-1).sum(-1)\n",
    "    log_p = Normal(posterior_mean, torch.sqrt(posterior_covs)).log_prob(mus).sum(-1).sum(-1)\n",
    "    MCKl_exclusive = (log_q - log_p).mean(0).mean()\n",
    "    TrueKl_exclusive = kl_normal_normal(mus_mean, mus_std, posterior_mean, torch.sqrt(posterior_covs)).mean()\n",
    "    \n",
    "    MCKl_inclusive = torch.mul(weights, log_p - log_q).sum(0).mean()\n",
    "    TrueKl_inclusive = kl_normal_normal(posterior_mean, torch.sqrt(posterior_covs), mus_mean, mus_std).mean()\n",
    "    return MCKl_inclusive, TrueKl_inclusive, MCKl_exclusive, TrueKl_exclusive\n",
    "    \n",
    "def rws(Xs, Zs, Pi, covs, N, K, D, num_samples, batch_size):\n",
    "    stat1, stat2, stats = StatsGMM(Xs, Zs, K, D)\n",
    "    data = torch.cat((Xs, Zs), dim=-1).view(batch_size*N, -1)\n",
    "    mus_mean, mus_std, mus = enc(data, K, D, covs, num_samples, batch_size)\n",
    "    log_q = Normal(mus_mean, mus_std).log_prob(mus).sum(-1).sum(-1) ## S * B\n",
    "#     print(log_q.sum())\n",
    "    log_p = log_joints_gmm(Zs, Pi, mus, covs, Xs, N, D, K, num_samples, batch_size)\n",
    "    log_weights = log_p - log_q\n",
    "    weights = torch.exp(log_weights - logsumexp(log_weights, dim=0)).detach()\n",
    "    eubo = torch.mul(weights, log_weights).sum(0).mean()\n",
    "    elbo = log_weights.mean(0).mean()\n",
    "    ess = (1. / (weights ** 2).sum(0)).mean()\n",
    "    posterior_mean, posterior_covs = conjugate_posterior(stat1, stat2, covs, K, D, batch_size)\n",
    "    MCKl_inclusive, TrueKl_inclusive, MCKl_exclusive, TrueKl_exclusive = kls_gaussians(weights, mus, mus_mean, mus_std, posterior_mean, posterior_covs, K, D)\n",
    "    return eubo, elbo, ess, MCKl_inclusive, TrueKl_inclusive, MCKl_exclusive, TrueKl_exclusive\n",
    "\n",
    "def shuffler(batch_Xs, batch_Zs, N, K, D, batch_size):\n",
    "    indices = torch.cat([torch.randperm(N).unsqueeze(0) for b in range(batch_size)])\n",
    "    indices_Xs = indices.unsqueeze(-1).repeat(1, 1, D)\n",
    "    indices_Zs = indices.unsqueeze(-1).repeat(1, 1, K)\n",
    "    return torch.gather(batch_Xs, 1, indices_Xs), torch.gather(batch_Zs, 1, indices_Zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EUBOs = []\n",
    "ELBOs = []\n",
    "ESSs = []\n",
    "MCKls_inclusive = []\n",
    "TrueKls_inclusive = []\n",
    "MCKls_exclusive = []\n",
    "TrueKls_exclusive = []\n",
    "\n",
    "num_batches = int((Xs.shape[0] / BATCH_SIZE))\n",
    "time_start = time.time()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    indices = torch.randperm(num_seqs)\n",
    "\n",
    "    EUBO = 0.0\n",
    "    ELBO = 0.0\n",
    "    ESS = 0.0\n",
    "    MCKl_inclusive = 0.0\n",
    "    TrueKl_inclusive = 0.0\n",
    "    MCKl_exclusive = 0.0\n",
    "    TrueKl_exclusive = 0.0\n",
    "    for step in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "        batch_indices = indices[step*BATCH_SIZE : (step+1)*BATCH_SIZE]\n",
    "        batch_Xs = Xs[batch_indices]\n",
    "        batch_Zs = Zs[batch_indices]\n",
    "        batch_Covs = covs[batch_indices]\n",
    "        batch_Xs, batch_Zs = shuffler(batch_Xs, batch_Zs, N, K, D, BATCH_SIZE)\n",
    "        eubo, elbo, ess, mckl_inclusive, truekl_inclusive, mckl_exclusive, truekl_exclusive = rws(batch_Xs, batch_Zs, Pi, batch_Covs, N, K, D, NUM_SAMPLES, BATCH_SIZE)\n",
    "        eubo.backward()\n",
    "        optimizer.step()\n",
    "        EUBO += eubo.item()\n",
    "        ELBO += elbo.item()\n",
    "        ESS += ess.item()\n",
    "        MCKl_inclusive += mckl_inclusive.item()\n",
    "        MCKl_exclusive += mckl_exclusive.item()\n",
    "        TrueKl_inclusive += truekl_inclusive.item()\n",
    "        TrueKl_exclusive += truekl_exclusive.item()\n",
    "        \n",
    "    EUBO /= num_batches\n",
    "    ELBO /= num_batches\n",
    "    ESS /= num_batches\n",
    "    MCKl_inclusive /= num_batches\n",
    "    TrueKl_inclusive /= num_batches\n",
    "    MCKl_exclusive /= num_batches\n",
    "    TrueKl_exclusive /= num_batches\n",
    "    \n",
    "    EUBOs.append(EUBO)\n",
    "    ELBOs.append(ELBO)\n",
    "    ESSs.append(ESS)\n",
    "    MCKls_inclusive.append(MCKl_inclusive)\n",
    "    TrueKls_inclusive.append(TrueKl_inclusive)\n",
    "    MCKls_exclusive.append(MCKl_exclusive)\n",
    "    TrueKls_exclusive.append(TrueKl_exclusive)\n",
    "    \n",
    "#     time_end = time.time()\n",
    "    if epoch % 10 == 0:\n",
    "        time_end = time.time()\n",
    "        print('epoch=%d, EUBO=%f, ELBO=%f, ESS=%.3f, inc MCKL=%f, inc TKL=%f, exc MCKL=%f, exc TKL=%f (%ds)' % (epoch, EUBO, ELBO, ESS, MCKl_inclusive, TrueKl_inclusive, MCKl_exclusive, TrueKl_exclusive, time_end - time_start))\n",
    "        time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(EUBOs, ELBOs, ESSs, MCKls_exclusive, TrueKls_exclusive, MCKls_inclusive, TrueKls_inclusive, num_samples, num_epochs, lr):\n",
    "    fig = plt.figure(figsize=(30, 30))\n",
    "    fig.tight_layout()\n",
    "    ax1 = fig.add_subplot(3, 1, 1)\n",
    "    ax2 = fig.add_subplot(3, 1, 2)\n",
    "    ax3 = fig.add_subplot(3, 1, 3)\n",
    "    ax1.plot(EUBOs, 'r', label='EUBOs')\n",
    "    ax1.plot(ELBOs, 'b', label='ELBOs')\n",
    "    ax2.plot(TrueKls_exclusive, '#66b3ff', label='true exclusive KL')\n",
    "    ax2.plot(MCKls_exclusive, '#ff9999', label='est exclusive KL')\n",
    "    ax2.plot(TrueKls_inclusive, '#99ff99', label='true inclusive KL')\n",
    "    ax2.plot(MCKls_inclusive, 'gold', label='est inclusive KL')\n",
    "    \n",
    "    ax1.tick_params(labelsize=18)\n",
    "    ax3.plot(np.array(ESSs) / num_samples, 'm', label='ESS')\n",
    "    ax1.set_title('epoch=%d, batch_size=%d, lr=%.1E, samples=%d' % (num_epochs, BATCH_SIZE, lr, num_samples), fontsize=18)\n",
    "    ax1.set_ylim([-300, -80])\n",
    "    ax1.legend()\n",
    "    ax2.set_ylim([-50, 50])\n",
    "    ax2.legend()\n",
    "    ax3.legend()\n",
    "    ax2.tick_params(labelsize=18)\n",
    "    ax3.tick_params(labelsize=18)\n",
    "    plt.savefig('encode_stats_sigmas_lr=%.1E_samples=%d.svg' % (lr, num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(EUBOs, ELBOs, ESSs, MCKls_exclusive, TrueKls_exclusive, MCKls_inclusive, TrueKls_inclusive, NUM_SAMPLES, NUM_EPOCHS, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "\n",
    "indices = torch.randperm(num_seqs)\n",
    "batch_indices = indices[0*BATCH_SIZE : (0+1)*BATCH_SIZE]\n",
    "batch_Xs = Xs[batch_indices]\n",
    "batch_Zs = Zs[batch_indices]\n",
    "batch_Covs = covs[batch_indices]\n",
    "batch_Mus = mus_true[batch_indices]\n",
    "batch_Xs, batch_Zs = shuffler(batch_Xs, batch_Zs, N, K, D, BATCH_SIZE)\n",
    "\n",
    "data = torch.cat((batch_Xs, batch_Zs), dim=-1).view(BATCH_SIZE*N, -1)\n",
    "mus_mean, mus_std, mus = enc(data, K, D, batch_Covs, 10, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(Xs, mus_true1, covs_true1, mus_pred):\n",
    "    Xs = Xs.data.numpy()\n",
    "    mus_true1 = mus_true1.data.numpy()\n",
    "    covs_true1 = covs_true1.data.numpy()\n",
    "    \n",
    "    fig = plt.figure(figsize=(40, 80))\n",
    "    for i in range(10):\n",
    "        for j in range(5):\n",
    "            batch_covs_expand = torch.zeros((K, D, D))\n",
    "            for k in range(K):\n",
    "                batch_covs_expand[k] = torch.diag(batch_Covs[i*5+j][k])\n",
    "            mus_flat = mus_pred[:, i*5+j].contiguous().view(10*K, D).data.numpy()\n",
    "            ax = fig.add_subplot(10, 5, i*5+j+1)\n",
    "            ax.plot(Xs[i*5+j][:,0], Xs[i*5+j][:,1], 'ro')\n",
    "            ax.plot(mus_flat[:, 0], mus_flat[:, 1], 'ko')\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            plot_cov_ellipse(cov=batch_covs_expand[0], pos=mus_true1[i*5+j, 0], nstd=2, ax=ax, alpha=0.5)\n",
    "            plot_cov_ellipse(cov=batch_covs_expand[1], pos=mus_true1[i*5+j, 1], nstd=2, ax=ax, alpha=0.5)\n",
    "            plot_cov_ellipse(cov=batch_covs_expand[2], pos=mus_true1[i*5+j, 2], nstd=2, ax=ax, alpha=0.5)\n",
    "            ax.set_ylim([-10, 10])\n",
    "            ax.set_xlim([-10, 10])\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('encode_stats_sigmas_predictions_only_learn_mus.svg')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_predictions(batch_Xs, batch_Mus, batch_Covs, mus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(enc.state_dict(), 'models/enc-encode_stats_sigmas-only-learn-mus-samples=10-iters=1e6-lr=1e-4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(EUBOs, ELBOs, ESSs, MCKls_exclusive, TrueKls_exclusive, MCKls_inclusive, TrueKls_inclusive, NUM_SAMPLES, NUM_EPOCHS, LEARNING_RATE):\n",
    "    fout = open('encode_stats_sigmas_only_learn_mus_samples=%d_epochs=%d_lr=%1e-4.txt', 'w+')\n",
    "    fout.write('EUBOs, ELBOs, ESSs, MCKls_exclusive, TrueKls_exclusive, MCKls_inclusive, TrueKls_inclusive\\n')\n",
    "    for i in range(len(EUBOs)):\n",
    "        fout.write(str(EUBOs[i]) + ', ' + str(ELBOs[i]) + ', ' + str(ESSs[i]) + ', ' + str(MCKls_exclusive[i]) + ', ' + str(TrueKls_exclusive[i]) + ', ' + str(MCKls_inclusive[i]) + ', ' + str(TrueKls_inclusive[i]) + '\\n')\n",
    "    fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(EUBOs, ELBOs, ESSs, MCKls_exclusive, TrueKls_exclusive, MCKls_inclusive, TrueKls_inclusive, NUM_SAMPLES, NUM_EPOCHS, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "fin = open('results/OneShotEncoder_only_learn_mus_samples=%d_epochs=%d_lr=%1e-4.txt')\n",
    "EUBOs0 = []\n",
    "ELBOs0 = []\n",
    "ESSs0 = []\n",
    "MCKl_exclusive0 = []\n",
    "MCKl_inclusive0 = []\n",
    "TrueKl_exclusive0 = []\n",
    "TrueKl_inclusive0 = []\n",
    "\n",
    "for line in fin.readlines():\n",
    "    line = line.strip().split(', ')\n",
    "    if line[0] != 'EUBOs':\n",
    "        EUBOs0.append(float(line[0]))\n",
    "        ELBOs0.append(float(line[1]))\n",
    "        ESSs0.append(float(line[2]))\n",
    "        MCKl_exclusive0.append(line[3])\n",
    "        TrueKl_exclusive0.append(line[4])\n",
    "        MCKl_inclusive0.append(line[5])\n",
    "        TrueKl_inclusive0.append(line[6])\n",
    "fin.close()\n",
    "\n",
    "\n",
    "fin2 = open('results/encode_stats_sigmas_only_learn_mus_samples=%d_epochs=%d_lr=%1e-4.txt')\n",
    "EUBOs = []\n",
    "ELBOs = []\n",
    "ESSs = []\n",
    "MCKl_exclusive = []\n",
    "MCKl_inclusive = []\n",
    "TrueKl_exclusive = []\n",
    "TrueKl_inclusive = []\n",
    "\n",
    "for line in fin2.readlines():\n",
    "    line = line.strip().split(', ')\n",
    "    if line[0] != 'EUBOs':\n",
    "        EUBOs.append(float(line[0]))\n",
    "        ELBOs.append(float(line[1]))\n",
    "        ESSs.append(float(line[2]))\n",
    "        MCKl_exclusive.append(line[3])\n",
    "        TrueKl_exclusive.append(line[4])\n",
    "        MCKl_inclusive.append(line[5])\n",
    "        TrueKl_inclusive.append(line[6])\n",
    "fin2.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30, 30))\n",
    "fig.tight_layout()\n",
    "ax1 = fig.add_subplot(2, 1, 1)\n",
    "ax3 = fig.add_subplot(2, 1, 2)\n",
    "ax1.plot(EUBOs, 'r', label='EUBOs -- encode sigmas')\n",
    "ax1.plot(ELBOs, 'b', label='ELBOs -- encode sigmas')\n",
    "ax1.plot(EUBOs0, 'g', label='EUBOs ')\n",
    "ax1.plot(ELBOs0, 'k', label='ELBOs')\n",
    "\n",
    "\n",
    "ax1.tick_params(labelsize=18)\n",
    "# ax3.plot(np.array(ESSs) / NUM_SAMPLES, 'm', label='ESS -- encode sigmas')\n",
    "# ax3.plot(np.array(ESSs0) / NUM_SAMPLES, 'k', label='ESS')\n",
    "\n",
    "ax1.set_title('epoch=%d, batch_size=%d, lr=%.1E, samples=%d' % (NUM_EPOCHS, BATCH_SIZE, LEARNING_RATE, NUM_SAMPLES), fontsize=18)\n",
    "ax1.set_ylim([-300, -80])\n",
    "ax1.legend()\n",
    "# ax3.legend()\n",
    "# ax3.tick_params(labelsize=18)\n",
    "# plt.savefig('encode_stats_sigmas_lr=%.1E_samples=%d.svg' % (lr, num_samples))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(ESSs0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
