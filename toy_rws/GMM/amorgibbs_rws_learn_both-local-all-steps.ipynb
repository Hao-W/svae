{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probtorch: 0.0+5a2c637 torch: 1.0.0 cuda: True\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from plots import *\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal as mvn\n",
    "from torch.distributions.one_hot_categorical import OneHotCategorical as cat\n",
    "from torch.distributions.gamma import Gamma\n",
    "from torch import logsumexp\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "sys.path.append('/home/hao/Research/probtorch/')\n",
    "import probtorch\n",
    "print('probtorch:', probtorch.__version__, \n",
    "      'torch:', torch.__version__, \n",
    "      'cuda:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "K = 3\n",
    "D = 2\n",
    "\n",
    "## Model Parameters\n",
    "NUM_SAMPLES = 10\n",
    "NUM_HIDDEN = 64\n",
    "STEPS = 10\n",
    "NUM_STATS = K+D*K+D*K\n",
    "NUM_LATENTS = D * K\n",
    "NUM_OBS_GLOBAL = D + K\n",
    "NUM_OBS_LOCAL = D + K*D + K*D\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = torch.from_numpy(np.load('gmm_dataset/sequences.npy')).float()\n",
    "Zs_true = torch.from_numpy(np.load('gmm_dataset/states.npy')).float()\n",
    "mus_true = torch.from_numpy(np.load('gmm_dataset/means.npy')).float()\n",
    "sigma2_true = torch.from_numpy(np.load('gmm_dataset/covariances.npy')).float()\n",
    "Pi = torch.from_numpy(np.load('gmm_dataset/init.npy')).float()\n",
    "num_seqs = Xs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_global(nn.Module):\n",
    "    def __init__(self, num_obs=NUM_OBS_GLOBAL,\n",
    "                       num_stats=NUM_STATS,\n",
    "                       num_hidden=NUM_HIDDEN,\n",
    "                       num_latents=NUM_LATENTS):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.enc_stats = nn.Sequential(\n",
    "            nn.Linear(num_obs, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_stats))\n",
    "        self.enc_hidden = nn.Sequential(\n",
    "            nn.Linear(num_stats, num_hidden),\n",
    "            nn.ReLU())\n",
    "        self.sigmas_log_alpha = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        self.sigmas_log_beta = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        \n",
    "        self.enc_hidden2 = nn.Sequential(\n",
    "            nn.Linear(num_stats, num_hidden),\n",
    "            nn.ReLU())\n",
    "        self.mus_mean = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        self.mus_log_std = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        \n",
    "    def forward(self, obs, K, D, num_samples, batch_size):\n",
    "        stats = self.enc_stats(obs).view(batch_size, N, -1).sum(1)\n",
    "        hidden = self.enc_hidden(stats)\n",
    "        alpha = torch.exp(self.sigmas_log_alpha(hidden)).view(-1, K, D) ## B * K * D\n",
    "        beta = torch.exp(self.sigmas_log_beta(hidden)).view(-1, K, D) ## B * K * D\n",
    "        precisions = Gamma(alpha, beta).sample((num_samples,)) ## S * B * K * D\n",
    "        \n",
    "        hidden2 = self.enc_hidden2(stats)                 \n",
    "        mus_mean = self.mus_mean(hidden2).view(-1, K, D)\n",
    "        mus_sigma = torch.exp(self.mus_log_std(hidden2).view(-1, K, D))\n",
    "        mus = Normal(mus_mean, mus_sigma).sample((num_samples,))  \n",
    "        return alpha, beta, precisions, mus_mean, mus_sigma, mus\n",
    "    \n",
    "class Encoder_local(nn.Module):\n",
    "    def __init__(self, num_obs=NUM_OBS_LOCAL,\n",
    "                       num_hidden=NUM_HIDDEN,\n",
    "                       num_latents=K):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.enc_onehot = nn.Sequential(\n",
    "            nn.Linear(num_obs, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_latents),\n",
    "            nn.Softmax(-1))\n",
    "        \n",
    "    def forward(self, obs, N, K, D, num_samples, batch_size):\n",
    "        zs_pi = self.enc_onehot(obs).view(batch_size, N, K)\n",
    "        zs = cat(zs_pi).sample((num_samples,))\n",
    "        log_qz = cat(zs_pi).log_prob(zs).view(num_samples, batch_size, -1).sum(-1) ## S * B\n",
    "        zs = zs.view(num_samples, batch_size, -1, K) ## S * B * N * K\n",
    "        return zs_pi, zs, log_qz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 1e-2)\n",
    "        \n",
    "        \n",
    "def initialize():\n",
    "    enc_global = Encoder_global()\n",
    "    enc_local = Encoder_local()\n",
    "    enc_global.apply(weights_init)\n",
    "\n",
    "    optimizer =  torch.optim.Adam(list(enc_global.parameters()) + list(enc_local.parameters()),lr=LEARNING_RATE, betas=(0.9, 0.99))    \n",
    "    return enc_global, enc_local, optimizer\n",
    "enc_global, enc_local, optimizer = initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_mean = torch.zeros((BATCH_SIZE, K, D))\n",
    "prior_sigma = torch.ones((BATCH_SIZE, K, D))\n",
    "prior_alpha = torch.ones((BATCH_SIZE, K, D)) * 2.0\n",
    "prior_beta = torch.ones((BATCH_SIZE, K, D)) * 2.0\n",
    "\n",
    "def log_joints_gmm(X, Z, Pi, mus, precisions, N, D, K, prior_mean, prior_sigma, prior_alpha, prior_beta, batch_size):\n",
    "    log_probs = torch.zeros(batch_size).float()\n",
    "    ## priors on mus and sigmas, S * B\n",
    "    log_probs = log_probs + Normal(prior_mean, prior_sigma).log_prob(mus).sum(-1).sum(-1)\n",
    "    log_probs = log_probs + Gamma(prior_alpha, prior_beta).log_prob(precisions).sum(-1).sum(-1)\n",
    "    ## Z B-by-T-by-K\n",
    "    log_probs = log_probs + cat(Pi).log_prob(Z).sum(-1)\n",
    "    labels = Z.nonzero()\n",
    "    sigmas = 1. / torch.sqrt(precisions)\n",
    "    log_probs = log_probs + Normal(mus[labels[:, 0], labels[:, -1], :].view(batch_size, N, D), \n",
    "                                   sigmas[labels[:, 0], labels[:, -1], :].view(batch_size, N, D)).log_prob(X).sum(-1).sum(-1)\n",
    "    return log_probs\n",
    "\n",
    "def inti_global(K, D, prior_mean, prior_sigma, prior_alpha, prior_beta, batch_size):\n",
    "    mus = Normal(prior_mean, prior_sigma).sample()\n",
    "    precisions = Gamma(prior_alpha, prior_beta).sample()\n",
    "    ## log prior size B\n",
    "    log_p =  Normal(prior_mean, prior_sigma).log_prob(mus).sum(-1).sum(-1) + Gamma(prior_alpha, prior_beta).log_prob(precisions).sum(-1).sum(-1)\n",
    "    return mus, precisions, log_p\n",
    "\n",
    "def E_step(X, mus, precisions, N, D, K, batch_size):\n",
    "    mus_flat = mus.view(-1, K*D).unsqueeze(1).repeat(1, N, 1)\n",
    "    covs = 1. / torch.sqrt(precisions)\n",
    "    covs_flat = covs.view(-1, K*D).unsqueeze(1).repeat(1, N, 1)\n",
    "    data = torch.cat((X, mus_flat, covs_flat), -1).view(batch_size*N, -1)\n",
    "    zs_pi, zs, log_q_z = enc_local(data, N, K, D, 1, batch_size)\n",
    "    return zs_pi, zs[0], log_q_z[0]\n",
    "\n",
    "def M_step(X, z, N, D, K, batch_size):\n",
    "    data = torch.cat((X, z), dim=-1).view(batch_size*N, -1)\n",
    "    alpha, beta, precisions, mus_mean, mus_sigma, mus = enc_global(data, K, D, 1, batch_size)            \n",
    "    log_q_eta =  Normal(mus_mean, mus_sigma).log_prob(mus[0]).sum(-1).sum(-1) + Gamma(alpha, beta).log_prob(precisions[0]).sum(-1).sum(-1)## B\n",
    "    return mus[0], precisions[0], log_q_eta, alpha, beta, mus_mean, mus_sigma\n",
    "\n",
    "\n",
    "def conjugate_posterior(Xs, Zs, prior_mean, prior_sigma, prior_alpha, prior_beta, mus_true, sigma2_true, N, K, D, batch_size):\n",
    "    stat1 = Zs.sum(1).unsqueeze(-1).repeat(1, 1, D) ## B * K * D\n",
    "    stat2 = torch.mul(Zs.unsqueeze(-1).repeat(1, 1, 1, D), Xs.unsqueeze(-1).repeat(1, 1, 1, K).transpose(-1, -2)).sum(1) ## B*K*D\n",
    "    stat3 = torch.mul(Zs.unsqueeze(-1).repeat(1, 1, 1, D), torch.mul(Xs, Xs).unsqueeze(-1).repeat(1, 1, 1, K).transpose(-1, -2)).sum(1) \n",
    "    ## p(\\mu_k | x, z, \\sigma2_k_true)\n",
    "    posterior_sigma2 =  1. / (1. / (prior_sigma ** 2) + stat1 / (sigma2_true))\n",
    "    posterior_mean = (prior_mean / (prior_sigma ** 2) + stat2 / sigma2_true) * posterior_sigma2\n",
    "    ## p(\\sigma2_k | x, z, \\mu_k_true)\n",
    "    posterior_alpha = prior_alpha + (stat1 / 2)\n",
    "    posterior_beta = prior_beta + (stat3 + stat1 * (mus_true ** 2) - 2 * mus_true * stat2) / 2.\n",
    "    return posterior_mean, posterior_sigma2, posterior_alpha, posterior_beta\n",
    "\n",
    "def kl_normal_normal(p_mean, p_std, q_mean, q_std):\n",
    "    var_ratio = (p_std / q_std).pow(2)\n",
    "    t1 = ((p_mean - q_mean) / q_std).pow(2)\n",
    "    return 0.5 * (var_ratio + t1 - 1 - var_ratio.log())\n",
    "\n",
    "def kls_gaussians(mus_mean, mus_sigma, posterior_mean, posterior_sigma2):\n",
    "    Kl_exclusive = kl_normal_normal(mus_mean, mus_sigma, posterior_mean, torch.sqrt(posterior_sigma2)).mean()\n",
    "    Kl_inclusive = kl_normal_normal(posterior_mean, torch.sqrt(posterior_sigma2), mus_mean, mus_sigma).mean()\n",
    "    return Kl_exclusive, Kl_inclusive\n",
    "\n",
    "def kl_gamma_gamma(p_alpha, p_beta, q_alpha, q_beta):\n",
    "    t1 = q_alpha * (p_beta / q_beta).log()\n",
    "    t2 = torch.lgamma(q_alpha) - torch.lgamma(p_alpha)\n",
    "    t3 = (p_alpha - q_alpha) * torch.digamma(p_alpha)\n",
    "    t4 = (q_beta - p_beta) * (p_alpha / p_beta)\n",
    "    return t1 + t2 + t3 + t4\n",
    "def kls_gammas(precsions_alpha, precisions_beta, posterior_alpha, posterior_beta):\n",
    "    KL_exclusive = kl_gamma_gamma(precsions_alpha, precisions_beta, posterior_alpha, posterior_beta).mean()\n",
    "    KL_inclusive = kl_gamma_gamma(posterior_alpha, posterior_beta, precsions_alpha, precisions_beta).mean()\n",
    "    return KL_exclusive, KL_inclusive\n",
    "    \n",
    "def rws(Xs, Pi, Zs_true, mus_true, sigma2_true, N, K, D, num_samples, steps, batch_size):\n",
    "    \"\"\"\n",
    "    train both encoders\n",
    "    rws gradient estimator\n",
    "    sis sampling scheme\n",
    "    no resampling\n",
    "    \"\"\"\n",
    "    posterior_mean, posterior_sigma2, posterior_alpha, posterior_beta = conjugate_posterior(Xs, Zs_true, prior_mean, prior_sigma, prior_alpha, prior_beta, mus_true, sigma2_true, N, K, D, batch_size)\n",
    "    log_increment_weights = torch.zeros((steps, num_samples, batch_size))\n",
    "    log_uptonow_weights = torch.zeros((steps, num_samples, batch_size))\n",
    "    Z_samples = torch.zeros((num_samples, batch_size, N, K))\n",
    "    for m in range(steps):\n",
    "        if m == 0:\n",
    "            for l in range(num_samples):\n",
    "                mus, precisions, log_p_eta = inti_global(K, D, prior_mean, prior_sigma, prior_alpha, prior_beta, batch_size)\n",
    "                zs_pi, z, log_q_z = E_step(Xs, mus, precisions, N, D, K, batch_size)\n",
    "                Z_samples[l] = z\n",
    "                labels = z.nonzero()\n",
    "                log_p_z = cat(Pi).log_prob(z).sum(-1)\n",
    "                sigmas = 1. / torch.sqrt(precisions)\n",
    "                log_p_x = Normal(mus[labels[:, 0], labels[:, -1], :].view(batch_size, N, D), sigmas[labels[:, 0], labels[:, -1], :].view(batch_size, N, D)).log_prob(Xs).sum(-1).sum(-1)\n",
    "                log_increment_weights[m, l] = log_p_x + log_p_z - log_q_z     \n",
    "                log_uptonow_weights[m, l] = log_p_x + log_p_z - log_q_z       \n",
    "        else:\n",
    "            for l in range(num_samples):\n",
    "                z_prev = Z_samples[l]\n",
    "                mus, precisions, log_q_eta, alpha, beta, mus_mean, mus_sigma = M_step(Xs, z_prev, N, D, K, batch_size)\n",
    "                zs_pi, z, log_q_z = E_step(Xs, mus, precisions, N, D, K, batch_size)\n",
    "                Z_samples[l] = z\n",
    "                log_p_joint = log_joints_gmm(Xs, z, Pi, mus, precisions, N, D, K, prior_mean, prior_sigma, prior_alpha, prior_beta, batch_size)\n",
    "                log_increment_weights[m, l] = log_p_joint - log_q_z - log_q_eta\n",
    "#                 log_uptonow_weights[m ,l] = log_increment_weights[m, l] + log_uptonow_weights[m-1 ,l]\n",
    "\n",
    "    increment_weights = torch.exp(log_increment_weights - logsumexp(log_increment_weights, 1).unsqueeze(1).repeat(1, num_samples, 1)).detach()\n",
    "    ess = (1./ (increment_weights ** 2).sum(1)).mean(0).mean()\n",
    "    \n",
    "    euboall = torch.mul(increment_weights, log_increment_weights).sum(1).mean(0).mean()\n",
    "    elboall = log_increment_weights.mean(1).mean(0).mean()\n",
    "    kl_mus_ex, kl_mus_in = kls_gaussians(mus_mean, mus_sigma, posterior_mean, posterior_sigma2)\n",
    "    kl_precisions_ex, kl_precisions_in = kls_gammas(alpha, beta, posterior_alpha, posterior_beta)\n",
    "    return euboall, elboall, ess, kl_mus_ex, kl_mus_in, kl_precisions_ex, kl_precisions_in\n",
    "\n",
    "def shuffler(batch_Xs, batch_Zs, N, K, D, batch_size):\n",
    "    indices = torch.cat([torch.randperm(N).unsqueeze(0) for b in range(batch_size)])\n",
    "    indices_Xs = indices.unsqueeze(-1).repeat(1, 1, D)\n",
    "    indices_Zs = indices.unsqueeze(-1).repeat(1, 1, K)\n",
    "    return torch.gather(batch_Xs, 1, indices_Xs), torch.gather(batch_Zs, 1, indices_Zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, EUBO=-335.176, ELBO=-643.973, ESS=1.020, KL_MU_EX=.395.819007, KL_MU_IN=8.175, KL_PC_EX=3.928, KL_PC_IN=0.791 (6s)\n",
      "epoch=10, EUBO=-239.932, ELBO=-395.362, ESS=1.054, KL_MU_EX=.395.337586, KL_MU_IN=10.352, KL_PC_EX=6.921, KL_PC_IN=1.389 (6s)\n",
      "epoch=20, EUBO=-217.238, ELBO=-285.422, ESS=1.126, KL_MU_EX=.396.943596, KL_MU_IN=11.255, KL_PC_EX=12.853, KL_PC_IN=4.009 (6s)\n",
      "epoch=30, EUBO=-208.371, ELBO=-246.623, ESS=1.339, KL_MU_EX=.397.062206, KL_MU_IN=9.038, KL_PC_EX=11.411, KL_PC_IN=17.647 (5s)\n",
      "epoch=40, EUBO=-207.295, ELBO=-240.581, ESS=1.487, KL_MU_EX=.396.748728, KL_MU_IN=7.829, KL_PC_EX=11.150, KL_PC_IN=49.262 (5s)\n",
      "epoch=50, EUBO=-207.068, ELBO=-239.513, ESS=1.514, KL_MU_EX=.396.430090, KL_MU_IN=7.592, KL_PC_EX=11.161, KL_PC_IN=52.209 (5s)\n",
      "epoch=60, EUBO=-206.533, ELBO=-238.620, ESS=1.526, KL_MU_EX=.396.337200, KL_MU_IN=7.394, KL_PC_EX=11.089, KL_PC_IN=53.192 (5s)\n",
      "epoch=70, EUBO=-205.496, ELBO=-237.836, ESS=1.539, KL_MU_EX=.396.215945, KL_MU_IN=7.098, KL_PC_EX=11.172, KL_PC_IN=54.856 (5s)\n",
      "epoch=80, EUBO=-204.951, ELBO=-237.163, ESS=1.541, KL_MU_EX=.394.557319, KL_MU_IN=6.901, KL_PC_EX=10.933, KL_PC_IN=53.780 (5s)\n",
      "epoch=90, EUBO=-203.429, ELBO=-234.660, ESS=1.599, KL_MU_EX=.387.901210, KL_MU_IN=6.840, KL_PC_EX=10.484, KL_PC_IN=49.148 (5s)\n",
      "epoch=100, EUBO=-202.606, ELBO=-233.253, ESS=1.621, KL_MU_EX=.382.058665, KL_MU_IN=6.872, KL_PC_EX=10.179, KL_PC_IN=43.677 (5s)\n",
      "epoch=110, EUBO=-201.593, ELBO=-231.429, ESS=1.667, KL_MU_EX=.378.931232, KL_MU_IN=6.855, KL_PC_EX=10.067, KL_PC_IN=43.730 (5s)\n",
      "epoch=120, EUBO=-200.287, ELBO=-229.622, ESS=1.755, KL_MU_EX=.372.085722, KL_MU_IN=7.127, KL_PC_EX=9.649, KL_PC_IN=38.760 (8s)\n",
      "epoch=130, EUBO=-199.275, ELBO=-228.955, ESS=1.723, KL_MU_EX=.369.148342, KL_MU_IN=6.957, KL_PC_EX=9.295, KL_PC_IN=33.753 (8s)\n",
      "epoch=140, EUBO=-198.873, ELBO=-228.382, ESS=1.747, KL_MU_EX=.368.983125, KL_MU_IN=6.841, KL_PC_EX=9.342, KL_PC_IN=34.202 (8s)\n",
      "epoch=150, EUBO=-198.828, ELBO=-228.342, ESS=1.759, KL_MU_EX=.368.981732, KL_MU_IN=6.731, KL_PC_EX=9.302, KL_PC_IN=34.328 (8s)\n",
      "epoch=160, EUBO=-198.253, ELBO=-227.641, ESS=1.731, KL_MU_EX=.369.120454, KL_MU_IN=6.400, KL_PC_EX=9.057, KL_PC_IN=32.770 (8s)\n",
      "epoch=170, EUBO=-197.946, ELBO=-227.230, ESS=1.749, KL_MU_EX=.368.992392, KL_MU_IN=6.453, KL_PC_EX=9.202, KL_PC_IN=33.519 (8s)\n",
      "epoch=180, EUBO=-197.786, ELBO=-226.934, ESS=1.749, KL_MU_EX=.369.157654, KL_MU_IN=6.174, KL_PC_EX=9.134, KL_PC_IN=32.951 (8s)\n",
      "epoch=190, EUBO=-197.173, ELBO=-226.082, ESS=1.740, KL_MU_EX=.369.006089, KL_MU_IN=6.113, KL_PC_EX=9.036, KL_PC_IN=32.728 (8s)\n",
      "epoch=200, EUBO=-196.838, ELBO=-226.093, ESS=1.735, KL_MU_EX=.368.983769, KL_MU_IN=6.010, KL_PC_EX=8.937, KL_PC_IN=31.863 (8s)\n",
      "epoch=210, EUBO=-196.733, ELBO=-226.012, ESS=1.730, KL_MU_EX=.368.978328, KL_MU_IN=5.946, KL_PC_EX=8.802, KL_PC_IN=31.603 (8s)\n",
      "epoch=220, EUBO=-196.576, ELBO=-225.365, ESS=1.708, KL_MU_EX=.368.876035, KL_MU_IN=5.887, KL_PC_EX=8.757, KL_PC_IN=31.116 (8s)\n",
      "epoch=230, EUBO=-196.460, ELBO=-225.227, ESS=1.738, KL_MU_EX=.368.862666, KL_MU_IN=5.771, KL_PC_EX=8.866, KL_PC_IN=32.549 (9s)\n",
      "epoch=240, EUBO=-195.617, ELBO=-224.805, ESS=1.716, KL_MU_EX=.368.867247, KL_MU_IN=5.726, KL_PC_EX=8.771, KL_PC_IN=30.954 (53s)\n",
      "epoch=250, EUBO=-195.616, ELBO=-224.545, ESS=1.727, KL_MU_EX=.368.917562, KL_MU_IN=5.595, KL_PC_EX=8.764, KL_PC_IN=31.494 (7s)\n",
      "epoch=260, EUBO=-195.410, ELBO=-224.500, ESS=1.718, KL_MU_EX=.368.672653, KL_MU_IN=5.716, KL_PC_EX=8.636, KL_PC_IN=30.196 (5s)\n",
      "epoch=270, EUBO=-195.330, ELBO=-224.143, ESS=1.688, KL_MU_EX=.368.821156, KL_MU_IN=5.488, KL_PC_EX=8.621, KL_PC_IN=29.805 (5s)\n",
      "epoch=280, EUBO=-195.159, ELBO=-223.738, ESS=1.698, KL_MU_EX=.368.949197, KL_MU_IN=5.397, KL_PC_EX=8.513, KL_PC_IN=29.259 (5s)\n",
      "epoch=290, EUBO=-194.339, ELBO=-223.121, ESS=1.715, KL_MU_EX=.368.973080, KL_MU_IN=5.316, KL_PC_EX=8.508, KL_PC_IN=29.502 (5s)\n",
      "epoch=300, EUBO=-194.422, ELBO=-222.785, ESS=1.741, KL_MU_EX=.368.874068, KL_MU_IN=5.329, KL_PC_EX=8.342, KL_PC_IN=28.894 (5s)\n",
      "epoch=310, EUBO=-193.532, ELBO=-221.540, ESS=1.771, KL_MU_EX=.368.752775, KL_MU_IN=5.365, KL_PC_EX=8.341, KL_PC_IN=30.400 (6s)\n",
      "epoch=320, EUBO=-193.266, ELBO=-221.288, ESS=1.802, KL_MU_EX=.368.804819, KL_MU_IN=5.297, KL_PC_EX=8.168, KL_PC_IN=30.936 (5s)\n",
      "epoch=330, EUBO=-192.680, ELBO=-220.534, ESS=1.822, KL_MU_EX=.368.593566, KL_MU_IN=5.336, KL_PC_EX=8.245, KL_PC_IN=32.683 (5s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-0911d6e73db8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mbatch_sigma2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msigma2_true\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mbatch_Xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_Zs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_Xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_Zs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0meubo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melbo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_mus_ex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_mus_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_precisions_ex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkl_precisions_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrws\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_Xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_Zs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_mus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sigma2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_SAMPLES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSTEPS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0meubo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7d344f0238b0>\u001b[0m in \u001b[0;36mrws\u001b[0;34m(Xs, Pi, Zs_true, mus_true, sigma2_true, N, K, D, num_samples, steps, batch_size)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0mz_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mZ_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mmus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecisions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_q_eta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmus_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmus_sigma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mM_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m                 \u001b[0mzs_pi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_q_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mE_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecisions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0mZ_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mlog_p_joint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_joints_gmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecisions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_sigma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_alpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprior_beta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-7d344f0238b0>\u001b[0m in \u001b[0;36mE_step\u001b[0;34m(X, mus, precisions, N, D, K, batch_size)\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mcovs_flat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcovs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepeat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmus_flat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcovs_flat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0mzs_pi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_q_z\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menc_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mzs_pi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_q_z\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dev/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-693b262a827d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, obs, N, K, D, num_samples, batch_size)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mzs_pi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menc_onehot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mzs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzs_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mlog_qz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzs_pi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## S * B\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dev/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dev/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dev/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dev/lib/python3.6/site-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    863\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dev/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1186\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EUBOs = []\n",
    "ELBOs = []\n",
    "ESSs = []\n",
    "KLs_mus_ex = []\n",
    "KLs_mus_in = []\n",
    "KLs_precisions_ex = []\n",
    "KLs_precisions_in = []\n",
    "\n",
    "num_batches = int((Xs.shape[0] / BATCH_SIZE))\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    time_start = time.time()\n",
    "    indices = torch.randperm(num_seqs)\n",
    "    EUBO = 0.0\n",
    "    ELBO = 0.0\n",
    "    ESS = 0.0\n",
    "    KL_mus_ex = 0.0\n",
    "    KL_mus_in = 0.0\n",
    "    KL_precisions_ex = 0.0\n",
    "    KL_precisions_in = 0.0\n",
    "    for step in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "        batch_indices = indices[step*BATCH_SIZE : (step+1)*BATCH_SIZE]\n",
    "        batch_Xs = Xs[batch_indices]\n",
    "        batch_Zs = Zs_true[batch_indices]\n",
    "        batch_mus = mus_true[batch_indices]\n",
    "        batch_sigma2 = sigma2_true[batch_indices]\n",
    "        batch_Xs, batch_Zs = shuffler(batch_Xs, batch_Zs, N, K, D, BATCH_SIZE)\n",
    "        eubo, elbo, ess, kl_mus_ex, kl_mus_in, kl_precisions_ex, kl_precisions_in = rws(batch_Xs, Pi, batch_Zs, batch_mus, batch_sigma2, N, K, D, NUM_SAMPLES, STEPS, BATCH_SIZE)\n",
    "        eubo.backward()\n",
    "        optimizer.step()\n",
    "        EUBO += eubo.item()\n",
    "        ELBO += elbo.item()\n",
    "        ESS += ess.item()\n",
    "        KL_mus_ex += kl_mus_ex.item()\n",
    "        KL_mus_in += kl_mus_in.item()\n",
    "        KL_precisions_ex += kl_precisions_ex.item()\n",
    "        KL_precisions_in += kl_precisions_in.item()\n",
    "    EUBO /= num_batches\n",
    "    ELBO /= num_batches\n",
    "    ESS /= num_batches\n",
    "    KL_mus_ex /= num_batches\n",
    "    KL_mus_in /= num_batches\n",
    "    KL_precisions_ex /= num_batches\n",
    "    KL_precisions_in /= num_batches\n",
    "    EUBOs.append(EUBO)\n",
    "    ELBOs.append(ELBO)\n",
    "    ESSs.append(ESS)\n",
    "    KLs_mus_ex.append(KL_mus_ex)\n",
    "    KLs_mus_in.append(KL_mus_in)\n",
    "    KLs_precisions_ex.append(KL_precisions_ex)\n",
    "    KLs_precisions_in.append(KL_precisions_in)\n",
    "\n",
    "    time_end = time.time()\n",
    "    if epoch % 10 == 0:\n",
    "        print('epoch=%d, EUBO=%.3f, ELBO=%.3f, ESS=%.3f, KL_MU_EX=.%3f, KL_MU_IN=%.3f, KL_PC_EX=%.3f, KL_PC_IN=%.3f (%ds)' % (epoch, EUBO, ELBO, ESS,  KL_mus_ex, KL_mus_in, KL_precisions_ex, KL_precisions_in , time_end - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(EUBOs, ELBOs, ESSs, NUM_SAMPLES, NUM_EPOCHS, LEARNING_RATE):\n",
    "    fout = open('local_amorgibbs-steps=%d-samples=%d-lr=%d.txt' % (STEPS, NUM_SAMPLES, LEARNING_RATE), 'w+')\n",
    "    fout.write('EUBOs, ELBOs, ESSs\\n')\n",
    "    for i in range(len(EUBOs)):\n",
    "        fout.write(str(EUBOs[i]) + ', ' + str(ELBOs[i]) + ', ' + str(ESSs[i]) + '\\n')\n",
    "    fout.close()\n",
    "torch.save(enc.state_dict(), 'models/local_amorgibbs-steps=%d-samples=%d-lr=%d' % (STEPS, NUM_SAMPLES, LEARNING_RATE))\n",
    "save_results(EUBOs, ELBOs, ESSs, NUM_SAMPLES, NUM_EPOCHS, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(EUBOs, ELBOs, ESSs, num_samples, num_epochs, lr):\n",
    "    fig = plt.figure(figsize=(30, 30))\n",
    "    fig.tight_layout()\n",
    "    ax1 = fig.add_subplot(2, 1, 1)\n",
    "    ax2 = fig.add_subplot(2, 1, 2)\n",
    "    ax1.plot(EUBOs, 'r', label='EUBOs')\n",
    "    ax1.plot(ELBOs, 'b', label='ELBOs')\n",
    "    ax1.tick_params(labelsize=18)\n",
    "    ax2.plot(np.array(ESSs) / num_samples, 'm', label='ESS')\n",
    "    ax1.set_title('epoch=%d, batch_size=%d, lr=%.1E, samples=%d' % (num_epochs, BATCH_SIZE, lr, num_samples), fontsize=18)\n",
    "    ax1.set_ylim([-200, -150])\n",
    "    ax1.legend()\n",
    "    ax2.legend()\n",
    "    ax2.tick_params(labelsize=18)\n",
    "    plt.savefig('local_gibbs_results_learn_both_lr=%.1E_samples=%d.svg' % (lr, num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(EUBOs, ELBOs, ESSs, NUM_SAMPLES, NUM_EPOCHS, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(enc_global.state_dict(), 'models/enc-global-steps=%d-samples=%d-lr=%d' % (STEPS, NUM_SAMPLES, LEARNING_RATE))\n",
    "torch.save(enc_local.state_dict(), 'models/enc-local-steps=%d-samples=%d-lr=%d' % (STEPS, NUM_SAMPLES, LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sample_single_batch(num_seqs, N, K, D, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 50\n",
    "STEPS = 1000\n",
    "prior_mean = torch.zeros((BATCH_SIZE, K, D))\n",
    "prior_sigma = torch.ones((BATCH_SIZE, K, D))\n",
    "prior_alpha = torch.ones((BATCH_SIZE, K, D)) * 2.0\n",
    "prior_beta = torch.ones((BATCH_SIZE, K, D)) * 2.0\n",
    "\n",
    "\n",
    "def sample_single_batch(num_seqs, N, K, D, batch_size):\n",
    "    indices = torch.randperm(num_seqs)\n",
    "    batch_indices = indices[0*batch_size : (0+1)*batch_size]\n",
    "    batch_Xs = Xs[batch_indices]\n",
    "    batch_Xs = shuffler(batch_Xs, N, K, D, batch_size)\n",
    "    return batch_Xs\n",
    "\n",
    "def test(x, num_seqs, Pi, N, K, D, steps, batch_size):\n",
    "    LLs = [] \n",
    "    \n",
    "    for m in range(steps):\n",
    "        if m == 0:\n",
    "            mus, precisions, log_p_eta = inti_global(K, D, prior_mean, prior_sigma, prior_alpha, prior_beta, batch_size)\n",
    "            zs_pi, z, log_q_z = E_step(x, mus, precisions, N, D, K, batch_size)\n",
    "        else:\n",
    "            mus, precisions, log_q_eta, alpha, beta, mus_mean, mus_sigma = M_step(x, z, N, D, K, batch_size)\n",
    "            zs_pi, z, log_q_z = E_step(x, mus, precisions, N, D, K, batch_size)\n",
    "            labels = z.nonzero()\n",
    "            sigmas = 1. / torch.sqrt(precisions)\n",
    "            ll = Normal(mus[labels[:, 0], labels[:, -1], :].view(batch_size, N, D), sigmas[labels[:, 0], labels[:, -1], :].view(batch_size, N, D)).log_prob(x).sum(-1).sum(-1).mean()\n",
    "            LLs.append(ll.item())\n",
    "    E_precisions = alpha / beta\n",
    "    E_mus = mus_mean\n",
    "    E_z = torch.argmax(zs_pi, dim=-1)\n",
    "    return x, z, mus, precisions, LLs, E_mus, E_precisions, E_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_final_samples(Xs, Zs, mus, precisions, steps, batch_size):\n",
    "    colors = ['r', 'b', 'gold']\n",
    "    fig = plt.figure(figsize=(25,50))\n",
    "    for b in range(batch_size):\n",
    "        ax = fig.add_subplot(int(batch_size / 5), 5, b+1)\n",
    "        x = Xs[b].data.numpy()\n",
    "        z = Zs[b].data.numpy()\n",
    "        mu = mus[b].data.numpy()\n",
    "        precision = precisions[b].data.numpy()\n",
    "\n",
    "        covs = np.zeros((K, D, D))\n",
    "        assignments = np.nonzero(z)[1]\n",
    "        for k in range(K):\n",
    "            covs[k] = np.diag(1. / precision[k])\n",
    "            xk = x[np.where(assignments == k)]\n",
    "            ax.scatter(xk[:, 0], xk[:, 1], c=colors[k])\n",
    "            plot_cov_ellipse(cov=covs[k], pos=mu[k], nstd=2, ax=ax, alpha=0.2, color=colors[k])\n",
    "        ax.set_ylim([-10, 10])\n",
    "        ax.set_xlim([-10, 10])\n",
    "    plt.savefig('samples_steps=%d.svg' % (steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, z_samples, mus_samples, precisions_samples, LLS, E_mus, E_precisions, E_z = test(x, num_seqs, Pi, N, K, D, STEPS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_final_samples(x, z_samples, mus_samples, precisions_samples, STEPS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_final_samples(Xs, Zs, mus, precisions, steps, batch_size):\n",
    "    colors = ['r', 'b', 'gold']\n",
    "    fig = plt.figure(figsize=(25,50))\n",
    "    for b in range(batch_size):\n",
    "        ax = fig.add_subplot(int(batch_size / 5), 5, b+1)\n",
    "        x = Xs[b].data.numpy()\n",
    "        z = Zs[b].data.numpy()\n",
    "        mu = mus[b].data.numpy()\n",
    "        precision = precisions[b].data.numpy()\n",
    "\n",
    "        covs = np.zeros((K, D, D))\n",
    "        assignments = z\n",
    "        for k in range(K):\n",
    "            covs[k] = np.diag(1. / precision[k])\n",
    "            xk = x[np.where(assignments == k)]\n",
    "            ax.scatter(xk[:, 0], xk[:, 1], c=colors[k])\n",
    "            plot_cov_ellipse(cov=covs[k], pos=mu[k], nstd=2, ax=ax, alpha=0.2, color=colors[k])\n",
    "        ax.set_ylim([-10, 10])\n",
    "        ax.set_xlim([-10, 10])\n",
    "    plt.savefig('modes_steps=%d.svg' % (steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_final_samples(x, E_z, E_mus, E_precisions, STEPS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LLS[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LLS[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_mus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
