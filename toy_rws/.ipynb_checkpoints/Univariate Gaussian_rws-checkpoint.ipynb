{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "from torch import logsumexp\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal\n",
    "from torch.distributions.normal import Normal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "iterations = 100\n",
    "p_mu = 0.0\n",
    "\n",
    "lr = 1e-3\n",
    "\n",
    "log_Z = np.log(np.sqrt((2*np.pi)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(num_samples, q_mu, lr):\n",
    "    EUBOs = []\n",
    "    ELBOs = []\n",
    "    AGrads = []\n",
    "    Grads = []\n",
    "    for i in range(iterations):\n",
    "        proposal = Normal(q_mu, q_sigma)\n",
    "        xs = proposal.sample((num_samples,))\n",
    "        log_gammas = (-1.0 / 2.0) * ((xs - p_mu) ** 2)\n",
    "        log_q = proposal.log_prob(xs)\n",
    "\n",
    "        log_weights = log_gammas - log_q\n",
    "        weights = torch.exp(log_weights - logsumexp(log_weights, dim=0)).detach()\n",
    "        eubo = torch.mul(weights, log_weights).sum()\n",
    "        elbo = log_weights.mean()\n",
    "        gradient = torch.autograd.grad(eubo, q_mu)\n",
    "        analytical_gradient = q_mu\n",
    "        q_mu = q_mu - lr * gradient[0]\n",
    "        EUBOs.append(eubo.item())\n",
    "        ELBOs.append(elbo.item())\n",
    "        AGrads.append(analytical_gradient.item())\n",
    "        Grads.append(gradient[0].item())\n",
    "        \n",
    "    return EUBOs, ELBOs, AGrads, Grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth_exponential(values, beta=0.9):\n",
    "    v = np.asarray(values)\n",
    "    t = np.arange(len(values))\n",
    "    dt = t[None, :] - t[:,None] \n",
    "    w = np.exp((1-beta) * dt + np.log(t[None,:] <= t[:, None]))\n",
    "    w /= w.sum(1)[:, None]\n",
    "    v_smooth = np.dot(w, v[:, None])\n",
    "    return v_smooth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# T = 1001\n",
    "# t = np.arange(T)\n",
    "# x = t/T\n",
    "# y = 1 - x + 0.5 * np.random.randn(T)\n",
    "# y_avg = smooth_exponential(y, 0.9)\n",
    "# y2_avg = smooth_exponential(y, 0.999)\n",
    "# y_adam = smooth_exponential(y, 0.9) / y2_avg**0.5\n",
    "# plt.figure()\n",
    "# plt.plot(t, y, label='noisy')\n",
    "# plt.plot(t, y_adam, label='adam')\n",
    "# plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-8-87b4093d10c9>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-87b4093d10c9>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    ax.plot(EUBOs[], 'r', label='EUBOs')\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def plot_results(EUBOs, ELBOs, Grads, AGrads, num_mus, num_sigmas, num_samples):\n",
    "    for i in range(num_mus):\n",
    "        fig = plt.figure(figsize=(30, 40))\n",
    "        for j in range(num_sigmas):\n",
    "            for k in range(num_samples):\n",
    "                ax = fig.add_subplot(num_sigmas, num_samples, j*num_samples + k + 1)\n",
    "                ax.plot(EUBOs[i*()], 'r', label='EUBOs')\n",
    "                ax.plot(ELBOs[], 'b', label='ELBOs')\n",
    "                ax.plot(Grads, 'g', label='estimated grad')\n",
    "                #AdamGrads = smooth_exponential(Grads, adam_b1) / smooth_exponential(Grads, adam_b2) \n",
    "                #ax.plot(AdamGrads, 'g', label='adam grad')\n",
    "                ax.plot(AGrads, 'gray', label='true grad' )\n",
    "                ax.plot(np.ones(iterations) * log_Z, 'k', label='log_Z')\n",
    "                ax.tick_params(labelsize=18)\n",
    "                ax.set_ylim([-50, 20])\n",
    "            if i == 0 and j == 0:\n",
    "                ax.legend()\n",
    "                    ax.set_title('mu=%d, sigma=%d, samples=%d' % (init_mu[i], init_sigma[j], NUM_SAMPLES[k]), fontsize=18)\n",
    "    plt.savefig('univariate_gaussian_rws_mu=%d.svg' % (init_mu[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init_mus = np.array([6, 8, 10])\n",
    "init_sigmas = np.array([1.0, 2.0, 4.0, 6.0])\n",
    "SAMPLES = np.array([100, 1000])\n",
    "num_mus = init_mus.shape[0]\n",
    "num_sigmas = init_sigmas.shape[0]\n",
    "num_samples = SAMPLES.shape[0]\n",
    "\n",
    "adam_b1 = 0.9\n",
    "adam_b2 = 0.999\n",
    "\n",
    "EUBOs = []\n",
    "ELBOs = []\n",
    "AGrads = []\n",
    "Grads = []\n",
    "\n",
    "for i in range(num_mus):\n",
    "    for j in range(num_sigmas):\n",
    "        for k in range(num_samples):\n",
    "            time_start = time.time()\n",
    "            q_mu = torch.tensor([1.0], requires_grad=True) * init_mus[i]\n",
    "            q_sigma = torch.tensor([1.0]) * init_sigmas[j]\n",
    "            EUBO, ELBO, AGrad, Grad = train(SAMPLES[k], q_mu, lr)\n",
    "            EUBOs.append(EUBO)\n",
    "            ELBOs.append(ELBO)\n",
    "            AGrads.append(AGrad)\n",
    "            Grads.append(Grad)\n",
    "            time_end = time.time()\n",
    "            print('init_mu=%d, init_sigma=%d, samples : %d (%ds)' % (init_mus[i], init_sigmas[j], SAMPLES[k], time_end - time_start))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8,8))\n",
    "# plt.plot(ELBOs)\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(EUBOs, 'r', label='EUBOs')\n",
    "ax.plot(ELBOs, 'b', label='ELBOs')\n",
    "ax.plot(np.ones(iterations) * log_Z, 'k', label='log_Z')\n",
    "ax.legend()\n",
    "plt.savefig('rws-univariate-samples=%d-prior=2.png' % num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
