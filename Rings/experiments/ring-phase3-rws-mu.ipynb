{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%run ../../import_envs.py\n",
    "print('probtorch:', probtorch.__version__, \n",
    "      'torch:', torch.__version__, \n",
    "      'cuda:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load dataset\n",
    "data_path = \"../../../Rings_2D\"\n",
    "OB = torch.from_numpy(np.load(data_path + '/ob.npy')).float()\n",
    "STATE = torch.from_numpy(np.load(data_path + '/state.npy')).float()\n",
    "MU = torch.from_numpy(np.load(data_path + '/mu.npy')).float()\n",
    "ANGLE = torch.from_numpy(np.load(data_path + '/angle.npy')).float()\n",
    "\n",
    "NUM_DATASETS, N, D = OB.shape\n",
    "K = 3\n",
    "MCMC_SIZE = 10\n",
    "SAMPLE_SIZE = 10\n",
    "BATCH_SIZE = 20\n",
    "NUM_BATCHES = int((NUM_DATASETS / BATCH_SIZE))\n",
    "NUM_EPOCHS = 300\n",
    "NUM_HIDDEN = 32\n",
    "NUM_NSS = 8\n",
    "LEARNING_RATE = 1e-3\n",
    "CUDA = torch.cuda.is_available()\n",
    "DEVICE = 0\n",
    "\n",
    "RECON_SIGMA = torch.ones(1) * 0.1\n",
    "RADI = torch.ones(1) * 2.0\n",
    "lg2pi = torch.log(torch.ones(1) * 2 * math.pi)\n",
    "\n",
    "if CUDA:\n",
    "    with torch.cuda.device(DEVICE):\n",
    "        lg2pi = lg2pi.cuda().to(DEVICE)\n",
    "PATH = 'ag--phase3-mu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from global_enc_mu_v2 import *\n",
    "from decoder_v2 import *\n",
    "# initialization\n",
    "enc_mu = Enc_mu(K, D, NUM_HIDDEN, NUM_NSS, CUDA, DEVICE)\n",
    "dec_x = Dec_x(D, NUM_HIDDEN, RECON_SIGMA, CUDA, DEVICE)\n",
    "if CUDA:\n",
    "    with torch.cuda.device(DEVICE):\n",
    "        enc_mu.cuda()\n",
    "        dec_x.cuda()\n",
    "optimizer =  torch.optim.Adam(list(dec_x.parameters())+list(enc_mu.parameters()),lr=LEARNING_RATE, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(NUM_EPOCHS):\n",
    "    time_start = time.time()\n",
    "    ELBO = 0.0\n",
    "    EUBO = 0.0\n",
    "    ESS = 0.0\n",
    "    indices = torch.randperm(NUM_DATASETS)\n",
    "    for step in range(NUM_BATCHES):\n",
    "        optimizer.zero_grad()\n",
    "        batch_indices = indices[step*BATCH_SIZE : (step+1)*BATCH_SIZE]\n",
    "        ob = OB[batch_indices]\n",
    "        state = STATE[batch_indices]\n",
    "        angle = ANGLE[batch_indices]\n",
    "        embed = shuffler(torch.cat((ob, state, angle), -1)).repeat(SAMPLE_SIZE, 1, 1, 1)\n",
    "        if CUDA:\n",
    "            with torch.cuda.device(DEVICE):\n",
    "                embed = embed.cuda()\n",
    "        ob = embed[:, :, :, :2]\n",
    "        state = embed[:, :, :, 2:-1]\n",
    "        angle = embed[:, :, :, -1].unsqueeze(-1)\n",
    "        q_mu, p_mu = enc_mu.forward(ob, state, angle)\n",
    "        mu = q_mu['means'].value\n",
    "        p_recon = dec_x(ob, state, angle, mu)\n",
    "        ll = p_recon['likelihood'].log_prob.sum(-1)\n",
    "        log_recon = torch.cat([((state.argmax(-1)==k).float() * ll).sum(-1).unsqueeze(-1) for k in range(K)], -1) # S * B * K\n",
    "        log_q = q_mu['means'].log_prob.sum(-1)\n",
    "        log_p = p_mu['means'].log_prob.sum(-1) + log_recon\n",
    "        log_w = log_p.detach() - log_q\n",
    "        w = torch.softmax(log_w, 0).detach()\n",
    "        loss_phi = (w.detach() * (- log_q)).sum(0).sum(-1).mean()\n",
    "#         loss_theta = (- ll).sum(-1).mean()\n",
    "        loss_theta = (w.detach() * (- log_p)).sum(0).sum(-1).mean() \n",
    "        elbo = log_w.sum(-1).mean().detach()\n",
    "        eubo = (w * log_w).sum(0).sum(-1).mean().detach()\n",
    "        \n",
    "#         loss_phi = (w.detach() * (- log_q)).sum(0).mean()\n",
    "#         loss_theta = (w.detach() * (- log_p)).sum(0).mean() \n",
    "#         elbo = log_w.mean().detach()\n",
    "#         eubo = (w * log_w).sum(0).mean().detach()\n",
    "        ess = (1 / (w ** 2).sum(0)).mean()\n",
    "        loss_phi.backward(retain_graph=True)\n",
    "        loss_theta.backward()\n",
    "        optimizer.step()\n",
    "        ELBO += elbo.detach()\n",
    "        EUBO += eubo.detach()\n",
    "        ESS += ess\n",
    "    time_end = time.time()\n",
    "    print('epoch=%d, eubo=%.4f, elbo=%.4f, ess=%.4f (%ds)' % (epoch,  EUBO / NUM_BATCHES, ELBO / NUM_BATCHES, ESS / NUM_BATCHES, time_end - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_final_samples(ob, mu, state, recon, K, PATH, bound):\n",
    "    page_width = 25\n",
    "    B, N, D = ob.shape\n",
    "    plt.rc('axes',edgecolor='#eeeeee')\n",
    "    gs = gridspec.GridSpec(int(B / 5), 5)\n",
    "    gs.update(left=0.0 , bottom=0.0, right=1.0, top=1.0, wspace=0, hspace=0)\n",
    "    fig = plt.figure(figsize=(page_width,page_width*4/5))\n",
    "    colors = ['#EE7733', 'm', '#0077BB', '#009988']\n",
    "    for b in range(B):\n",
    "        ax = fig.add_subplot(gs[int(b/5), int(b%5)])\n",
    "        assignments = state[b].argmax(-1)\n",
    "        ax.scatter(recon[b, :, 0], recon[b, :, 1], c='k',alpha=0.2)\n",
    "        for k in range(K):\n",
    "            ob_k = ob[b][np.where(assignments == k)]\n",
    "            ax.scatter(ob_k[:, 0], ob_k[:, 1], c=colors[k], s=6, alpha=0.8)\n",
    "            ax.set_ylim([-bound, bound])\n",
    "            ax.set_xlim([-bound, bound])\n",
    "#             ax.set_xticks([])\n",
    "#             ax.set_yticks([])\n",
    "            ax.scatter(mu[b, k, 0], mu[b, k, 1], c=colors[k], marker='X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = torch.randperm(NUM_DATASETS)\n",
    "batch_indices = indices[0*BATCH_SIZE : (0+1)*BATCH_SIZE]\n",
    "ob = OB[batch_indices]\n",
    "state = STATE[batch_indices]\n",
    "angle = ANGLE[batch_indices]\n",
    "mu_true = MU[batch_indices].repeat(SAMPLE_SIZE, 1, 1, 1)\n",
    "embed = shuffler(torch.cat((ob, state, angle), -1)).repeat(SAMPLE_SIZE, 1, 1, 1)\n",
    "if CUDA:\n",
    "    with torch.cuda.device(DEVICE):\n",
    "        embed = embed.cuda()\n",
    "        mu_true = mu_true.cuda()\n",
    "ob = embed[:, :, :, :2]\n",
    "state = embed[:, :, :, 2:-1]\n",
    "angle = embed[:, :, :, -1].unsqueeze(-1)\n",
    "q_mu, p_mu = enc_mu.forward(ob, state, angle)\n",
    "mu = q_mu['means'].value\n",
    "p_recon = dec_x(ob, state, angle, mu)\n",
    "recon_mu = p_recon['likelihood'].dist.loc[0].cpu().data.numpy()\n",
    "# recon_mu = p_recon['likelihood'].dist.loc\n",
    "# recon_mu_centered = (recon_mu - global_to_local(mu, state))[0].cpu().data.numpy()\n",
    "state_true = state[0].cpu().data.numpy()\n",
    "mu_mu = q_mu['means'].dist.loc[0].cpu().data.numpy()\n",
    "test_ob = ob[0].cpu().data.numpy()\n",
    "\n",
    "mu_true = mu_true[0].cpu().data.numpy()\n",
    "plot_final_samples(test_ob, mu_mu, state_true, recon_mu, K, PATH, 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = recon_mu_centered[0]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
