{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from bivariate import *\n",
    "from utils import *\n",
    "from enc import *\n",
    "from plots import *\n",
    "import torch\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal as mvn\n",
    "# from torch.distributions.one_hot_categorical import OneHotCategorical as cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Define a target bivariate Gaussian for unit test\n",
    "# mu1 = torch.ones(1) * 5.0\n",
    "# mu2 = torch.ones(1) * 8.0\n",
    "# sigma1 = torch.ones(1) * 1.0\n",
    "# sigma2 = torch.ones(1) * 2.5\n",
    "# rho = torch.ones(1) * 0.6\n",
    "\n",
    "# bg = Bi_Gaussian(mu1, mu2, sigma1, sigma2, rho, CUDA=False, device=None)\n",
    "\n",
    "# STEPS = 500\n",
    "\n",
    "# updates = Gibbs(bg, STEPS, sampling=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot_updates(updates, bg, sigma_factor=5, pts=1000, fs=10, levels=5, back_to_cpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Amortized Gibbs Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAD_STEPS = 1000\n",
    "LEARNING_RATE = 1e-3\n",
    "NUM_SAMPLES = 10\n",
    "MCMC_STEPS = 10\n",
    "NUM_HIDDENS = 8\n",
    "\n",
    "CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a target bivariate Gaussian for unit test\n",
    "mu1 = torch.ones(1) * 5.0\n",
    "mu2 = torch.ones(1) * 8.0\n",
    "sigma1 = torch.ones(1) * 1.0\n",
    "sigma2 = torch.ones(1) * 2.5\n",
    "rho = torch.ones(1) * 0.6\n",
    "\n",
    "q_x1 = Kernel(NUM_HIDDENS, mu1, sigma1, CUDA, DEVICE)\n",
    "q_x2 = Kernel(NUM_HIDDENS, mu2, sigma2, CUDA, DEVICE)\n",
    "bg = Bi_Gaussian(mu1, mu2, sigma1, sigma2, rho, CUDA=True, device=DEVICE)\n",
    "\n",
    "if CUDA:\n",
    "     with torch.cuda.device(DEVICE):\n",
    "        q_x1.cuda()\n",
    "        q_x2.cuda()\n",
    "optimizer = torch.optim.Adam(list(q_x1.parameters())+list(q_x2.parameters()), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fb(q, bg, x_cond, x_cond_name, x_old, sampling=True):\n",
    "    r_mu, r_sigma = bg.conditional(x_cond, cond=x_cond_name)\n",
    "    if sampling:\n",
    "        x_new, log_q_f, q_mu, q_sigma = q.forward(x_cond)\n",
    "    else:\n",
    "        _, _, q_mu, q_sigma = q.forward(x_cond)\n",
    "        x_new = q_mu\n",
    "        \n",
    "    kls = kl_normal_normal(r_mu, r_sigma, q_mu, q_sigma).mean()\n",
    "    log_p_f = bg.log_pdf_gamma(x_new, r_mu, r_sigma)\n",
    "    log_q_b = q.backward(x_cond, x_old)\n",
    "    log_p_b = bg.log_pdf_gamma(x_old, r_mu, r_sigma)  \n",
    "    log_w_f = log_p_f.sum(-1) - log_q_f.sum(-1)\n",
    "    log_w_b = log_p_b.sum(-1) - log_q_b.sum(-1)\n",
    "    log_w = log_w_f - log_w_b\n",
    "    w = F.softmax(log_w, 0).detach()\n",
    "    loss = (w * log_w_f).sum(0).mean().unsqueeze(0)\n",
    "    ess = (1. / (w ** 2).sum(0)).mean().unsqueeze(0)\n",
    "    return loss, ess, x_new, log_w, w, kls\n",
    "\n",
    "def ag(q_x1, q_x2, bg, mcmc_steps, num_samples):\n",
    "    losss = []\n",
    "    esss = []\n",
    "\n",
    "    ## start with sampling x1 from the prior\n",
    "    x1, log_p_x1_f = q_x1.sample_prior(num_samples)\n",
    "    x2, log_q_x2, _, _ = q_x2.forward(x1)\n",
    "    r_mu, r_sigma = bg.conditional(x1, cond='x1')\n",
    "    log_p_x2_f = bg.log_pdf_gamma(x2, r_mu, r_sigma)\n",
    "    log_w = log_p_x2_f.sum(-1).detach() - log_q_x2.sum(-1)\n",
    "    w_x2 = F.softmax(log_w, 0).detach()\n",
    "    losss.append((w_x2 * log_w).sum(0).mean().unsqueeze(0))\n",
    "    esss.append((1. / (w_x2** 2).sum(0)).mean().unsqueeze(0))\n",
    "    for m in range(mcmc_steps):\n",
    "        x2 = resample(x2, w_x2) ## resample x2\n",
    "        x1_old = x1\n",
    "        loss_x1, ess_x1, x1, log_w_x1, w_x1, _ = fb(q_x1, bg, x2, 'x2', x1_old) ## update x1\n",
    "        x1 = resample(x1, w_x1) ## resample x1\n",
    "        x2_old = x2\n",
    "        loss_x2, ess_x2, x2, log_w_x2, w_x2, _ = fb(q_x2, bg, x1, 'x1', x2_old) ## update x2\n",
    "        losss.append(loss_x1+loss_x2)\n",
    "        esss.append((ess_x1 + ess_x2) / 2)\n",
    "    return torch.cat(losss, 0).sum(), torch.cat(esss, 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOSSS = []\n",
    "ESSS = []\n",
    "\n",
    "time_start = time.time()\n",
    "for i in range(GRAD_STEPS):\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss, ess = ag(q_x1, q_x2, bg, MCMC_STEPS, NUM_SAMPLES)\n",
    "    ##\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    LOSSS.append(loss)\n",
    "    ESSS.append(ess)\n",
    "    if i % 100 == 0:\n",
    "        time_end = time.time()\n",
    "        print('Step=%d, Loss=%.4f, ESS=%.4f (%ds)' % (i, loss, ess, time_end - time_start))\n",
    "        time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(q_x1, q_x2, bg, mcmc_steps, num_samples, sampling=True):\n",
    "    updates = []\n",
    "    DBs = []\n",
    "    KLs = []\n",
    "    ## start with sampling x1 from the prior\n",
    "    x1, log_p_x1_f = q_x1.sample_prior(num_samples)\n",
    "    if sampling:\n",
    "        x2, log_q_x2, q_mu, q_sigma = q_x2.forward(x1)\n",
    "    else:\n",
    "        _, _, q_mu, q_sigma = q_x2.forward(x1)\n",
    "        x2 = q_mu\n",
    "    r_mu, r_sigma = bg.conditional(x1, cond='x1')\n",
    "    kls = kl_normal_normal(r_mu, r_sigma, q_mu, q_sigma).mean()\n",
    "    KLs.append(kls.unsqueeze(0))\n",
    "    log_p_x2_f = bg.log_pdf_gamma(x2, r_mu, r_sigma)\n",
    "    log_w = log_p_x2_f.sum(-1).detach() - log_q_x2.sum(-1)\n",
    "    w_x2 = F.softmax(log_w, 0).detach()\n",
    "    updates.append(torch.cat((x1, x2), -1))\n",
    "    for m in range(mcmc_steps):\n",
    "#         x2 = resample(x2, w_x2) ## resample x2\n",
    "        x1_old = x1\n",
    "        loss_x1, ess_x1, x1, log_w_x1, w_x1, kls_x1 = fb(q_x1, bg, x2, 'x2', x1_old) ## update x1\n",
    "        DBs.append((w_x1 * log_w_x1).sum(0).mean().unsqueeze(0))\n",
    "#         x1 = resample(x1, w_x1) ## resample x1\n",
    "        x2_old = x2\n",
    "        loss_x2, ess_x2, x2, log_w_x2, w_x2, kls_x2 = fb(q_x2, bg, x1, 'x1', x2_old) ## update x2\n",
    "        KLs.append((kls_x1.unsqueeze(0) + kls_x2.unsqueeze(0)) / 2)\n",
    "\n",
    "        updates.append(torch.cat((x1, x2), -1))\n",
    "        DBs.append((w_x2 * log_w_x2).sum(0).mean().unsqueeze(0))\n",
    "    return torch.cat(updates, 0), torch.cat(DBs, 0), torch.cat(KLs, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "updates, DBs, KLs = test(q_x1, q_x2, bg, 500, 1)\n",
    "plot_kls(DBs.cpu().data.numpy(), KLs.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot_updates(updates, bg, sigma_factor=5, pts=100, fs=10, levels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
