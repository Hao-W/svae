{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probtorch: 0.0+5a2c637 torch: 0.4.1 cuda: True\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%run ../../import_envs.py\n",
    "print('probtorch:', probtorch.__version__, \n",
    "      'torch:', torch.__version__, \n",
    "      'cuda:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load dataset\n",
    "data_path = \"../gmm_dataset_c20k\"\n",
    "Data = torch.from_numpy(np.load(data_path + '/obs.npy')).float()\n",
    "\n",
    "NUM_DATASETS, N, D = Data.shape\n",
    "K = 3 ## number of clusters\n",
    "SAMPLE_SIZE = 10\n",
    "NUM_HIDDEN_LOCAL = 32\n",
    "\n",
    "MCMC_SIZE = 10\n",
    "BATCH_SIZE = 20\n",
    "NUM_EPOCHS = 350\n",
    "LEARNING_RATE = 1e-4\n",
    "CUDA = torch.cuda.is_available()\n",
    "PATH = 'AG-only-stats-%dsteps-%dsamples' % (MCMC_SIZE, SAMPLE_SIZE)\n",
    "DEVICE = torch.device('cuda:1')\n",
    "\n",
    "Train_Params = (NUM_EPOCHS, NUM_DATASETS, SAMPLE_SIZE, BATCH_SIZE, CUDA, DEVICE, PATH)\n",
    "Model_Params = (N, K, D, MCMC_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_enc import *\n",
    "from global_oneshot import *\n",
    "from global_enc import *\n",
    "## if reparameterize continuous variables\n",
    "Reparameterized = False\n",
    "# initialization\n",
    "enc_z = Enc_z(K, D, NUM_HIDDEN_LOCAL, CUDA, DEVICE)\n",
    "enc_eta = Enc_eta(K, D, CUDA, DEVICE, Reparameterized)\n",
    "oneshot_eta = Oneshot_eta(K, D, CUDA, DEVICE, Reparameterized)\n",
    "\n",
    "if CUDA:\n",
    "    enc_z.cuda().to(DEVICE)\n",
    "    enc_eta.cuda().to(DEVICE)\n",
    "    oneshot_eta.cuda().to(DEVICE)\n",
    "optimizer =  torch.optim.Adam(list(oneshot_eta.parameters())+list(enc_eta.parameters())+list(enc_z.parameters()),lr=LEARNING_RATE, betas=(0.9, 0.99))\n",
    "models = (oneshot_eta, enc_eta, enc_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\\350 (134s),  symKL_DB_eta: 345.084,  symKL_DB_z: 389.030,  loss: -1471.152,  ess: 3.020,  kl_eta_ex: 993.368,  kl_eta_in: 160.669,  kl_z_ex: 275.248,  kl_z_in: 36.137\n",
      "epoch: 1\\350 (134s),  symKL_DB_eta: 194.762,  symKL_DB_z: 193.710,  loss: -1031.986,  ess: 3.375,  kl_eta_ex: 494.740,  kl_eta_in: 124.321,  kl_z_ex: 123.027,  kl_z_in: 25.824\n",
      "epoch: 2\\350 (129s),  symKL_DB_eta: 123.486,  symKL_DB_z: 114.452,  loss: -826.956,  ess: 3.681,  kl_eta_ex: 280.960,  kl_eta_in: 103.071,  kl_z_ex: 64.205,  kl_z_in: 19.236\n",
      "epoch: 3\\350 (131s),  symKL_DB_eta: 82.680,  symKL_DB_z: 81.932,  loss: -721.374,  ess: 3.978,  kl_eta_ex: 181.547,  kl_eta_in: 94.379,  kl_z_ex: 42.632,  kl_z_in: 14.919\n",
      "epoch: 4\\350 (134s),  symKL_DB_eta: 60.220,  symKL_DB_z: 63.654,  loss: -654.154,  ess: 4.283,  kl_eta_ex: 126.288,  kl_eta_in: 84.717,  kl_z_ex: 31.531,  kl_z_in: 11.577\n",
      "epoch: 5\\350 (135s),  symKL_DB_eta: 44.235,  symKL_DB_z: 51.018,  loss: -597.720,  ess: 4.548,  kl_eta_ex: 86.827,  kl_eta_in: 71.730,  kl_z_ex: 24.817,  kl_z_in: 9.317\n",
      "epoch: 6\\350 (146s),  symKL_DB_eta: 32.392,  symKL_DB_z: 43.082,  loss: -550.876,  ess: 4.777,  kl_eta_ex: 57.917,  kl_eta_in: 58.135,  kl_z_ex: 20.444,  kl_z_in: 7.693\n",
      "epoch: 7\\350 (153s),  symKL_DB_eta: 22.967,  symKL_DB_z: 35.479,  loss: -510.660,  ess: 5.042,  kl_eta_ex: 37.536,  kl_eta_in: 48.328,  kl_z_ex: 17.042,  kl_z_in: 5.921\n",
      "epoch: 8\\350 (154s),  symKL_DB_eta: 17.263,  symKL_DB_z: 29.699,  loss: -480.418,  ess: 5.337,  kl_eta_ex: 26.766,  kl_eta_in: 40.456,  kl_z_ex: 14.629,  kl_z_in: 4.267\n",
      "epoch: 9\\350 (144s),  symKL_DB_eta: 14.893,  symKL_DB_z: 26.195,  loss: -464.068,  ess: 5.564,  kl_eta_ex: 22.834,  kl_eta_in: 34.256,  kl_z_ex: 13.019,  kl_z_in: 3.358\n",
      "epoch: 10\\350 (142s),  symKL_DB_eta: 13.894,  symKL_DB_z: 22.633,  loss: -456.662,  ess: 5.722,  kl_eta_ex: 20.839,  kl_eta_in: 30.710,  kl_z_ex: 11.295,  kl_z_in: 2.887\n",
      "epoch: 11\\350 (144s),  symKL_DB_eta: 13.268,  symKL_DB_z: 19.023,  loss: -452.498,  ess: 5.843,  kl_eta_ex: 18.599,  kl_eta_in: 28.703,  kl_z_ex: 9.259,  kl_z_in: 2.509\n",
      "epoch: 12\\350 (142s),  symKL_DB_eta: 12.398,  symKL_DB_z: 16.808,  loss: -448.449,  ess: 5.944,  kl_eta_ex: 16.530,  kl_eta_in: 26.794,  kl_z_ex: 7.726,  kl_z_in: 2.258\n",
      "epoch: 13\\350 (142s),  symKL_DB_eta: 11.367,  symKL_DB_z: 15.145,  loss: -444.793,  ess: 6.051,  kl_eta_ex: 14.255,  kl_eta_in: 24.862,  kl_z_ex: 6.690,  kl_z_in: 2.099\n",
      "epoch: 14\\350 (147s),  symKL_DB_eta: 10.345,  symKL_DB_z: 14.086,  loss: -441.701,  ess: 6.163,  kl_eta_ex: 12.490,  kl_eta_in: 22.568,  kl_z_ex: 6.087,  kl_z_in: 1.981\n",
      "epoch: 15\\350 (143s),  symKL_DB_eta: 9.602,  symKL_DB_z: 13.542,  loss: -440.320,  ess: 6.261,  kl_eta_ex: 11.335,  kl_eta_in: 21.427,  kl_z_ex: 5.681,  kl_z_in: 1.900\n"
     ]
    }
   ],
   "source": [
    "from ag_ep import *\n",
    "train(models, EUBO_init_eta, optimizer, Data, Model_Params, Train_Params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(enc_z.state_dict(), \"../weights/enc-z-%s\" % PATH)\n",
    "torch.save(enc_eta.state_dict(), \"../weights/enc-eta-%s\" % PATH)\n",
    "torch.save(oneshot_eta.state_dict(), \"../weights/oneshot-eta-%s\" % PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
