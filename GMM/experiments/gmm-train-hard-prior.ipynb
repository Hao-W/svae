{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probtorch: 0.0+5a2c637 torch: 1.1.0 cuda: True\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%run ../../import_envs.py\n",
    "print('probtorch:', probtorch.__version__, \n",
    "      'torch:', torch.__version__, \n",
    "      'cuda:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load dataset\n",
    "data_path = \"../gmm_dataset_c20k\"\n",
    "Data = torch.from_numpy(np.load(data_path + '/obs.npy')).float()\n",
    "\n",
    "NUM_DATASETS, N, D = Data.shape\n",
    "K = 3 ## number of clusters\n",
    "SAMPLE_SIZE = 10\n",
    "NUM_HIDDEN_LOCAL = 32\n",
    "\n",
    "MCMC_SIZE = 10\n",
    "BATCH_SIZE = 20\n",
    "NUM_EPOCHS = 250\n",
    "LEARNING_RATE = 1e-4\n",
    "CUDA = torch.cuda.is_available()\n",
    "PATH = 'hard-%dsteps-%dsamples' % (MCMC_SIZE, SAMPLE_SIZE)\n",
    "DEVICE = torch.device('cuda:1')\n",
    "\n",
    "Train_Params = (NUM_EPOCHS, NUM_DATASETS, SAMPLE_SIZE, BATCH_SIZE, CUDA, DEVICE, PATH)\n",
    "Model_Params = (N, K, D, MCMC_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from local_enc import *\n",
    "from global_enc import *\n",
    "## if reparameterize continuous variables\n",
    "Reparameterized = False\n",
    "# initialization\n",
    "enc_z = Enc_z(K, D, NUM_HIDDEN_LOCAL, CUDA, DEVICE)\n",
    "enc_eta = Enc_eta(K, D, CUDA, DEVICE, Reparameterized)\n",
    "if CUDA:\n",
    "    enc_z.cuda().to(DEVICE)\n",
    "    enc_eta.cuda().to(DEVICE)\n",
    "optimizer =  torch.optim.Adam(list(enc_eta.parameters())+list(enc_z.parameters()),lr=LEARNING_RATE, betas=(0.9, 0.99))\n",
    "models = (enc_eta, enc_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0\\250 (93s),  symKL_DB_eta: 489.662,  symKL_DB_z: 503.363,  loss: -1744.509,  ess: 2.747,  kl_eta_ex: 759.323,  kl_eta_in: 191.983,  kl_z_ex: 169.241,  kl_z_in: 32.687\n",
      "epoch: 1\\250 (93s),  symKL_DB_eta: 388.689,  symKL_DB_z: 383.313,  loss: -1425.195,  ess: 3.029,  kl_eta_ex: 561.861,  kl_eta_in: 203.041,  kl_z_ex: 127.453,  kl_z_in: 27.208\n",
      "epoch: 2\\250 (93s),  symKL_DB_eta: 324.350,  symKL_DB_z: 290.872,  loss: -1237.121,  ess: 3.259,  kl_eta_ex: 448.937,  kl_eta_in: 228.902,  kl_z_ex: 113.763,  kl_z_in: 24.710\n",
      "epoch: 3\\250 (93s),  symKL_DB_eta: 291.651,  symKL_DB_z: 251.249,  loss: -1129.058,  ess: 3.349,  kl_eta_ex: 375.538,  kl_eta_in: 262.261,  kl_z_ex: 98.110,  kl_z_in: 22.388\n",
      "epoch: 4\\250 (93s),  symKL_DB_eta: 282.089,  symKL_DB_z: 228.777,  loss: -1069.017,  ess: 3.415,  kl_eta_ex: 328.353,  kl_eta_in: 303.252,  kl_z_ex: 79.985,  kl_z_in: 19.731\n",
      "epoch: 5\\250 (93s),  symKL_DB_eta: 266.768,  symKL_DB_z: 198.563,  loss: -1026.624,  ess: 3.556,  kl_eta_ex: 301.827,  kl_eta_in: 356.337,  kl_z_ex: 67.493,  kl_z_in: 17.654\n",
      "epoch: 6\\250 (93s),  symKL_DB_eta: 258.298,  symKL_DB_z: 168.107,  loss: -1000.479,  ess: 3.710,  kl_eta_ex: 288.730,  kl_eta_in: 419.679,  kl_z_ex: 60.768,  kl_z_in: 16.427\n",
      "epoch: 7\\250 (93s),  symKL_DB_eta: 246.108,  symKL_DB_z: 143.300,  loss: -978.499,  ess: 3.838,  kl_eta_ex: 283.720,  kl_eta_in: 489.373,  kl_z_ex: 56.720,  kl_z_in: 15.642\n",
      "epoch: 8\\250 (93s),  symKL_DB_eta: 234.563,  symKL_DB_z: 127.491,  loss: -962.806,  ess: 3.925,  kl_eta_ex: 282.913,  kl_eta_in: 556.204,  kl_z_ex: 53.662,  kl_z_in: 15.163\n",
      "epoch: 9\\250 (93s),  symKL_DB_eta: 228.506,  symKL_DB_z: 115.920,  loss: -955.020,  ess: 3.990,  kl_eta_ex: 284.176,  kl_eta_in: 618.006,  kl_z_ex: 51.246,  kl_z_in: 14.711\n",
      "epoch: 10\\250 (93s),  symKL_DB_eta: 225.453,  symKL_DB_z: 107.275,  loss: -948.777,  ess: 4.038,  kl_eta_ex: 285.987,  kl_eta_in: 670.955,  kl_z_ex: 49.250,  kl_z_in: 14.388\n",
      "epoch: 11\\250 (92s),  symKL_DB_eta: 221.406,  symKL_DB_z: 101.306,  loss: -944.777,  ess: 4.066,  kl_eta_ex: 288.022,  kl_eta_in: 713.085,  kl_z_ex: 47.160,  kl_z_in: 14.063\n",
      "epoch: 12\\250 (92s),  symKL_DB_eta: 220.262,  symKL_DB_z: 97.690,  loss: -943.920,  ess: 4.095,  kl_eta_ex: 288.061,  kl_eta_in: 740.889,  kl_z_ex: 45.103,  kl_z_in: 13.714\n",
      "epoch: 13\\250 (92s),  symKL_DB_eta: 217.528,  symKL_DB_z: 93.180,  loss: -942.701,  ess: 4.116,  kl_eta_ex: 288.982,  kl_eta_in: 765.273,  kl_z_ex: 43.755,  kl_z_in: 13.358\n",
      "epoch: 14\\250 (93s),  symKL_DB_eta: 216.095,  symKL_DB_z: 89.850,  loss: -939.967,  ess: 4.149,  kl_eta_ex: 289.467,  kl_eta_in: 781.732,  kl_z_ex: 41.946,  kl_z_in: 12.857\n",
      "epoch: 15\\250 (101s),  symKL_DB_eta: 214.668,  symKL_DB_z: 87.481,  loss: -939.744,  ess: 4.178,  kl_eta_ex: 287.022,  kl_eta_in: 789.203,  kl_z_ex: 39.666,  kl_z_in: 12.310\n",
      "epoch: 16\\250 (108s),  symKL_DB_eta: 213.891,  symKL_DB_z: 84.450,  loss: -938.600,  ess: 4.209,  kl_eta_ex: 287.820,  kl_eta_in: 796.955,  kl_z_ex: 37.997,  kl_z_in: 11.697\n",
      "epoch: 17\\250 (112s),  symKL_DB_eta: 215.658,  symKL_DB_z: 82.442,  loss: -938.213,  ess: 4.244,  kl_eta_ex: 287.462,  kl_eta_in: 803.165,  kl_z_ex: 35.852,  kl_z_in: 11.020\n",
      "epoch: 18\\250 (113s),  symKL_DB_eta: 211.558,  symKL_DB_z: 78.051,  loss: -934.525,  ess: 4.291,  kl_eta_ex: 286.234,  kl_eta_in: 803.357,  kl_z_ex: 33.970,  kl_z_in: 10.346\n",
      "epoch: 19\\250 (114s),  symKL_DB_eta: 212.529,  symKL_DB_z: 75.997,  loss: -936.913,  ess: 4.330,  kl_eta_ex: 285.996,  kl_eta_in: 806.925,  kl_z_ex: 31.973,  kl_z_in: 9.744\n",
      "epoch: 20\\250 (105s),  symKL_DB_eta: 211.537,  symKL_DB_z: 74.133,  loss: -936.577,  ess: 4.372,  kl_eta_ex: 286.231,  kl_eta_in: 811.678,  kl_z_ex: 30.683,  kl_z_in: 9.280\n",
      "epoch: 21\\250 (102s),  symKL_DB_eta: 212.277,  symKL_DB_z: 71.827,  loss: -935.087,  ess: 4.403,  kl_eta_ex: 286.905,  kl_eta_in: 813.991,  kl_z_ex: 29.370,  kl_z_in: 8.836\n",
      "epoch: 22\\250 (102s),  symKL_DB_eta: 210.246,  symKL_DB_z: 68.870,  loss: -933.240,  ess: 4.442,  kl_eta_ex: 289.639,  kl_eta_in: 816.042,  kl_z_ex: 28.028,  kl_z_in: 8.428\n",
      "epoch: 23\\250 (111s),  symKL_DB_eta: 210.710,  symKL_DB_z: 66.659,  loss: -932.308,  ess: 4.472,  kl_eta_ex: 290.603,  kl_eta_in: 819.611,  kl_z_ex: 26.729,  kl_z_in: 8.063\n",
      "epoch: 24\\250 (107s),  symKL_DB_eta: 210.930,  symKL_DB_z: 64.534,  loss: -930.672,  ess: 4.510,  kl_eta_ex: 294.804,  kl_eta_in: 822.504,  kl_z_ex: 25.555,  kl_z_in: 7.658\n",
      "epoch: 25\\250 (115s),  symKL_DB_eta: 211.432,  symKL_DB_z: 62.890,  loss: -930.864,  ess: 4.541,  kl_eta_ex: 295.637,  kl_eta_in: 819.331,  kl_z_ex: 24.340,  kl_z_in: 7.249\n",
      "epoch: 26\\250 (116s),  symKL_DB_eta: 212.314,  symKL_DB_z: 60.417,  loss: -930.232,  ess: 4.577,  kl_eta_ex: 298.328,  kl_eta_in: 816.890,  kl_z_ex: 23.193,  kl_z_in: 6.918\n",
      "epoch: 27\\250 (108s),  symKL_DB_eta: 211.320,  symKL_DB_z: 59.058,  loss: -925.152,  ess: 4.613,  kl_eta_ex: 300.813,  kl_eta_in: 812.182,  kl_z_ex: 21.967,  kl_z_in: 6.537\n",
      "epoch: 28\\250 (102s),  symKL_DB_eta: 209.565,  symKL_DB_z: 56.401,  loss: -922.820,  ess: 4.647,  kl_eta_ex: 305.400,  kl_eta_in: 806.027,  kl_z_ex: 20.840,  kl_z_in: 6.148\n",
      "epoch: 29\\250 (102s),  symKL_DB_eta: 213.586,  symKL_DB_z: 55.608,  loss: -922.167,  ess: 4.689,  kl_eta_ex: 308.967,  kl_eta_in: 795.757,  kl_z_ex: 19.412,  kl_z_in: 5.743\n",
      "epoch: 30\\250 (104s),  symKL_DB_eta: 214.668,  symKL_DB_z: 52.945,  loss: -918.832,  ess: 4.723,  kl_eta_ex: 313.705,  kl_eta_in: 784.893,  kl_z_ex: 18.112,  kl_z_in: 5.363\n"
     ]
    }
   ],
   "source": [
    "from ag_ep import *\n",
    "\n",
    "train_prior(models, EUBO_init_eta_prior, optimizer, Data, Model_Params, Train_Params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(enc_z.state_dict(), \"../weights/enc-z-%s\" % PATH)\n",
    "torch.save(enc_eta.state_dict(), \"../weights/enc-eta-%s\" % PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
