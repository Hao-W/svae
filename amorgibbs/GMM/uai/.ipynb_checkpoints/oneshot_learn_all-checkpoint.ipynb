{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from plots import *\n",
    "from kls import *\n",
    "from utils import *\n",
    "from torch._six import inf\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal as mvn\n",
    "from torch.distributions.one_hot_categorical import OneHotCategorical as cat\n",
    "from torch.distributions.gamma import Gamma\n",
    "from torch import logsumexp\n",
    "import sys\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "K = 3\n",
    "D = 2\n",
    "\n",
    "## Model Parameters\n",
    "NUM_SAMPLES = 10\n",
    "NUM_HIDDEN = 32\n",
    "NUM_STATS = K+D*K+D*K\n",
    "NUM_LATENTS = D * K\n",
    "NUM_OBS_GLOBAL = D\n",
    "NUM_OBS_LOCAL = D + K*D + K*D\n",
    "BATCH_SIZE = 50\n",
    "NUM_EPOCHS = 10000\n",
    "LEARNING_RATE = 1e-4\n",
    "PATH = 'uai-oneshot-all'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = torch.from_numpy(np.load('gmm_dataset_uai/sequences.npy')).float()\n",
    "# Zs_true = torch.from_numpy(np.load('gmm_dataset2/states.npy')).float()\n",
    "# mus_true = torch.from_numpy(np.load('gmm_dataset2/means.npy')).float()\n",
    "# sigma2_true = torch.from_numpy(np.load('gmm_dataset2/covariances.npy')).float()\n",
    "Pi = torch.from_numpy(np.load('gmm_dataset_uai/init.npy')).float()\n",
    "num_seqs = Xs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_global(nn.Module):\n",
    "    def __init__(self, num_obs=NUM_OBS_GLOBAL,\n",
    "                       num_stats=NUM_STATS,\n",
    "                       num_hidden=NUM_HIDDEN,\n",
    "                       num_latents=NUM_LATENTS):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.enc_stats = nn.Sequential(\n",
    "            nn.Linear(num_obs, num_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(num_hidden, num_stats))\n",
    "        self.enc_hidden = nn.Sequential(\n",
    "            nn.Linear(num_stats, num_hidden),\n",
    "            nn.Tanh())\n",
    "        self.sigmas_log_alpha = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        self.sigmas_log_beta = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        \n",
    "        self.enc_hidden2 = nn.Sequential(\n",
    "            nn.Linear(num_stats, num_hidden),\n",
    "            nn.Tanh())\n",
    "        self.mus_mean = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        self.mus_log_nu = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        \n",
    "    def forward(self, obs, K, D, num_samples, batch_size):\n",
    "        stats = self.enc_stats(obs).view(batch_size, N, -1).sum(1)\n",
    "        hidden = self.enc_hidden(stats)\n",
    "        q_alpha = torch.exp(self.sigmas_log_alpha(hidden)).view(-1, K, D) ## B * K * D\n",
    "        q_beta = torch.exp(self.sigmas_log_beta(hidden)).view(-1, K, D) ## B * K * D\n",
    "        precisions = Gamma(q_alpha, q_beta).sample((num_samples,)) ## S * B * K * D\n",
    "        \n",
    "        hidden2 = self.enc_hidden2(stats)                 \n",
    "        q_mean = self.mus_mean(hidden2).view(-1, K, D).unsqueeze(0).repeat(num_samples, 1, 1, 1)\n",
    "        q_nu = torch.exp(self.mus_log_nu(hidden2).view(-1, K, D))\n",
    "        q_sigma = torch.sqrt(1. / (q_nu.unsqueeze(0).repeat(num_samples, 1, 1, 1) * precisions))\n",
    "        mus = Normal(q_mean, q_sigma).sample()  \n",
    "        return q_mean, q_nu, q_alpha, q_beta, q_sigma, mus, precisions ## mus_mean and mus_sigma are S * B * K * D\n",
    "    \n",
    "class Encoder_local(nn.Module):\n",
    "    def __init__(self, num_obs=NUM_OBS_LOCAL,\n",
    "                       num_hidden=NUM_HIDDEN,\n",
    "                       num_latents=K):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.enc_onehot = nn.Sequential(\n",
    "            nn.Linear(num_obs, num_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(num_hidden, num_latents),\n",
    "            nn.Softmax(-1))\n",
    "        \n",
    "    def forward(self, obs, N, K, D, num_samples, batch_size):\n",
    "        zs_pi = self.enc_onehot(obs).view(batch_size, N, K)\n",
    "        zs = cat(zs_pi).sample((num_samples,))\n",
    "        log_qz = cat(zs_pi).log_prob(zs).view(num_samples, batch_size, -1).sum(-1) ## S * B\n",
    "        zs = zs.view(num_samples, batch_size, -1, K) ## S * B * N * K\n",
    "        return zs_pi, zs, log_qz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    enc_global = Encoder_global()\n",
    "    enc_local = Encoder_local()\n",
    "    enc_global.apply(weights_init)\n",
    "\n",
    "    optimizer =  torch.optim.Adam(list(enc_global.parameters()) + list(enc_local.parameters()),lr=LEARNING_RATE, betas=(0.9, 0.99))    \n",
    "    return enc_global, enc_local, optimizer\n",
    "enc_global, enc_local, optimizer = initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oneshot(x, Pi, N, K, D, num_samples, batch_size):\n",
    "    kls_eta_ex = torch.zeros((num_samples, batch_size))\n",
    "    kls_eta_in = torch.zeros((num_samples, batch_size))\n",
    "    kls_z_ex = torch.zeros((num_samples, batch_size))\n",
    "    kls_z_in = torch.zeros((num_samples, batch_size))\n",
    "    ##\n",
    "    log_weights = torch.zeros((num_samples, batch_size))\n",
    "\n",
    "    for l in range(num_samples):\n",
    "        q_mean, q_nu, q_alpha, q_beta, q_sigma, mus, precisions, log_q_eta = M_step(enc_global, x, N, D, K, batch_size)\n",
    "        zs_pi, z, log_q_z = E_step(enc_local, x, mus, precisions, N, D, K, batch_size)\n",
    "        log_p_joint = log_joints_gmm(x, z, Pi, mus, precisions, N, D, K, prior_mean, prior_nu, prior_alpha, prior_beta, batch_size)\n",
    "        log_weights[l] = log_p_joint - log_q_z - log_q_eta\n",
    "        ## kl of global\n",
    "        p_mean, p_nu, p_alpha, p_beta = post_global(x, z, prior_mean, prior_nu, prior_alpha, prior_beta, N, K, D, batch_size)\n",
    "        kl_eta_ex, kl_eta_in = kls_NGs(p_mean, p_nu, p_alpha, p_beta, q_mean, q_nu, q_alpha, q_beta)\n",
    "        kls_eta_ex[l] = kl_eta_ex\n",
    "        kls_eta_in[l] = kl_eta_in\n",
    "        ## kl of local\n",
    "        p_logits = post_local(x, Pi, mus, precisions, N, K, D, batch_size)\n",
    "        kl_z_ex, kl_z_in = kls_cats(p_logits, torch.log(zs_pi))\n",
    "        kls_z_ex[l] = kl_z_ex\n",
    "        kls_z_in[l] = kl_z_in   \n",
    "\n",
    "    weights = torch.exp(log_weights - logsumexp(log_weights, 0)).detach()\n",
    "    ess = (1./ (weights ** 2).sum(0)).mean()\n",
    "    ## EUBO and ELBO\n",
    "    eubo = torch.mul(weights, log_weights).sum(0).mean()\n",
    "    elbo = log_weights.mean(0).mean()\n",
    "    ## weighted average KLs for local and global posterior\n",
    "    KL_eta_ex = torch.mul(weights, kls_eta_ex).sum(0).mean()\n",
    "    KL_eta_in = torch.mul(weights, kls_eta_in).sum(0).mean()\n",
    "    KL_z_ex = torch.mul(weights, kls_z_ex).sum(0).mean()\n",
    "    KL_z_in = torch.mul(weights, kls_z_in).sum(0).mean()     \n",
    "    return eubo, elbo, ess, KL_eta_ex, KL_eta_in, KL_z_ex, KL_z_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EUBOs = []\n",
    "ELBOs = []\n",
    "ESSs = []\n",
    "KLs_eta_ex = []\n",
    "KLs_eta_in = []\n",
    "KLs_z_ex = []\n",
    "KLs_z_in = []\n",
    "\n",
    "EUBOs_gs = []\n",
    "ELBOs_gs = []\n",
    "ESSs_gs = []\n",
    "KLs_eta_ex_gs = []\n",
    "KLs_eta_in_gs = []\n",
    "KLs_z_ex_gs = []\n",
    "KLs_z_in_gs = []\n",
    "\n",
    "prior_mean, prior_nu, prior_alpha, prior_beta = init_priors(K, D, BATCH_SIZE)\n",
    "\n",
    "num_batches = int((Xs.shape[0] / BATCH_SIZE))\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    time_start = time.time()\n",
    "    indices = torch.randperm(num_seqs)\n",
    "    EUBO = 0.0\n",
    "    ELBO = 0.0\n",
    "    ESS = 0.0\n",
    "    KL_eta_ex = 0.0\n",
    "    KL_eta_in = 0.0\n",
    "    KL_z_ex = 0.0\n",
    "    KL_z_in = 0.0\n",
    "\n",
    "    for step in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "        batch_indices = indices[step*BATCH_SIZE : (step+1)*BATCH_SIZE]\n",
    "        batch_Xs = Xs[batch_indices]\n",
    "        batch_Xs = shuffler(batch_Xs, N, K, D, BATCH_SIZE)\n",
    "        eubo, elbo, ess, kl_eta_ex, kl_eta_in, kl_z_ex, kl_z_in = oneshot(batch_Xs, Pi, N, K, D, NUM_SAMPLES, BATCH_SIZE)\n",
    "        eubo.backward()\n",
    "        optimizer.step()\n",
    "        EUBO += eubo.item()\n",
    "        ELBO += elbo.item()\n",
    "        ESS += ess.item()\n",
    "        KL_eta_ex += kl_eta_ex.item()\n",
    "        KL_eta_in += kl_eta_in.item()\n",
    "        KL_z_ex += kl_z_ex.item()\n",
    "        KL_z_in += kl_z_in.item()\n",
    "        ##\n",
    "        EUBOs_gs.append(eubo.item())\n",
    "        ELBOs_gs.append(elbo.item())\n",
    "        ESSs_gs.append(ess.item())\n",
    "        ##\n",
    "        KLs_eta_ex_gs.append(kl_eta_ex.item())\n",
    "        KLs_eta_in_gs.append(kl_eta_in.item())\n",
    "        KLs_z_ex_gs.append(kl_z_ex.item())\n",
    "        KLs_z_in_gs.append(kl_z_in.item())\n",
    "        \n",
    "    EUBO /= num_batches\n",
    "    ELBO /= num_batches\n",
    "    ESS /= num_batches\n",
    "    KL_eta_ex /= num_batches\n",
    "    KL_eta_in /= num_batches\n",
    "    KL_z_ex /= num_batches\n",
    "    KL_z_in /= num_batches\n",
    "\n",
    "    EUBOs.append(EUBO)\n",
    "    ELBOs.append(ELBO)\n",
    "    ESSs.append(ESS)\n",
    "    ##\n",
    "    KLs_eta_ex.append(KL_eta_ex)\n",
    "    KLs_eta_in.append(KL_eta_in)\n",
    "    KLs_z_ex.append(KL_z_ex)\n",
    "    KLs_z_in.append(KL_z_in)\n",
    "\n",
    "    time_end = time.time()\n",
    "    time_end = time.time()\n",
    "    print('epoch=%d, EUBO=%.3f, ELBO=%.3f, ESS=%.3f, EX_eta=%.3f, IN_eta=%.3f, EX_z=%.3f, IN_z=%.3f (%ds)'\n",
    "              % (epoch, EUBO, ELBO, ESS,  KL_eta_ex, KL_eta_in, KL_z_ex, KL_z_in, time_end - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(enc_global.state_dict(), 'models/global-enc-' + PATH)\n",
    "torch.save(enc_local.state_dict(), 'models/local-enc' + PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(EUBOs, ELBOs, ESSs, KLs_eta_ex, KLs_eta_in, KLs_z_ex, KLs_z_in, PATH, 'Epoch')\n",
    "save_results(EUBOs_gs, ELBOs_gs, ESSs_gs, KLs_eta_ex_gs, KLs_eta_in_gs, KLs_z_ex_gs, KLs_z_in_gs, PATH, 'GS')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(EUBOs, ELBOs, ESSs, KLs_eta_ex, KLs_eta_in, KLs_z_ex, KLs_z_in, NUM_SAMPLES, num_batches, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_single_batch(num_seqs, N, K, D, batch_size):\n",
    "    indices = torch.randperm(num_seqs)\n",
    "    batch_indices = indices[0*batch_size : (0+1)*batch_size]\n",
    "    batch_Xs = Xs[batch_indices]\n",
    "    batch_Xs = shuffler(batch_Xs, N, K, D, batch_size)\n",
    "    return batch_Xs\n",
    "\n",
    "def test(x, num_seqs, Pi, N, K, D, batch_size):\n",
    "    q_mean, q_nu, q_alpha, q_beta, q_sigma, mus, precisions, log_q_eta = M_step(enc_global, x, N, D, K, batch_size)\n",
    "    zs_pi, z, log_q_z = E_step(enc_local, x, mus, precisions, N, D, K, batch_size)\n",
    "    E_precisions = q_alpha / q_beta\n",
    "    E_mus = q_mean\n",
    "    E_z = torch.argmax(zs_pi, dim=-1)\n",
    "\n",
    "    return E_mus, E_precisions, E_z\n",
    "\n",
    "x = sample_single_batch(num_seqs, N, K, D, BATCH_SIZE)\n",
    "E_mus, E_precisions, E_z = test(x, num_seqs, Pi, N, K, D, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_final_samples(Xs, Zs, mus, precisions, batch_size, PATH):\n",
    "    colors = ['r', 'b', 'gold']\n",
    "    fig = plt.figure(figsize=(25,50))\n",
    "    for b in range(batch_size):\n",
    "        ax = fig.add_subplot(int(batch_size / 5), 5, b+1)\n",
    "        x = Xs[b].data.numpy()\n",
    "        z = Zs[b].data.numpy()\n",
    "        mu = mus[b].data.numpy()\n",
    "        precision = precisions[b].data.numpy()\n",
    "\n",
    "        covs = np.zeros((K, D, D))\n",
    "        assignments = z\n",
    "        for k in range(K):\n",
    "            covs[k] = np.diag(1. / precision[k])\n",
    "            xk = x[np.where(assignments == k)]\n",
    "            ax.scatter(xk[:, 0], xk[:, 1], c=colors[k])\n",
    "            plot_cov_ellipse(cov=covs[k], pos=mu[k], nstd=2, ax=ax, alpha=0.2, color=colors[k])\n",
    "        ax.set_ylim([-10, 10])\n",
    "        ax.set_xlim([-10, 10])\n",
    "    plt.savefig('results/modes' + PATH + '.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_final_samples(x, E_z, E_mus, E_precisions, BATCH_SIZE, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LLs[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LLs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(LLs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLs[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
