{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from plots import *\n",
    "from kls import *\n",
    "from torch._six import inf\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal as mvn\n",
    "from torch.distributions.one_hot_categorical import OneHotCategorical as cat\n",
    "from torch.distributions.gamma import Gamma\n",
    "from torch import logsumexp\n",
    "import sys\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "K = 3\n",
    "D = 2\n",
    "\n",
    "## Model Parameters\n",
    "NUM_SAMPLES = 10\n",
    "NUM_HIDDEN = 64\n",
    "STEPS = 10\n",
    "NUM_STATS = K+D*K+D*K\n",
    "NUM_LATENTS = D * K\n",
    "NUM_OBS_GLOBAL = D + K\n",
    "NUM_OBS_LOCAL = D + K*D + K*D\n",
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 10000\n",
    "LEARNING_RATE = 1e-3\n",
    "CUDA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = torch.from_numpy(np.load('gmm_dataset/sequences.npy')).float()\n",
    "Zs_true = torch.from_numpy(np.load('gmm_dataset/states.npy')).float()\n",
    "mus_true = torch.from_numpy(np.load('gmm_dataset/means.npy')).float()\n",
    "sigma2_true = torch.from_numpy(np.load('gmm_dataset/covariances.npy')).float()\n",
    "Pi = torch.from_numpy(np.load('gmm_dataset/init.npy')).float()\n",
    "num_seqs = Xs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_global(nn.Module):\n",
    "    def __init__(self, num_obs=NUM_OBS_GLOBAL,\n",
    "                       num_stats=NUM_STATS,\n",
    "                       num_hidden=NUM_HIDDEN,\n",
    "                       num_latents=NUM_LATENTS):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.enc_stats = nn.Sequential(\n",
    "            nn.Linear(num_obs, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_stats))\n",
    "        self.enc_hidden = nn.Sequential(\n",
    "            nn.Linear(num_stats, num_hidden),\n",
    "            nn.ReLU())\n",
    "        self.sigmas_log_alpha = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        self.sigmas_log_beta = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        \n",
    "        self.enc_hidden2 = nn.Sequential(\n",
    "            nn.Linear(num_stats, num_hidden),\n",
    "            nn.ReLU())\n",
    "        self.mus_mean = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        self.mus_log_nu = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        \n",
    "    def forward(self, obs, K, D, num_samples, batch_size):\n",
    "        stats = self.enc_stats(obs).view(batch_size, N, -1).sum(1)\n",
    "        hidden = self.enc_hidden(stats)\n",
    "        q_alpha = torch.exp(self.sigmas_log_alpha(hidden)).view(-1, K, D) ## B * K * D\n",
    "        q_beta = torch.exp(self.sigmas_log_beta(hidden)).view(-1, K, D) ## B * K * D\n",
    "        precisions = Gamma(q_alpha, q_beta).sample((num_samples,)) ## S * B * K * D\n",
    "        \n",
    "        hidden2 = self.enc_hidden2(stats)                 \n",
    "        q_mean = self.mus_mean(hidden2).view(-1, K, D).unsqueeze(0).repeat(num_samples, 1, 1, 1)\n",
    "        q_nu = torch.exp(self.mus_log_nu(hidden2).view(-1, K, D))\n",
    "        q_sigma = torch.sqrt(1. / (q_nu.unsqueeze(0).repeat(num_samples, 1, 1, 1) * precisions))\n",
    "        mus = Normal(q_mean, q_sigma).sample()  \n",
    "        return q_mean, q_nu, q_alpha, q_beta, q_sigma, mus, precisions ## mus_mean and mus_sigma are S * B * K * D\n",
    "    \n",
    "class Encoder_local(nn.Module):\n",
    "    def __init__(self, num_obs=NUM_OBS_LOCAL,\n",
    "                       num_hidden=NUM_HIDDEN,\n",
    "                       num_latents=K):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.enc_onehot = nn.Sequential(\n",
    "            nn.Linear(num_obs, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_latents),\n",
    "            nn.Softmax(-1))\n",
    "        \n",
    "    def forward(self, obs, N, K, D, num_samples, batch_size):\n",
    "        zs_pi = self.enc_onehot(obs).view(batch_size, N, K)\n",
    "        zs = cat(zs_pi).sample((num_samples,))\n",
    "        log_qz = cat(zs_pi).log_prob(zs).view(num_samples, batch_size, -1).sum(-1) ## S * B\n",
    "        zs = zs.view(num_samples, batch_size, -1, K) ## S * B * N * K\n",
    "        return zs_pi, zs, log_qz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 1e-2)     \n",
    "        \n",
    "def initialize():\n",
    "    enc_global = Encoder_global()\n",
    "    enc_local = Encoder_local()\n",
    "    enc_global.apply(weights_init)\n",
    "\n",
    "    optimizer =  torch.optim.Adam(list(enc_global.parameters()) + list(enc_local.parameters()),lr=LEARNING_RATE, betas=(0.9, 0.99))    \n",
    "    return enc_global, enc_local, optimizer\n",
    "enc_global, enc_local, optimizer = initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_mean = torch.zeros((BATCH_SIZE, K, D))\n",
    "prior_nu = torch.ones((BATCH_SIZE, K, D))\n",
    "prior_alpha = torch.ones((BATCH_SIZE, K, D)) * 2.0\n",
    "prior_beta = torch.ones((BATCH_SIZE, K, D)) * 2.0\n",
    "\n",
    "def log_joints_gmm(X, Z, Pi, mus, precisions, N, D, K, prior_mean, prior_nu, prior_alpha, prior_beta, batch_size):\n",
    "    log_probs = torch.zeros(batch_size).float()\n",
    "    ## priors on mus and sigmas size B\n",
    "    log_probs = log_probs + Gamma(prior_alpha, prior_beta).log_prob(precisions).sum(-1).sum(-1)\n",
    "    prior_sigma = 1. / torch.sqrt(prior_nu * precisions)\n",
    "    log_probs = log_probs + Normal(prior_mean, prior_sigma).log_prob(mus).sum(-1).sum(-1)\n",
    "    ## Z B-by-T-by-K\n",
    "    log_probs = log_probs + cat(Pi).log_prob(Z).sum(-1)\n",
    "    labels = Z.nonzero()\n",
    "    sigmas = 1. / torch.sqrt(precisions)\n",
    "    log_probs = log_probs + Normal(mus[labels[:, 0], labels[:, -1], :].view(batch_size, N, D), \n",
    "                                   sigmas[labels[:, 0], labels[:, -1], :].view(batch_size, N, D)).log_prob(X).sum(-1).sum(-1)\n",
    "    return log_probs\n",
    "\n",
    "def inti_global(K, D, prior_mean, prior_nu, prior_alpha, prior_beta, batch_size):\n",
    "    precisions = Gamma(prior_alpha, prior_beta).sample()\n",
    "    prior_sigma = 1. / torch.sqrt(prior_nu * precisions)\n",
    "    mus = Normal(prior_mean, prior_sigma).sample()\n",
    "    ## log prior size B\n",
    "    log_p =  Normal(prior_mean, prior_sigma).log_prob(mus).sum(-1).sum(-1) + Gamma(prior_alpha, prior_beta).log_prob(precisions).sum(-1).sum(-1)\n",
    "    return mus, precisions, log_p\n",
    "\n",
    "def E_step(X, mus, precisions, N, D, K, batch_size):\n",
    "    mus_flat = mus.view(-1, K*D).unsqueeze(1).repeat(1, N, 1)\n",
    "    sigma = 1. / torch.sqrt(precisions)\n",
    "    sigma_flat = sigma.view(-1, K*D).unsqueeze(1).repeat(1, N, 1)\n",
    "    data = torch.cat((X, mus_flat, sigma_flat), -1).view(batch_size*N, -1)\n",
    "    zs_pi, zs, log_q_z = enc_local(data, N, K, D, 1, batch_size)\n",
    "    return zs_pi, zs[0], log_q_z[0]\n",
    "\n",
    "def M_step(X, z, N, D, K, batch_size):\n",
    "    data = torch.cat((X, z), dim=-1).view(batch_size*N, -1)\n",
    "    q_mean, q_nu, q_alpha, q_beta, q_sigma, mus, precisions = enc_global(data, K, D, 1, batch_size)  \n",
    "    log_q_eta =  Normal(q_mean[0], q_sigma[0]).log_prob(mus[0]).sum(-1).sum(-1) + Gamma(q_alpha, q_beta).log_prob(precisions[0]).sum(-1).sum(-1)## B\n",
    "    return q_mean[0], q_nu, q_alpha, q_beta, q_sigma[0], mus[0], precisions[0], log_q_eta\n",
    "\n",
    "def post_global(Xs, Zs, prior_mean, prior_nu, prior_alpha, prior_beta, N, K, D, batch_size):\n",
    "    stat1 = Zs.sum(1).unsqueeze(-1).repeat(1, 1, D) ## B * K * D\n",
    "    xz_nk = torch.mul(Zs.unsqueeze(-1).repeat(1, 1, 1, D), Xs.unsqueeze(-1).repeat(1, 1, 1, K).transpose(-1, -2)) # B*N*K*D\n",
    "    stat2 = xz_nk.sum(1) ## B*K*D\n",
    "    stat3 = torch.mul(Zs.unsqueeze(-1).repeat(1, 1, 1, D), torch.mul(Xs, Xs).unsqueeze(-1).repeat(1, 1, 1, K).transpose(-1, -2)).sum(1) \n",
    "    stat1_nonzero = stat1\n",
    "    stat1_nonzero[stat1_nonzero == 0.0] = 1.0\n",
    "    x_bar = stat2 / stat1\n",
    "    posterior_beta = prior_beta + (stat3 - (stat2 ** 2) / stat1_nonzero) / 2. + (stat1 * prior_nu / (stat1 + prior_nu)) * ((prior_nu**2) + x_bar**2 - 2 * x_bar *  prior_nu) / 2.\n",
    "    posterior_nu = prior_nu + stat1\n",
    "    posterior_mean = (prior_mean * prior_nu + stat2) / (prior_nu + stat1) \n",
    "    posterior_alpha = prior_alpha + (stat1 / 2.)\n",
    "#     posterior_sigma = torch.sqrt(posterior_nu * (posterior_beta / posterior_alpha))\n",
    "    return posterior_mean, posterior_nu, posterior_alpha, posterior_beta\n",
    "\n",
    "def post_local(Xs, Pi, mus, precisions, N, K, D, batch_size):\n",
    "    sigma = 1. / torch.sqrt(precisions)\n",
    "    mus_expand = mus.unsqueeze(2).repeat(1, 1, N, 1)\n",
    "    sigma_expand = sigma.unsqueeze(2).repeat(1, 1, N, 1)\n",
    "    Xs_expand = Xs.unsqueeze(1).repeat(1, K, 1, 1)\n",
    "    log_gammas = Normal(mus_expand, sigma_expand).log_prob(Xs_expand).sum(-1).transpose(-1, -2) # B * N * K\n",
    "    log_pis = log_gammas - logsumexp(log_gammas, dim=-1).unsqueeze(-1)\n",
    "    return log_pis\n",
    "    \n",
    "\n",
    "\n",
    "def ag(Xs, Pi, Zs_true, mus_true, sigma2_true, N, K, D, num_samples, steps, batch_size):\n",
    "    \"\"\"\n",
    "    train both encoders\n",
    "    rws gradient estimator\n",
    "    sis sampling scheme\n",
    "    no resampling\n",
    "    \"\"\"\n",
    "    kls_eta_ex = torch.zeros((num_samples, batch_size))\n",
    "    kls_eta_in = torch.zeros((num_samples, batch_size))\n",
    "    kls_z_ex = torch.zeros((num_samples, batch_size))\n",
    "    kls_z_in = torch.zeros((num_samples, batch_size))\n",
    "    log_increment_weights = torch.zeros((steps, num_samples, batch_size))\n",
    "    Z_samples = torch.zeros((num_samples, batch_size, N, K))\n",
    "    for m in range(steps):\n",
    "        if m == 0:\n",
    "            for l in range(num_samples):\n",
    "                mus, precisions, log_p_eta = inti_global(K, D, prior_mean, prior_nu, prior_alpha, prior_beta, batch_size)\n",
    "                zs_pi, z, log_q_z = E_step(Xs, mus, precisions, N, D, K, batch_size)\n",
    "                Z_samples[l] = z\n",
    "                labels = z.nonzero()\n",
    "                log_p_z = cat(Pi).log_prob(z).sum(-1)\n",
    "                sigmas = 1. / torch.sqrt(precisions)\n",
    "                log_p_x = Normal(mus[labels[:, 0], labels[:, -1], :].view(batch_size, N, D), sigmas[labels[:, 0], labels[:, -1], :].view(batch_size, N, D)).log_prob(Xs).sum(-1).sum(-1)\n",
    "                log_increment_weights[m, l] = log_p_x + log_p_z - log_q_z     \n",
    "        else:\n",
    "            for l in range(num_samples):\n",
    "                z_prev = Z_samples[l]\n",
    "                q_mean, q_nu, q_alpha, q_beta, q_sigma, mus, precisions, log_q_eta = M_step(Xs, z_prev, N, D, K, batch_size)\n",
    "                zs_pi, z, log_q_z = E_step(Xs, mus, precisions, N, D, K, batch_size)\n",
    "                Z_samples[l] = z\n",
    "                log_p_joint = log_joints_gmm(Xs, z, Pi, mus, precisions, N, D, K, prior_mean, prior_nu, prior_alpha, prior_beta, batch_size)\n",
    "                log_increment_weights[m, l] = log_p_joint - log_q_z - log_q_eta\n",
    "                if m == (steps-1):\n",
    "                    ## kl of global\n",
    "                    p_mean, p_nu, p_alpha, p_beta = post_global(Xs, z, prior_mean, prior_nu, prior_alpha, prior_beta, N, K, D, batch_size)\n",
    "                    kl_eta_ex, kl_eta_in = kls_NGs(p_mean, p_nu, p_alpha, p_beta, q_mean, q_nu, q_alpha, q_beta)\n",
    "                    kls_eta_ex[l] = kl_eta_ex\n",
    "                    kls_eta_in[l] = kl_eta_in\n",
    "                    ## kl of local\n",
    "                    p_logits = post_local(Xs, Pi, mus, precisions, N, K, D, batch_size)\n",
    "                    kl_z_ex, kl_z_in = kls_cats(p_logits, torch.log(zs_pi))\n",
    "                    kls_z_ex[l] = kl_z_ex\n",
    "                    kls_z_in[l] = kl_z_in         \n",
    "\n",
    "    increment_weights = torch.exp(log_increment_weights - logsumexp(log_increment_weights, 1).unsqueeze(1).repeat(1, num_samples, 1)).detach()\n",
    "    ess = (1./ (increment_weights ** 2).sum(1)).mean(0).mean()\n",
    "    ## EUBO and ELBO\n",
    "    eubo = torch.mul(increment_weights, log_increment_weights).sum(1).mean(0).mean()\n",
    "    elbo = log_increment_weights.mean(1).mean(0).mean()\n",
    "    ## weighted average KLs for local and global posterior\n",
    "    final_weights = increment_weights[-1]\n",
    "    KL_eta_ex = torch.mul(final_weights, kls_eta_ex).sum(0).mean()\n",
    "    KL_eta_in = torch.mul(final_weights, kls_eta_in).sum(0).mean()\n",
    "    KL_z_ex = torch.mul(final_weights, kls_z_ex).sum(0).mean()\n",
    "    KL_z_in = torch.mul(final_weights, kls_z_in).sum(0).mean()    \n",
    "    return eubo, elbo, ess, KL_eta_ex, KL_eta_in, KL_z_ex, KL_z_in\n",
    "\n",
    "def shuffler(batch_Xs, batch_Zs, N, K, D, batch_size):\n",
    "    indices = torch.cat([torch.randperm(N).unsqueeze(0) for b in range(batch_size)])\n",
    "    indices_Xs = indices.unsqueeze(-1).repeat(1, 1, D)\n",
    "    indices_Zs = indices.unsqueeze(-1).repeat(1, 1, K)\n",
    "    return torch.gather(batch_Xs, 1, indices_Xs), torch.gather(batch_Zs, 1, indices_Zs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, EUBO=-317.976, ELBO=-595.914, ESS=1.024, KL_eta_ex=408.927, KL_eta_in=16.563, KL_z_ex=nan, KL_z_in=25.482 (7s)\n",
      "epoch=10, EUBO=-202.308, ELBO=-250.401, ESS=1.234, KL_eta_ex=39.251, KL_eta_in=53.651, KL_z_ex=inf, KL_z_in=8.053 (67s)\n",
      "epoch=20, EUBO=-192.872, ELBO=-240.199, ESS=1.242, KL_eta_ex=41.294, KL_eta_in=53.934, KL_z_ex=3.478, KL_z_in=3.058 (64s)\n",
      "epoch=30, EUBO=-185.938, ELBO=-229.874, ESS=1.291, KL_eta_ex=34.686, KL_eta_in=38.446, KL_z_ex=2.537, KL_z_in=2.504 (63s)\n",
      "epoch=40, EUBO=-183.175, ELBO=-226.286, ESS=1.295, KL_eta_ex=33.657, KL_eta_in=33.461, KL_z_ex=2.301, KL_z_in=2.152 (63s)\n",
      "epoch=50, EUBO=-180.114, ELBO=-222.598, ESS=1.337, KL_eta_ex=32.309, KL_eta_in=30.260, KL_z_ex=1.820, KL_z_in=1.828 (63s)\n",
      "epoch=60, EUBO=-179.389, ELBO=-221.447, ESS=1.350, KL_eta_ex=31.592, KL_eta_in=28.624, KL_z_ex=inf, KL_z_in=1.632 (63s)\n",
      "epoch=70, EUBO=-178.595, ELBO=-220.850, ESS=1.331, KL_eta_ex=33.420, KL_eta_in=28.544, KL_z_ex=inf, KL_z_in=1.505 (63s)\n",
      "epoch=80, EUBO=-177.650, ELBO=-219.850, ESS=1.343, KL_eta_ex=32.991, KL_eta_in=27.039, KL_z_ex=inf, KL_z_in=1.327 (63s)\n",
      "epoch=90, EUBO=-177.019, ELBO=-218.323, ESS=1.355, KL_eta_ex=33.626, KL_eta_in=25.452, KL_z_ex=1.301, KL_z_in=1.262 (63s)\n",
      "epoch=100, EUBO=-176.620, ELBO=-217.761, ESS=1.350, KL_eta_ex=33.254, KL_eta_in=26.295, KL_z_ex=inf, KL_z_in=1.277 (63s)\n",
      "epoch=110, EUBO=-175.858, ELBO=-216.296, ESS=1.358, KL_eta_ex=33.258, KL_eta_in=24.371, KL_z_ex=inf, KL_z_in=1.132 (63s)\n",
      "epoch=120, EUBO=-174.034, ELBO=-212.028, ESS=1.394, KL_eta_ex=29.094, KL_eta_in=20.870, KL_z_ex=inf, KL_z_in=1.137 (63s)\n",
      "epoch=130, EUBO=-172.444, ELBO=-209.082, ESS=1.423, KL_eta_ex=28.026, KL_eta_in=16.242, KL_z_ex=inf, KL_z_in=1.064 (62s)\n",
      "epoch=140, EUBO=-171.549, ELBO=-207.302, ESS=1.426, KL_eta_ex=27.043, KL_eta_in=16.304, KL_z_ex=inf, KL_z_in=1.014 (62s)\n",
      "epoch=150, EUBO=-171.136, ELBO=-205.497, ESS=1.457, KL_eta_ex=25.380, KL_eta_in=16.221, KL_z_ex=inf, KL_z_in=1.033 (63s)\n",
      "epoch=160, EUBO=-170.138, ELBO=-204.773, ESS=1.453, KL_eta_ex=25.432, KL_eta_in=15.703, KL_z_ex=inf, KL_z_in=0.973 (62s)\n",
      "epoch=170, EUBO=-169.795, ELBO=-203.654, ESS=1.468, KL_eta_ex=25.189, KL_eta_in=15.709, KL_z_ex=inf, KL_z_in=0.932 (62s)\n",
      "epoch=180, EUBO=-169.639, ELBO=-203.429, ESS=1.471, KL_eta_ex=25.299, KL_eta_in=15.130, KL_z_ex=inf, KL_z_in=0.913 (62s)\n",
      "epoch=190, EUBO=-169.217, ELBO=-202.704, ESS=1.468, KL_eta_ex=24.576, KL_eta_in=15.175, KL_z_ex=inf, KL_z_in=0.889 (62s)\n",
      "epoch=200, EUBO=-168.801, ELBO=-202.373, ESS=1.481, KL_eta_ex=24.871, KL_eta_in=14.984, KL_z_ex=inf, KL_z_in=0.836 (62s)\n",
      "epoch=210, EUBO=-168.264, ELBO=-201.446, ESS=1.488, KL_eta_ex=24.451, KL_eta_in=14.432, KL_z_ex=inf, KL_z_in=0.925 (62s)\n",
      "epoch=220, EUBO=-168.089, ELBO=-200.158, ESS=1.507, KL_eta_ex=23.536, KL_eta_in=14.668, KL_z_ex=inf, KL_z_in=0.771 (63s)\n"
     ]
    }
   ],
   "source": [
    "EUBOs = []\n",
    "ELBOs = []\n",
    "ESSs = []\n",
    "KLs_eta_ex = []\n",
    "KLs_eta_in = []\n",
    "KLs_z_ex = []\n",
    "KLs_z_in = []\n",
    "\n",
    "num_batches = int((Xs.shape[0] / BATCH_SIZE))\n",
    "time_start = time.time()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    indices = torch.randperm(num_seqs)\n",
    "    EUBO = 0.0\n",
    "    ELBO = 0.0\n",
    "    ESS = 0.0\n",
    "    KL_eta_ex = 0.0\n",
    "    KL_eta_in = 0.0\n",
    "    KL_z_ex = 0.0\n",
    "    KL_z_in = 0.0\n",
    "    for step in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "        batch_indices = indices[step*BATCH_SIZE : (step+1)*BATCH_SIZE]\n",
    "        batch_Xs = Xs[batch_indices]\n",
    "        batch_Zs = Zs_true[batch_indices]\n",
    "        batch_mus = mus_true[batch_indices]\n",
    "        batch_sigma2 = sigma2_true[batch_indices]\n",
    "        batch_Xs, batch_Zs = shuffler(batch_Xs, batch_Zs, N, K, D, BATCH_SIZE)\n",
    "        eubo, elbo, ess, kl_eta_ex, kl_eta_in, kl_z_ex, kl_z_in = ag(batch_Xs, Pi, batch_Zs, batch_mus, batch_sigma2, N, K, D, NUM_SAMPLES, STEPS, BATCH_SIZE)\n",
    "        eubo.backward()\n",
    "        optimizer.step()\n",
    "        EUBO += eubo.item()\n",
    "        ELBO += elbo.item()\n",
    "        ESS += ess.item()\n",
    "        KL_eta_ex += kl_eta_ex.item()\n",
    "        KL_eta_in += kl_eta_in.item()\n",
    "        KL_z_ex += kl_z_ex.item()\n",
    "        KL_z_in += kl_z_in.item()\n",
    "        \n",
    "    EUBO /= num_batches\n",
    "    ELBO /= num_batches\n",
    "    ESS /= num_batches\n",
    "    KL_eta_ex /= num_batches\n",
    "    KL_eta_in /= num_batches\n",
    "    KL_z_ex /= num_batches\n",
    "    KL_z_in /= num_batches\n",
    "    \n",
    "    EUBOs.append(EUBO)\n",
    "    ELBOs.append(ELBO)\n",
    "    ESSs.append(ESS)\n",
    "    KLs_eta_ex.append(KL_eta_ex)\n",
    "    KLs_eta_in.append(KL_eta_in)\n",
    "    KLs_z_ex.append(KL_z_ex)\n",
    "    KLs_z_in.append(KL_z_in)\n",
    "\n",
    "    time_end = time.time()\n",
    "    if epoch % 10 == 0:\n",
    "        time_end = time.time()\n",
    "        print('epoch=%d, EUBO=%.3f, ELBO=%.3f, ESS=%.3f, KL_eta_ex=%.3f, KL_eta_in=%.3f, KL_z_ex=%.3f, KL_z_in=%.3f (%ds)' % (epoch, EUBO, ELBO, ESS,  KL_eta_ex, KL_eta_in, KL_z_ex, KL_z_in, time_end - time_start))\n",
    "        time_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(enc_global.state_dict(), 'models/gmm-enc-global-steps=%d-samples=%d-lr=%d' % (STEPS, NUM_SAMPLES, LEARNING_RATE))\n",
    "torch.save(enc_local.state_dict(), 'models/gmm-enc-local-steps=%d-samples=%d-lr=%d' % (STEPS, NUM_SAMPLES, LEARNING_RATE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(EUBOs, ELBOs, ESSs, KLs_ex, KLs_in, NUM_SAMPLES, NUM_EPOCHS, LEARNING_RATE):\n",
    "    fout = open('gmm_steps=%d-samples=%d-lr=%d.txt' % (STEPS, NUM_SAMPLES, LEARNING_RATE), 'w+')\n",
    "    fout.write('EUBOs, ELBOs, ESSs, KLs_EX, KLs_IN\\n')\n",
    "    for i in range(len(EUBOs)):\n",
    "        fout.write(str(EUBOs[i]) + ', ' + str(ELBOs[i]) + ', ' + str(ESSs[i]) + str(KLs_ex[i]) + str(KLs_in[i]) + '\\n')\n",
    "    fout.close()\n",
    "save_results(EUBOs, ELBOs, ESSs, KLs_ex, KLs_in, NUM_SAMPLES, NUM_EPOCHS, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(EUBOs, ELBOs, ESSs, KLs_ex, KLs_in, num_samples, num_epochs, lr):\n",
    "    fig = plt.figure(figsize=(20, 20))\n",
    "    fig.tight_layout()\n",
    "    ax1 = fig.add_subplot(3, 1, 1)\n",
    "    ax2 = fig.add_subplot(3, 1, 2)\n",
    "    ax3 = fig.add_subplot(3, 1, 3)\n",
    "    ax1.plot(EUBOs, 'r', label='EUBOs')\n",
    "    ax1.plot(ELBOs, 'b', label='ELBOs')\n",
    "    ax2.plot(KLs_ex, '#66b3ff', label='KLs_mus_ex')\n",
    "    ax2.plot(KLs_in, '#ff9999', label='KLs_mus_in')\n",
    "    \n",
    "    ax1.tick_params(labelsize=18)\n",
    "    ax3.plot(np.array(ESSs) / num_samples, 'm', label='ESS')\n",
    "    ax1.set_title('epoch=%d, batch_size=%d, lr=%.1E, samples=%d' % (num_epochs, BATCH_SIZE, lr, num_samples), fontsize=18)\n",
    "    ax1.set_ylim([-300, -80])\n",
    "    ax1.legend()\n",
    "    ax2.set_ylim([-1, 350])\n",
    "    ax2.legend()\n",
    "    ax3.legend()\n",
    "    ax2.tick_params(labelsize=18)\n",
    "    ax3.tick_params(labelsize=18)\n",
    "    plt.savefig('ag-all-steps_lr=%.1E_samples=%d.svg' % (lr, num_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(EUBOs, ELBOs, ESSs, KLs_ex, KLs_in, NUM_SAMPLES, NUM_EPOCHS, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "STEPS = 20\n",
    "\n",
    "def sample_single_batch(num_seqs, N, K, D, batch_size):\n",
    "    indices = torch.randperm(num_seqs)\n",
    "    batch_indices = indices[0*batch_size : (0+1)*batch_size]\n",
    "    batch_Xs = Xs[batch_indices]\n",
    "    batch_Zs = Zs_true[batch_indices]\n",
    "    batch_Xs, batch_Zs = shuffler(batch_Xs, batch_Zs, N, K, D, batch_size)\n",
    "    return batch_Xs, batch_Zs\n",
    "\n",
    "def test(x, num_seqs, Pi, N, K, D, steps, batch_size):\n",
    "    LLs = [] \n",
    "    for m in range(steps):\n",
    "        if m == 0:\n",
    "            mus, precisions, log_p_eta = inti_global(K, D, prior_mean, prior_nu, prior_alpha, prior_beta, batch_size)\n",
    "            zs_pi, z, log_q_z = E_step(x, mus, precisions, N, D, K, batch_size)\n",
    "        else:\n",
    "            q_mean, q_nu, q_alpha, q_beta, q_sigma, mus, precisions, log_q_eta = M_step(x, z, N, D, K, batch_size)\n",
    "            zs_pi, z, log_q_z = E_step(x, mus, precisions, N, D, K, batch_size)\n",
    "            labels = z.nonzero()\n",
    "            sigmas = 1. / torch.sqrt(precisions)\n",
    "            ll = Normal(mus[labels[:, 0], labels[:, -1], :].view(batch_size, N, D), sigmas[labels[:, 0], labels[:, -1], :].view(batch_size, N, D)).log_prob(x).sum(-1).sum(-1).mean()\n",
    "            LLs.append(ll.item())\n",
    "    E_precisions = q_alpha / q_beta\n",
    "    E_mus = q_mean\n",
    "    E_z = torch.argmax(zs_pi, dim=-1)\n",
    "\n",
    "    return z, mus, precisions, LLs, E_mus, E_precisions, E_z\n",
    "\n",
    "x,z_true = sample_single_batch(num_seqs, N, K, D, BATCH_SIZE)\n",
    "z, mus, precisions, LLs, E_mus, E_precisions, E_z = test(x, num_seqs, Pi, N, K, D, STEPS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_final_samples(Xs, Zs, mus, precisions, steps, batch_size):\n",
    "    colors = ['r', 'b', 'gold']\n",
    "    fig = plt.figure(figsize=(25,100))\n",
    "    for b in range(batch_size):\n",
    "        ax = fig.add_subplot(int(batch_size / 5), 5, b+1)\n",
    "        x = Xs[b].data.numpy()\n",
    "        z = Zs[b].data.numpy()\n",
    "        mu = mus[b].data.numpy()\n",
    "        precision = precisions[b].data.numpy()\n",
    "\n",
    "        covs = np.zeros((K, D, D))\n",
    "        assignments = z\n",
    "        for k in range(K):\n",
    "            covs[k] = np.diag(1. / precision[k])\n",
    "            xk = x[np.where(assignments == k)]\n",
    "            ax.scatter(xk[:, 0], xk[:, 1], c=colors[k])\n",
    "            plot_cov_ellipse(cov=covs[k], pos=mu[k], nstd=2, ax=ax, alpha=0.2, color=colors[k])\n",
    "        ax.set_ylim([-10, 10])\n",
    "        ax.set_xlim([-10, 10])\n",
    "    plt.savefig('modes_steps=%d.svg' % (steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_final_samples(x, E_z, E_mus, E_precisions, STEPS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LLS[:15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LLS[:80])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "E_mus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
