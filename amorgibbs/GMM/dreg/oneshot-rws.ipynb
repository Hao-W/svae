{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from plots import *\n",
    "from kls import *\n",
    "from utils import *\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal as mvn\n",
    "from torch.distributions.one_hot_categorical import OneHotCategorical as cat\n",
    "from torch.distributions.gamma import Gamma\n",
    "from torch import logsumexp\n",
    "import sys\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "K = 3\n",
    "D = 2\n",
    "\n",
    "## Model Parameters\n",
    "NUM_SAMPLES = 20\n",
    "NUM_HIDDEN = 32\n",
    "NUM_STATS = K+D*K+D*K\n",
    "NUM_LATENTS = D * K\n",
    "NUM_OBS_GLOBAL = D\n",
    "NUM_OBS_LOCAL = D + K*D + K*D\n",
    "BATCH_SIZE = 50\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "CUDA = False\n",
    "PATH = 'oneshot-rws'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = torch.from_numpy(np.load('gmm_dataset/sequences.npy')).float()\n",
    "Pi = torch.from_numpy(np.load('gmm_dataset/init.npy')).float()\n",
    "num_seqs = Xs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_init(nn.Module):\n",
    "    def __init__(self, num_obs=NUM_OBS_GLOBAL,\n",
    "                       num_stats=NUM_STATS,\n",
    "                       num_hidden=NUM_HIDDEN,\n",
    "                       num_latents=NUM_LATENTS):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.enc_stats = nn.Sequential(\n",
    "            nn.Linear(num_obs, num_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(num_hidden, num_stats))\n",
    "\n",
    "        self.sigmas_log_alpha = nn.Sequential(\n",
    "            nn.Linear(num_stats, num_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        self.sigmas_log_beta = nn.Sequential(\n",
    "            nn.Linear(num_stats, num_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        \n",
    "        self.mus_mean = nn.Sequential(\n",
    "            nn.Linear(num_stats, num_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        self.mus_log_nu = nn.Sequential(\n",
    "            nn.Linear(num_stats, num_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        \n",
    "        self.prior_mean = torch.zeros(K*D)\n",
    "        self.prior_nu = torch.ones(K*D) * 0.3\n",
    "        self.prior_alpha = torch.ones(K*D) * 3.0\n",
    "        self.prior_beta = torch.ones(K*D) * 3.0\n",
    "        \n",
    "    def forward(self, obs):\n",
    "        stats = self.enc_stats(obs).sum(-2) ## B * N * D --> B * STATS_DIM \n",
    "        q_alpha = self.sigmas_log_alpha(stats).exp() ## B * K * D\n",
    "        q_beta = self.sigmas_log_beta(stats).exp() ## B * K * D\n",
    "        q_precisions = Gamma(q_alpha, q_beta)\n",
    "        precisions = q_precisions.sample() ##Non-Reparam\n",
    "        log_q_precisions = q_precisions.log_prob(precisions).sum(-1)\n",
    "        log_p_precisions = Gamma(self.prior_alpha, self.prior_beta).log_prob(precisions).sum(-1)\n",
    "        ##\n",
    "        q_mean = self.mus_mean(stats)\n",
    "        q_nu = self.mus_log_nu(stats).exp()\n",
    "        q_sigma = 1. / (q_nu * precisions).sqrt() ## S * B * (K*D)\n",
    "        q_means = Normal(q_mean, q_sigma)\n",
    "        means = q_means.sample()  \n",
    "        log_q_means = q_means.log_prob(means).sum(-1)\n",
    "        log_p_means = Normal(self.prior_mean, 1. / (self.prior_nu * precisions).sqrt()).log_prob(means).sum(-1)\n",
    "        return q_alpha, q_beta, q_mean, q_nu, means, precisions, log_q_means+log_q_precisions,  log_p_means+log_p_precisions## mus_mean and mus_sigma are S * B * K * D\n",
    "    \n",
    "class Encoder_local(nn.Module):\n",
    "    def __init__(self, num_obs=NUM_OBS_LOCAL,\n",
    "                       num_hidden=NUM_HIDDEN,\n",
    "                       num_latents=K):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.enc_onehot = nn.Sequential(\n",
    "            nn.Linear(num_obs, num_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(num_hidden, num_latents),\n",
    "            nn.Softmax(-1))\n",
    "        \n",
    "        self.prior_pi = torch.ones(K) * (1./ K)\n",
    "        \n",
    "    def forward(self, obs, means, precisions):\n",
    "        means_flat = means.unsqueeze(-2).repeat(1, 1, N, 1)\n",
    "        precisions_flat = precisions.unsqueeze(-2).repeat(1, 1, N, 1)\n",
    "        data = torch.cat((obs, means_flat, precisions_flat), -1)\n",
    "        \n",
    "        q_pi = self.enc_onehot(data)\n",
    "        q_zs = cat(q_pi)\n",
    "        zs = q_zs.sample()\n",
    "        log_q_z = q_zs.log_prob(zs).sum(-1) ## S * B\n",
    "        log_p_z = cat(self.prior_pi).log_prob(zs).sum(-1)\n",
    "        return q_pi, zs, log_q_z, log_p_z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    enc_init = Encoder_init()\n",
    "    enc_local = Encoder_local()\n",
    "\n",
    "    optimizer =  torch.optim.Adam(list(enc_init.parameters()) + list(enc_local.parameters()),lr=LEARNING_RATE, betas=(0.9, 0.99))    \n",
    "    return enc_init, enc_local, optimizer\n",
    "enc_init, enc_local, optimizer = initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rws(enc_init, enc_local, x, K, D, num_samples, batch_size):\n",
    "    q_alpha, q_beta, q_mean, q_nu, means, precisions, log_q_eta, log_p_eta = enc_init(batch_Xs) \n",
    "    q_pi, zs, log_q_z, log_p_z = enc_local(batch_Xs, means, precisions)\n",
    "\n",
    "    mus_flat = means.view(num_samples, batch_size, K, D)\n",
    "    precisions_flat = precisions.view(num_samples, batch_size, K, D)\n",
    "    ll = loglikelihood(batch_Xs, zs, mus_flat, precisions_flat, D)\n",
    "    log_weights = log_p_eta + log_p_z - log_q_eta - log_q_z + ll\n",
    "\n",
    "    weights = torch.exp(log_weights - logsumexp(log_weights, dim=0)).detach()\n",
    "    eubo = torch.mul(weights, log_weights).sum(0).mean()\n",
    "    elbo = log_weights.mean()\n",
    "    ess = (1. / (weights ** 2).sum(0)).mean()\n",
    "    ## KL\n",
    "    #### prior parameters\n",
    "    p_alpha = enc_init.prior_alpha.view(K, D).unsqueeze(0).unsqueeze(0).repeat(num_samples, batch_size, 1, 1)\n",
    "    p_beta = enc_init.prior_beta.view(K, D).unsqueeze(0).unsqueeze(0).repeat(num_samples, batch_size, 1, 1)\n",
    "    p_mean = enc_init.prior_mean.view(K, D).unsqueeze(0).unsqueeze(0).repeat(num_samples, batch_size, 1, 1)\n",
    "    p_nu = enc_init.prior_nu.view(K, D).unsqueeze(0).unsqueeze(0).repeat(num_samples, batch_size, 1, 1)\n",
    "    p_pi = enc_local.prior_pi.unsqueeze(0).unsqueeze(0).unsqueeze(0).repeat(num_samples, batch_size, N, 1)\n",
    "    #### variational parameters\n",
    "    q_mean_f = q_mean.view(num_samples, batch_size, K, D)\n",
    "    q_nu_f = q_nu.view(num_samples, batch_size, K, D)\n",
    "    q_alpha_f = q_alpha.view(num_samples, batch_size, K, D)\n",
    "    q_beta_f = q_beta.view(num_samples, batch_size, K, D)\n",
    "    means_f = means.view(num_samples, batch_size, K, D)\n",
    "    precisions_f = precisions.view(num_samples, batch_size, K, D)\n",
    "    ####\n",
    "    kl_eta_ex, kl_eta_in, kl_z_ex, kl_z_in = kls_step(x, zs, q_mean_f, q_nu_f, q_alpha_f, q_beta_f, q_pi, p_mean, p_nu, p_alpha, p_beta, p_pi, means_f, precisions_f, N, K, D, batch_size)\n",
    "    ##\n",
    "    KL_eta_ex = torch.mul(weights, kl_eta_ex).sum(0).mean()\n",
    "    KL_eta_in = torch.mul(weights, kl_eta_in).sum(0).mean()\n",
    "    KL_z_ex = torch.mul(weights, kl_z_ex).sum(0).mean()\n",
    "    KL_z_in = torch.mul(weights, kl_z_in).sum(0).mean() \n",
    "    return eubo, elbo, ess, KL_eta_ex, KL_eta_in, KL_z_ex, KL_z_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, EUBO=-235.098, ELBO=-423.718, ESS=1.102, EX_eta=250.614, IN_eta=16.875, EX_z=56.367, IN_z=13.732 (12s)\n",
      "epoch=1, EUBO=-192.484, ELBO=-264.407, ESS=1.195, EX_eta=93.007, IN_eta=16.426, EX_z=21.209, IN_z=9.474 (12s)\n",
      "epoch=2, EUBO=-184.828, ELBO=-231.280, ESS=1.303, EX_eta=62.678, IN_eta=19.956, EX_z=15.453, IN_z=7.779 (12s)\n",
      "epoch=3, EUBO=-181.203, ELBO=-217.908, ESS=1.370, EX_eta=51.681, IN_eta=22.516, EX_z=12.416, IN_z=6.396 (12s)\n",
      "epoch=4, EUBO=-178.246, ELBO=-211.961, ESS=1.381, EX_eta=48.762, IN_eta=22.689, EX_z=11.489, IN_z=5.807 (12s)\n",
      "epoch=5, EUBO=-175.025, ELBO=-207.533, ESS=1.383, EX_eta=47.547, IN_eta=24.578, EX_z=9.323, IN_z=5.426 (12s)\n",
      "epoch=6, EUBO=-172.762, ELBO=-204.018, ESS=1.393, EX_eta=47.468, IN_eta=25.860, EX_z=9.401, IN_z=4.950 (12s)\n",
      "epoch=7, EUBO=-169.858, ELBO=-200.832, ESS=1.388, EX_eta=46.760, IN_eta=27.156, EX_z=7.605, IN_z=4.429 (12s)\n",
      "epoch=8, EUBO=-167.732, ELBO=-199.761, ESS=1.381, EX_eta=48.204, IN_eta=25.922, EX_z=7.035, IN_z=4.426 (12s)\n",
      "epoch=9, EUBO=-166.118, ELBO=-197.565, ESS=1.414, EX_eta=47.866, IN_eta=25.722, EX_z=6.620, IN_z=4.149 (11s)\n",
      "epoch=10, EUBO=-165.441, ELBO=-195.988, ESS=1.406, EX_eta=47.260, IN_eta=26.017, EX_z=5.901, IN_z=3.902 (11s)\n",
      "epoch=11, EUBO=-164.102, ELBO=-194.194, ESS=1.427, EX_eta=45.831, IN_eta=26.434, EX_z=5.411, IN_z=3.968 (12s)\n",
      "epoch=12, EUBO=-162.826, ELBO=-192.129, ESS=1.446, EX_eta=44.859, IN_eta=24.167, EX_z=5.039, IN_z=3.693 (12s)\n",
      "epoch=13, EUBO=-161.967, ELBO=-191.956, ESS=1.394, EX_eta=45.155, IN_eta=23.159, EX_z=5.141, IN_z=3.744 (12s)\n",
      "epoch=14, EUBO=-161.235, ELBO=-190.347, ESS=1.413, EX_eta=44.030, IN_eta=22.541, EX_z=4.853, IN_z=3.577 (12s)\n",
      "epoch=15, EUBO=-160.217, ELBO=-188.626, ESS=1.459, EX_eta=42.855, IN_eta=22.247, EX_z=4.603, IN_z=3.854 (12s)\n",
      "epoch=16, EUBO=-159.743, ELBO=-188.428, ESS=1.460, EX_eta=42.724, IN_eta=21.911, EX_z=4.539, IN_z=3.703 (12s)\n",
      "epoch=17, EUBO=-158.606, ELBO=-185.518, ESS=1.484, EX_eta=40.433, IN_eta=22.296, EX_z=4.221, IN_z=3.550 (12s)\n",
      "epoch=18, EUBO=-158.556, ELBO=-184.478, ESS=1.479, EX_eta=39.028, IN_eta=21.742, EX_z=4.160, IN_z=3.472 (12s)\n",
      "epoch=19, EUBO=-157.703, ELBO=-183.687, ESS=1.469, EX_eta=38.653, IN_eta=21.328, EX_z=4.080, IN_z=3.523 (12s)\n",
      "epoch=20, EUBO=-157.259, ELBO=-183.624, ESS=1.460, EX_eta=39.413, IN_eta=19.953, EX_z=4.211, IN_z=3.400 (12s)\n",
      "epoch=21, EUBO=-156.921, ELBO=-181.834, ESS=1.528, EX_eta=37.037, IN_eta=20.598, EX_z=3.780, IN_z=3.345 (12s)\n",
      "epoch=22, EUBO=-156.516, ELBO=-181.655, ESS=1.511, EX_eta=37.962, IN_eta=20.101, EX_z=3.915, IN_z=3.469 (11s)\n",
      "epoch=23, EUBO=-155.963, ELBO=-180.815, ESS=1.506, EX_eta=37.316, IN_eta=19.745, EX_z=3.830, IN_z=3.278 (12s)\n",
      "epoch=24, EUBO=-155.812, ELBO=-180.985, ESS=1.511, EX_eta=36.869, IN_eta=19.033, EX_z=3.923, IN_z=3.325 (12s)\n",
      "epoch=25, EUBO=-155.485, ELBO=-179.652, ESS=1.539, EX_eta=36.157, IN_eta=19.116, EX_z=3.633, IN_z=3.066 (12s)\n",
      "epoch=26, EUBO=-155.088, ELBO=-179.530, ESS=1.518, EX_eta=36.145, IN_eta=18.909, EX_z=3.867, IN_z=3.172 (12s)\n",
      "epoch=27, EUBO=-155.026, ELBO=-179.253, ESS=1.495, EX_eta=35.797, IN_eta=18.135, EX_z=3.856, IN_z=3.376 (12s)\n",
      "epoch=28, EUBO=-154.331, ELBO=-178.474, ESS=1.507, EX_eta=35.744, IN_eta=18.114, EX_z=3.887, IN_z=3.242 (12s)\n",
      "epoch=29, EUBO=-154.043, ELBO=-177.786, ESS=1.542, EX_eta=35.575, IN_eta=18.044, EX_z=3.632, IN_z=2.996 (12s)\n",
      "epoch=30, EUBO=-153.693, ELBO=-177.017, ESS=1.527, EX_eta=34.939, IN_eta=17.592, EX_z=3.690, IN_z=3.201 (12s)\n",
      "epoch=31, EUBO=-153.715, ELBO=-176.425, ESS=1.515, EX_eta=34.117, IN_eta=18.054, EX_z=3.693, IN_z=3.161 (12s)\n",
      "epoch=32, EUBO=-153.370, ELBO=-175.823, ESS=1.531, EX_eta=33.347, IN_eta=17.920, EX_z=3.578, IN_z=3.044 (12s)\n",
      "epoch=33, EUBO=-153.056, ELBO=-175.956, ESS=1.516, EX_eta=33.432, IN_eta=17.574, EX_z=3.542, IN_z=2.880 (12s)\n",
      "epoch=34, EUBO=-153.210, ELBO=-175.509, ESS=1.528, EX_eta=33.414, IN_eta=17.112, EX_z=3.809, IN_z=2.905 (12s)\n",
      "epoch=35, EUBO=-152.656, ELBO=-175.064, ESS=1.573, EX_eta=33.757, IN_eta=17.215, EX_z=3.544, IN_z=2.885 (12s)\n",
      "epoch=36, EUBO=-152.739, ELBO=-174.807, ESS=1.520, EX_eta=33.293, IN_eta=17.507, EX_z=3.467, IN_z=2.754 (12s)\n",
      "epoch=37, EUBO=-152.172, ELBO=-174.250, ESS=1.528, EX_eta=32.938, IN_eta=17.033, EX_z=3.410, IN_z=2.713 (12s)\n",
      "epoch=38, EUBO=-152.145, ELBO=-174.157, ESS=1.538, EX_eta=33.212, IN_eta=16.541, EX_z=3.610, IN_z=2.816 (12s)\n",
      "epoch=39, EUBO=-151.738, ELBO=-174.198, ESS=1.565, EX_eta=33.420, IN_eta=16.397, EX_z=3.429, IN_z=2.798 (12s)\n",
      "epoch=40, EUBO=-151.772, ELBO=-174.128, ESS=1.545, EX_eta=33.951, IN_eta=16.042, EX_z=3.478, IN_z=2.660 (12s)\n",
      "epoch=41, EUBO=-151.436, ELBO=-173.706, ESS=1.538, EX_eta=33.272, IN_eta=16.402, EX_z=3.322, IN_z=2.603 (11s)\n",
      "epoch=42, EUBO=-151.689, ELBO=-173.166, ESS=1.580, EX_eta=32.583, IN_eta=16.743, EX_z=3.183, IN_z=2.636 (12s)\n",
      "epoch=43, EUBO=-150.895, ELBO=-173.001, ESS=1.571, EX_eta=33.231, IN_eta=16.329, EX_z=3.413, IN_z=2.633 (12s)\n",
      "epoch=44, EUBO=-150.959, ELBO=-172.784, ESS=1.578, EX_eta=33.327, IN_eta=15.734, EX_z=3.309, IN_z=2.494 (12s)\n",
      "epoch=45, EUBO=-151.030, ELBO=-172.754, ESS=1.581, EX_eta=32.986, IN_eta=16.111, EX_z=3.159, IN_z=2.525 (12s)\n",
      "epoch=46, EUBO=-150.678, ELBO=-172.219, ESS=1.566, EX_eta=32.491, IN_eta=16.012, EX_z=3.225, IN_z=2.562 (12s)\n",
      "epoch=47, EUBO=-150.678, ELBO=-172.216, ESS=1.560, EX_eta=33.207, IN_eta=15.876, EX_z=3.152, IN_z=2.467 (12s)\n",
      "epoch=48, EUBO=-150.723, ELBO=-171.894, ESS=1.617, EX_eta=32.264, IN_eta=16.010, EX_z=3.130, IN_z=2.512 (11s)\n",
      "epoch=49, EUBO=-150.242, ELBO=-172.371, ESS=1.561, EX_eta=33.858, IN_eta=15.347, EX_z=3.256, IN_z=2.537 (12s)\n",
      "epoch=50, EUBO=-150.213, ELBO=-171.597, ESS=1.570, EX_eta=33.210, IN_eta=15.395, EX_z=3.131, IN_z=2.325 (12s)\n",
      "epoch=51, EUBO=-149.787, ELBO=-171.311, ESS=1.573, EX_eta=33.287, IN_eta=15.491, EX_z=2.963, IN_z=2.281 (12s)\n",
      "epoch=52, EUBO=-149.947, ELBO=-171.290, ESS=1.596, EX_eta=33.149, IN_eta=15.782, EX_z=2.949, IN_z=2.378 (12s)\n",
      "epoch=53, EUBO=-149.758, ELBO=-170.935, ESS=1.584, EX_eta=32.868, IN_eta=15.686, EX_z=3.017, IN_z=2.346 (12s)\n",
      "epoch=54, EUBO=-149.826, ELBO=-171.018, ESS=1.591, EX_eta=32.237, IN_eta=15.190, EX_z=3.146, IN_z=2.456 (12s)\n",
      "epoch=55, EUBO=-149.436, ELBO=-171.199, ESS=1.551, EX_eta=33.412, IN_eta=14.805, EX_z=3.170, IN_z=2.348 (12s)\n",
      "epoch=56, EUBO=-149.513, ELBO=-170.840, ESS=1.581, EX_eta=33.072, IN_eta=15.295, EX_z=2.966, IN_z=2.327 (12s)\n",
      "epoch=57, EUBO=-149.694, ELBO=-170.703, ESS=1.610, EX_eta=32.954, IN_eta=15.271, EX_z=2.995, IN_z=2.197 (12s)\n",
      "epoch=58, EUBO=-149.135, ELBO=-170.411, ESS=1.589, EX_eta=33.445, IN_eta=15.529, EX_z=2.937, IN_z=2.229 (12s)\n",
      "epoch=59, EUBO=-148.750, ELBO=-170.507, ESS=1.577, EX_eta=33.810, IN_eta=15.007, EX_z=2.928, IN_z=2.190 (12s)\n",
      "epoch=60, EUBO=-148.860, ELBO=-170.403, ESS=1.600, EX_eta=33.424, IN_eta=14.729, EX_z=3.010, IN_z=2.186 (12s)\n",
      "epoch=61, EUBO=-148.915, ELBO=-170.012, ESS=1.617, EX_eta=32.926, IN_eta=14.661, EX_z=2.987, IN_z=2.244 (12s)\n",
      "epoch=62, EUBO=-148.749, ELBO=-170.017, ESS=1.608, EX_eta=33.125, IN_eta=14.852, EX_z=2.956, IN_z=2.232 (12s)\n",
      "epoch=63, EUBO=-148.915, ELBO=-169.668, ESS=1.622, EX_eta=32.147, IN_eta=15.096, EX_z=2.802, IN_z=2.032 (12s)\n",
      "epoch=64, EUBO=-148.811, ELBO=-170.425, ESS=1.598, EX_eta=33.619, IN_eta=14.820, EX_z=2.825, IN_z=2.088 (12s)\n",
      "epoch=65, EUBO=-148.475, ELBO=-170.236, ESS=1.582, EX_eta=33.868, IN_eta=13.976, EX_z=2.966, IN_z=2.119 (11s)\n",
      "epoch=66, EUBO=-148.456, ELBO=-169.750, ESS=1.613, EX_eta=33.493, IN_eta=14.697, EX_z=2.934, IN_z=2.082 (12s)\n",
      "epoch=67, EUBO=-148.369, ELBO=-169.325, ESS=1.606, EX_eta=32.527, IN_eta=14.514, EX_z=2.858, IN_z=2.091 (12s)\n",
      "epoch=68, EUBO=-148.174, ELBO=-169.228, ESS=1.617, EX_eta=32.628, IN_eta=14.519, EX_z=2.769, IN_z=2.077 (12s)\n",
      "epoch=69, EUBO=-148.252, ELBO=-169.570, ESS=1.613, EX_eta=32.961, IN_eta=14.086, EX_z=2.807, IN_z=2.025 (12s)\n",
      "epoch=70, EUBO=-148.414, ELBO=-169.792, ESS=1.578, EX_eta=32.376, IN_eta=14.528, EX_z=2.784, IN_z=2.190 (12s)\n",
      "epoch=71, EUBO=-148.418, ELBO=-169.104, ESS=1.643, EX_eta=32.067, IN_eta=14.700, EX_z=2.623, IN_z=2.020 (12s)\n",
      "epoch=72, EUBO=-147.838, ELBO=-169.041, ESS=1.628, EX_eta=32.730, IN_eta=14.219, EX_z=2.855, IN_z=2.003 (12s)\n",
      "epoch=73, EUBO=-147.728, ELBO=-168.471, ESS=1.638, EX_eta=32.274, IN_eta=14.227, EX_z=2.835, IN_z=2.059 (12s)\n",
      "epoch=74, EUBO=-148.038, ELBO=-168.969, ESS=1.615, EX_eta=32.419, IN_eta=13.822, EX_z=2.827, IN_z=1.990 (12s)\n",
      "epoch=75, EUBO=-147.778, ELBO=-169.070, ESS=1.604, EX_eta=33.469, IN_eta=14.090, EX_z=2.849, IN_z=2.061 (12s)\n",
      "epoch=76, EUBO=-147.667, ELBO=-168.561, ESS=1.624, EX_eta=32.263, IN_eta=14.228, EX_z=2.562, IN_z=1.866 (12s)\n",
      "epoch=77, EUBO=-147.637, ELBO=-169.230, ESS=1.615, EX_eta=33.620, IN_eta=13.605, EX_z=2.844, IN_z=1.906 (12s)\n",
      "epoch=78, EUBO=-147.811, ELBO=-169.171, ESS=1.626, EX_eta=33.975, IN_eta=14.234, EX_z=2.681, IN_z=1.971 (12s)\n",
      "epoch=79, EUBO=-147.763, ELBO=-169.127, ESS=1.629, EX_eta=34.026, IN_eta=14.602, EX_z=2.648, IN_z=1.928 (12s)\n",
      "epoch=80, EUBO=-147.671, ELBO=-168.822, ESS=1.650, EX_eta=33.268, IN_eta=13.613, EX_z=2.680, IN_z=2.037 (12s)\n",
      "epoch=81, EUBO=-147.426, ELBO=-168.663, ESS=1.641, EX_eta=32.675, IN_eta=13.890, EX_z=2.683, IN_z=1.941 (12s)\n",
      "epoch=82, EUBO=-147.583, ELBO=-169.166, ESS=1.627, EX_eta=33.781, IN_eta=13.902, EX_z=2.734, IN_z=2.005 (12s)\n",
      "epoch=83, EUBO=-147.457, ELBO=-167.954, ESS=1.673, EX_eta=32.821, IN_eta=13.783, EX_z=2.572, IN_z=1.996 (12s)\n",
      "epoch=84, EUBO=-147.341, ELBO=-167.681, ESS=1.652, EX_eta=31.459, IN_eta=13.943, EX_z=2.447, IN_z=1.836 (12s)\n",
      "epoch=85, EUBO=-147.080, ELBO=-167.076, ESS=1.647, EX_eta=31.602, IN_eta=14.180, EX_z=2.531, IN_z=1.833 (12s)\n",
      "epoch=86, EUBO=-147.058, ELBO=-167.937, ESS=1.618, EX_eta=32.858, IN_eta=13.432, EX_z=2.553, IN_z=1.888 (12s)\n",
      "epoch=87, EUBO=-146.988, ELBO=-167.822, ESS=1.631, EX_eta=32.507, IN_eta=13.550, EX_z=2.685, IN_z=1.986 (12s)\n",
      "epoch=88, EUBO=-147.066, ELBO=-168.362, ESS=1.604, EX_eta=33.354, IN_eta=13.472, EX_z=2.643, IN_z=1.924 (12s)\n",
      "epoch=89, EUBO=-146.909, ELBO=-167.676, ESS=1.667, EX_eta=32.235, IN_eta=13.413, EX_z=2.484, IN_z=1.808 (12s)\n",
      "epoch=90, EUBO=-147.135, ELBO=-167.277, ESS=1.627, EX_eta=31.632, IN_eta=13.842, EX_z=2.707, IN_z=1.879 (11s)\n",
      "epoch=91, EUBO=-147.015, ELBO=-167.761, ESS=1.661, EX_eta=32.394, IN_eta=13.213, EX_z=2.612, IN_z=1.958 (12s)\n",
      "epoch=92, EUBO=-147.067, ELBO=-167.798, ESS=1.658, EX_eta=32.491, IN_eta=13.225, EX_z=2.741, IN_z=1.908 (12s)\n",
      "epoch=93, EUBO=-146.765, ELBO=-167.482, ESS=1.651, EX_eta=32.571, IN_eta=13.486, EX_z=2.492, IN_z=1.891 (12s)\n",
      "epoch=94, EUBO=-146.603, ELBO=-167.861, ESS=1.641, EX_eta=32.907, IN_eta=13.307, EX_z=2.635, IN_z=1.790 (12s)\n",
      "epoch=95, EUBO=-146.569, ELBO=-167.238, ESS=1.653, EX_eta=32.654, IN_eta=13.146, EX_z=2.445, IN_z=1.861 (12s)\n",
      "epoch=96, EUBO=-146.882, ELBO=-167.050, ESS=1.668, EX_eta=32.029, IN_eta=13.646, EX_z=2.526, IN_z=1.890 (12s)\n",
      "epoch=97, EUBO=-146.748, ELBO=-167.730, ESS=1.608, EX_eta=33.038, IN_eta=13.307, EX_z=2.493, IN_z=1.732 (12s)\n",
      "epoch=98, EUBO=-146.592, ELBO=-166.651, ESS=1.651, EX_eta=31.882, IN_eta=13.250, EX_z=2.385, IN_z=1.798 (12s)\n",
      "epoch=99, EUBO=-146.689, ELBO=-167.058, ESS=1.655, EX_eta=32.054, IN_eta=13.370, EX_z=2.432, IN_z=1.817 (12s)\n"
     ]
    }
   ],
   "source": [
    "EUBOs = []\n",
    "ELBOs = []\n",
    "ESSs = []\n",
    "flog = open('results/log-' + PATH + '.txt', 'w+')\n",
    "flog.write('EUBO, ELBO, ESS, KLs_eta_ex, KLs_eta_in, KL_z_ex, KL_z_in\\n')\n",
    "flog.close()\n",
    "\n",
    "num_batches = int((Xs.shape[0] / BATCH_SIZE))\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    time_start = time.time()\n",
    "    indices = torch.randperm(num_seqs)\n",
    "    \n",
    "    EUBO = 0.0\n",
    "    ELBO = 0.0\n",
    "    ESS = 0.0\n",
    "    KL_eta_ex_os = 0.0\n",
    "    KL_eta_in_os = 0.0\n",
    "    KL_z_ex_os = 0.0\n",
    "    KL_z_in_os = 0.0\n",
    "    \n",
    "    \n",
    "    for step in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "        batch_indices = indices[step*BATCH_SIZE : (step+1)*BATCH_SIZE]\n",
    "        batch_Xs = Xs[batch_indices]\n",
    "        batch_Xs = shuffler(batch_Xs, N, K, D, BATCH_SIZE).repeat(NUM_SAMPLES, 1, 1, 1)\n",
    "        eubo, elbo, ess, KL_eta_ex, KL_eta_in, KL_z_ex, KL_z_in = rws(enc_init, enc_local, batch_Xs, K, D, NUM_SAMPLES, BATCH_SIZE)\n",
    "        eubo.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        EUBO += eubo.item()\n",
    "        ELBO += elbo.item()\n",
    "        ESS += ess.item()\n",
    "        KL_eta_ex_os += KL_eta_ex.item()\n",
    "        KL_eta_in_os += KL_eta_in.item()\n",
    "        KL_z_ex_os += KL_z_ex.item()\n",
    "        KL_z_in_os += KL_z_in.item()\n",
    "        flog = open('results/log-' + PATH + '.txt', 'a+')\n",
    "        flog.write(str(eubo.item()) + ', ' + str(elbo.item()) + ', ' + str(ess.item()) + ', ' + \n",
    "               str(KL_eta_ex.item()) + ', ' + str(KL_eta_in.item()) + ', ' + str(KL_z_ex.item()) + ', ' + str(KL_z_in.item()) + '\\n')\n",
    "                     \n",
    "    EUBO /= num_batches\n",
    "    ELBO /= num_batches\n",
    "    ESS /= num_batches\n",
    "    KL_eta_ex_os /= num_batches\n",
    "    KL_eta_in_os /= num_batches\n",
    "    KL_z_ex_os /= num_batches\n",
    "    KL_z_in_os /= num_batches \n",
    "    \n",
    "    EUBOs.append(EUBO)\n",
    "    ELBOs.append(ELBO)\n",
    "    ESSs.append(ESS) \n",
    "    time_end = time.time()\n",
    "    print('epoch=%d, EUBO=%.3f, ELBO=%.3f, ESS=%.3f, EX_eta=%.3f, IN_eta=%.3f, EX_z=%.3f, IN_z=%.3f (%ds)'\n",
    "            % (epoch, EUBO, ELBO, ESS,  KL_eta_ex_os, KL_eta_in_os, KL_z_ex_os, KL_z_in_os, time_end - time_start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc_global' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-47df27e31788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_global\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'models/global-enc-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'models/local-enc'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'enc_global' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(enc_global.state_dict(), 'models/global-enc-' + PATH)\n",
    "torch.save(enc_local.state_dict(), 'models/local-enc' + PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_results(EUBOs, ELBOs, ESSs, KLs_eta_ex, KLs_eta_in, KLs_z_ex, KLs_z_in, PATH, 'Epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(EUBOs, ELBOs, ESSs, KLs_eta_ex, KLs_eta_in, KLs_z_ex, KLs_z_in, NUM_SAMPLES, num_batches, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(x, num_seqs, Pi, N, K, D, batch_size):\n",
    "    q_mean, q_nu, q_alpha, q_beta, q_sigma, mus, precisions, log_q_eta = M_step(enc_global, x, N, D, K, batch_size)\n",
    "    zs_pi, z, log_q_z = E_step(enc_local, x, mus, precisions, N, D, K, batch_size)\n",
    "    E_precisions = q_alpha / q_beta\n",
    "    E_mus = q_mean\n",
    "    E_z = torch.argmax(zs_pi, dim=-1)\n",
    "\n",
    "    return E_mus, E_precisions, E_z\n",
    "BATCH_SIZE = 100\n",
    "x = Xs[BATCH_SIZE:2*BATCH_SIZE]\n",
    "E_mus, E_precisions, E_z = test(x, num_seqs, Pi, N, K, D, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_final_samples(Xs, Zs, mus, precisions, batch_size, PATH):\n",
    "    colors = ['r', 'b', 'gold']\n",
    "    fig = plt.figure(figsize=(25,100))\n",
    "    for b in range(batch_size):\n",
    "        ax = fig.add_subplot(int(batch_size / 5), 5, b+1)\n",
    "        x = Xs[b].data.numpy()\n",
    "        z = Zs[b].data.numpy()\n",
    "        mu = mus[b].data.numpy()\n",
    "        precision = precisions[b].data.numpy()\n",
    "\n",
    "        covs = np.zeros((K, D, D))\n",
    "        assignments = z\n",
    "        for k in range(K):\n",
    "            covs[k] = np.diag(1. / precision[k])\n",
    "            xk = x[np.where(assignments == k)]\n",
    "            ax.scatter(xk[:, 0], xk[:, 1], c=colors[k])\n",
    "            plot_cov_ellipse(cov=covs[k], pos=mu[k], nstd=2, ax=ax, alpha=0.2, color=colors[k])\n",
    "#         ax.set_ylim([-10, 10])\n",
    "#         ax.set_xlim([-10, 10])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "    plt.savefig('results/modes' + PATH + '.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_final_samples(x, E_z, E_mus, E_precisions, BATCH_SIZE, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LLs[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LLs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(LLs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LLs[20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
