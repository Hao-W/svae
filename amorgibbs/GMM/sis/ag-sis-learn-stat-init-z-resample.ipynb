{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probtorch: 0.0+5a2c637 torch: 1.0.0 cuda: True\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "sys.path.append('/home/hao/Research/probtorch/')\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from plots import *\n",
    "from kls import *\n",
    "from NG_nats import *\n",
    "from utils import *\n",
    "from objectives import *\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.one_hot_categorical import OneHotCategorical as cat\n",
    "from torch.distributions.gamma import Gamma\n",
    "import time\n",
    "import probtorch\n",
    "print('probtorch:', probtorch.__version__, \n",
    "      'torch:', torch.__version__, \n",
    "      'cuda:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 60\n",
    "K = 3\n",
    "D = 2\n",
    "\n",
    "## Model Parameters\n",
    "MCMC_SIZE = 10\n",
    "SAMPLE_SIZE = 10\n",
    "NUM_HIDDEN1 = 8\n",
    "NUM_STATS = 1 + 2 * D\n",
    "NUM_LATENTS =  D\n",
    "## Training Parameters\n",
    "SAMPLE_DIM = 0\n",
    "BATCH_DIM = 1\n",
    "BATCH_SIZE = 20\n",
    "NUM_EPOCHS = 1500\n",
    "LEARNING_RATE = 1e-4\n",
    "CUDA = torch.cuda.is_available()\n",
    "PATH = 'ag-sis-2k-learn-stat-init-z-resample'\n",
    "\n",
    "gpu2 = torch.device('cuda:0')\n",
    "data_path = \"../gmm_dataset_conjugate2k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = torch.from_numpy(np.load(data_path + '/obs.npy')).float()\n",
    "STATES = torch.from_numpy(np.load(data_path + '/states.npy')).float()\n",
    "Pi = torch.from_numpy(np.load(data_path + '/init.npy')).float()\n",
    "NUM_SEQS = Xs.shape[0]\n",
    "NUM_BATCHES = int((Xs.shape[0] / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enc_eta(nn.Module):\n",
    "    def __init__(self, num_obs=D,\n",
    "                       num_hidden1=NUM_HIDDEN1,\n",
    "                       num_stats=NUM_STATS,\n",
    "                       num_latents=NUM_LATENTS):\n",
    "        super(self.__class__, self).__init__()\n",
    "        \n",
    "        self.gamma = nn.Sequential(\n",
    "            nn.Linear(K+D, K),\n",
    "            nn.Softmax(-1))\n",
    "        \n",
    "        self.ob = nn.Sequential(\n",
    "            nn.Linear(K+D, D))\n",
    "        \n",
    "        self.prior_mu = torch.zeros((K, D))\n",
    "        self.prior_nu = torch.ones((K, D)) * 0.3\n",
    "        self.prior_alpha = torch.ones((K, D)) * 4\n",
    "        self.prior_beta = torch.ones((K, D)) * 4\n",
    "        if CUDA:\n",
    "            self.prior_mu = self.prior_mu.cuda().to(gpu2)\n",
    "            self.prior_nu = self.prior_nu.cuda().to(gpu2)\n",
    "            self.prior_alpha = self.prior_alpha.cuda().to(gpu2)\n",
    "            self.prior_beta = self.prior_beta.cuda().to(gpu2)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        gammas = self.gamma(data) # S * B * N * K --> S * B * N * K\n",
    "        xs = self.ob(data)  # S * B * N * D --> S * B * N * D\n",
    "        q_alpha, q_beta, q_mu, q_nu = post_param(xs, gammas, \n",
    "                                                 self.prior_alpha, self.prior_beta, self.prior_mu, self.prior_nu, K, D)\n",
    "   \n",
    "        q = probtorch.Trace()\n",
    "        precisions = Gamma(q_alpha, q_beta).sample()\n",
    "        q.gamma(q_alpha,\n",
    "                q_beta,\n",
    "                value=precisions,\n",
    "                name='precisions')\n",
    "        \n",
    "        p = probtorch.Trace()\n",
    "        p.gamma(self.prior_alpha,\n",
    "                self.prior_beta,\n",
    "                value=q['precisions'],\n",
    "                name='precisions')   \n",
    "  \n",
    "        means = Normal(q_mu, 1. / (q_nu * q['precisions'].value).sqrt()).sample()\n",
    "        q.normal(q_mu, \n",
    "                 1. / (q_nu * q['precisions'].value).sqrt(), \n",
    "                 value=means, \n",
    "                 name='means')\n",
    "        p.normal(self.prior_mu, \n",
    "                 1. / (self.prior_nu * q['precisions'].value).sqrt(), \n",
    "                 value=q['means'], \n",
    "                 name='means')    \n",
    "        return q, p, q_nu\n",
    "\n",
    "class Enc_z(nn.Module):\n",
    "    def __init__(self, num_obs=3*D,\n",
    "                       num_hidden=3*D,\n",
    "                       num_latents=K):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.pi_prob = nn.Sequential(\n",
    "            nn.Linear(num_obs, num_hidden),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(num_hidden, int(0.5*num_hidden)),\n",
    "            nn.Linear(int(0.5*num_hidden), 1))\n",
    "        \n",
    "        self.prior_pi = torch.ones(K) * (1./ K)\n",
    "        if CUDA:\n",
    "            self.prior_pi = self.prior_pi.cuda().to(gpu2)\n",
    "  \n",
    "    def forward(self, obs, obs_tau, obs_mu, sample_size, batch_size):\n",
    "        q = probtorch.Trace()\n",
    "        obs_tau_c1 = obs_tau[:, :, 0, :].unsqueeze(-2).repeat(1,1,N,1)\n",
    "        obs_mu_c1 = obs_mu[:, :, 0, :].unsqueeze(-2).repeat(1,1,N,1)\n",
    "        obs_tau_c2 = obs_tau[:, :, 1, :].unsqueeze(-2).repeat(1,1,N,1)\n",
    "        obs_mu_c2 = obs_mu[:, :, 1, :].unsqueeze(-2).repeat(1,1,N,1)\n",
    "        obs_tau_c3 = obs_tau[:, :, 2, :].unsqueeze(-2).repeat(1,1,N,1)\n",
    "        obs_mu_c3 = obs_mu[:, :, 2, :].unsqueeze(-2).repeat(1,1,N,1)\n",
    "        \n",
    "        data_c1 = torch.cat((obs, obs_mu_c1, obs_tau_c1), -1) ## S * B * N * 3D\n",
    "        data_c2 = torch.cat((obs, obs_mu_c2, obs_tau_c2), -1) ## S * B * N * 3D\n",
    "        data_c3 = torch.cat((obs, obs_mu_c3, obs_tau_c3), -1) ## S * B * N * 3D\n",
    "        \n",
    "        z_pi_c1 = self.pi_prob(data_c1)\n",
    "        z_pi_c2 = self.pi_prob(data_c2)\n",
    "        z_pi_c3 = self.pi_prob(data_c3)\n",
    "        \n",
    "        z_pi = F.softmax(torch.cat((z_pi_c1, z_pi_c2, z_pi_c3), -1), -1)\n",
    "        z = cat(z_pi).sample()\n",
    "        _ = q.variable(cat, probs=z_pi, value=z, name='zs')\n",
    "        p = probtorch.Trace()\n",
    "        _ = p.variable(cat, probs=self.prior_pi, value=z, name='zs')\n",
    "        return q, p\n",
    "    \n",
    "def initialize():\n",
    "    enc_eta = Enc_eta()\n",
    "    enc_z = Enc_z()\n",
    "    if CUDA:\n",
    "        enc_eta.cuda().to(gpu2)\n",
    "        enc_z.cuda().to(gpu2)\n",
    "    optimizer =  torch.optim.Adam(list(enc_z.parameters())+list(enc_eta.parameters()),lr=LEARNING_RATE, betas=(0.9, 0.99))    \n",
    "    return enc_eta, enc_z, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_eta, enc_z, optimizer = initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, EUBO=-1213.296, ELBO=-1515.008, ESS=1.020 (11s)\n",
      "epoch=1, EUBO=-1164.121, ELBO=-1455.905, ESS=1.023 (11s)\n",
      "epoch=2, EUBO=-1118.390, ELBO=-1398.085, ESS=1.023 (11s)\n",
      "epoch=3, EUBO=-1076.050, ELBO=-1341.919, ESS=1.023 (11s)\n",
      "epoch=4, EUBO=-1035.367, ELBO=-1289.004, ESS=1.026 (12s)\n",
      "epoch=5, EUBO=-994.170, ELBO=-1236.284, ESS=1.028 (11s)\n",
      "epoch=6, EUBO=-955.687, ELBO=-1186.036, ESS=1.028 (11s)\n",
      "epoch=7, EUBO=-917.892, ELBO=-1137.619, ESS=1.031 (11s)\n",
      "epoch=8, EUBO=-882.975, ELBO=-1092.126, ESS=1.030 (11s)\n",
      "epoch=9, EUBO=-849.845, ELBO=-1047.747, ESS=1.034 (11s)\n",
      "epoch=10, EUBO=-818.349, ELBO=-1006.614, ESS=1.034 (11s)\n",
      "epoch=11, EUBO=-788.740, ELBO=-967.115, ESS=1.039 (11s)\n",
      "epoch=12, EUBO=-760.635, ELBO=-929.147, ESS=1.038 (11s)\n",
      "epoch=13, EUBO=-734.611, ELBO=-893.898, ESS=1.044 (11s)\n",
      "epoch=14, EUBO=-708.982, ELBO=-859.542, ESS=1.042 (11s)\n",
      "epoch=15, EUBO=-686.144, ELBO=-828.073, ESS=1.046 (11s)\n",
      "epoch=16, EUBO=-664.812, ELBO=-799.342, ESS=1.049 (11s)\n",
      "epoch=17, EUBO=-644.600, ELBO=-772.050, ESS=1.052 (11s)\n",
      "epoch=18, EUBO=-626.605, ELBO=-746.710, ESS=1.057 (11s)\n",
      "epoch=19, EUBO=-609.970, ELBO=-723.561, ESS=1.062 (11s)\n",
      "epoch=20, EUBO=-595.710, ELBO=-703.024, ESS=1.065 (11s)\n",
      "epoch=21, EUBO=-581.755, ELBO=-684.111, ESS=1.069 (11s)\n",
      "epoch=22, EUBO=-570.060, ELBO=-667.115, ESS=1.076 (11s)\n",
      "epoch=23, EUBO=-559.095, ELBO=-652.226, ESS=1.077 (11s)\n",
      "epoch=24, EUBO=-549.524, ELBO=-638.609, ESS=1.080 (10s)\n",
      "epoch=25, EUBO=-540.670, ELBO=-626.210, ESS=1.088 (11s)\n",
      "epoch=26, EUBO=-532.994, ELBO=-615.421, ESS=1.090 (11s)\n",
      "epoch=27, EUBO=-526.073, ELBO=-605.217, ESS=1.098 (11s)\n",
      "epoch=28, EUBO=-519.088, ELBO=-596.037, ESS=1.105 (11s)\n",
      "epoch=29, EUBO=-513.413, ELBO=-587.608, ESS=1.110 (11s)\n",
      "epoch=30, EUBO=-508.101, ELBO=-579.721, ESS=1.118 (11s)\n",
      "epoch=31, EUBO=-503.160, ELBO=-573.120, ESS=1.117 (11s)\n",
      "epoch=32, EUBO=-498.234, ELBO=-566.511, ESS=1.127 (11s)\n",
      "epoch=33, EUBO=-494.331, ELBO=-560.130, ESS=1.129 (11s)\n",
      "epoch=34, EUBO=-489.829, ELBO=-554.578, ESS=1.133 (11s)\n",
      "epoch=35, EUBO=-485.998, ELBO=-548.904, ESS=1.138 (12s)\n",
      "epoch=36, EUBO=-482.655, ELBO=-543.899, ESS=1.146 (11s)\n",
      "epoch=37, EUBO=-478.615, ELBO=-538.532, ESS=1.148 (11s)\n",
      "epoch=38, EUBO=-475.948, ELBO=-534.252, ESS=1.157 (11s)\n",
      "epoch=39, EUBO=-472.377, ELBO=-529.639, ESS=1.161 (11s)\n",
      "epoch=40, EUBO=-468.815, ELBO=-525.218, ESS=1.166 (11s)\n",
      "epoch=41, EUBO=-466.453, ELBO=-521.253, ESS=1.168 (10s)\n",
      "epoch=42, EUBO=-463.580, ELBO=-517.347, ESS=1.171 (10s)\n",
      "epoch=43, EUBO=-460.973, ELBO=-513.496, ESS=1.181 (10s)\n",
      "epoch=44, EUBO=-458.280, ELBO=-509.961, ESS=1.184 (10s)\n",
      "epoch=45, EUBO=-455.877, ELBO=-506.335, ESS=1.189 (10s)\n",
      "epoch=46, EUBO=-453.434, ELBO=-503.366, ESS=1.198 (10s)\n",
      "epoch=47, EUBO=-451.058, ELBO=-499.749, ESS=1.195 (10s)\n",
      "epoch=48, EUBO=-448.978, ELBO=-496.740, ESS=1.203 (10s)\n",
      "epoch=49, EUBO=-446.757, ELBO=-493.830, ESS=1.208 (10s)\n",
      "epoch=50, EUBO=-444.190, ELBO=-490.794, ESS=1.210 (10s)\n",
      "epoch=51, EUBO=-442.943, ELBO=-488.468, ESS=1.213 (10s)\n",
      "epoch=52, EUBO=-440.453, ELBO=-485.391, ESS=1.219 (10s)\n",
      "epoch=53, EUBO=-438.663, ELBO=-482.887, ESS=1.223 (10s)\n",
      "epoch=54, EUBO=-436.993, ELBO=-480.322, ESS=1.227 (10s)\n",
      "epoch=55, EUBO=-435.380, ELBO=-478.192, ESS=1.228 (10s)\n",
      "epoch=56, EUBO=-433.781, ELBO=-476.188, ESS=1.234 (10s)\n",
      "epoch=57, EUBO=-432.218, ELBO=-473.802, ESS=1.235 (10s)\n",
      "epoch=58, EUBO=-430.940, ELBO=-472.034, ESS=1.238 (10s)\n",
      "epoch=59, EUBO=-429.448, ELBO=-469.981, ESS=1.246 (10s)\n",
      "epoch=60, EUBO=-427.864, ELBO=-467.842, ESS=1.252 (11s)\n",
      "epoch=61, EUBO=-426.711, ELBO=-466.241, ESS=1.250 (11s)\n",
      "epoch=62, EUBO=-425.133, ELBO=-464.449, ESS=1.253 (11s)\n",
      "epoch=63, EUBO=-424.250, ELBO=-462.668, ESS=1.257 (10s)\n",
      "epoch=64, EUBO=-422.878, ELBO=-461.096, ESS=1.255 (11s)\n",
      "epoch=65, EUBO=-421.794, ELBO=-459.685, ESS=1.260 (11s)\n",
      "epoch=66, EUBO=-420.892, ELBO=-458.144, ESS=1.270 (11s)\n",
      "epoch=67, EUBO=-419.675, ELBO=-456.803, ESS=1.270 (11s)\n",
      "epoch=68, EUBO=-418.436, ELBO=-455.272, ESS=1.269 (11s)\n",
      "epoch=69, EUBO=-417.647, ELBO=-454.353, ESS=1.275 (11s)\n",
      "epoch=70, EUBO=-416.936, ELBO=-453.094, ESS=1.286 (11s)\n",
      "epoch=71, EUBO=-416.066, ELBO=-451.997, ESS=1.278 (11s)\n",
      "epoch=72, EUBO=-414.981, ELBO=-450.760, ESS=1.276 (11s)\n",
      "epoch=73, EUBO=-414.573, ELBO=-449.773, ESS=1.280 (11s)\n",
      "epoch=74, EUBO=-413.673, ELBO=-448.831, ESS=1.287 (11s)\n",
      "epoch=75, EUBO=-412.808, ELBO=-447.807, ESS=1.288 (11s)\n",
      "epoch=76, EUBO=-411.858, ELBO=-446.633, ESS=1.286 (11s)\n",
      "epoch=77, EUBO=-411.473, ELBO=-445.987, ESS=1.289 (11s)\n",
      "epoch=78, EUBO=-410.763, ELBO=-445.098, ESS=1.291 (11s)\n",
      "epoch=79, EUBO=-409.957, ELBO=-444.269, ESS=1.292 (11s)\n",
      "epoch=80, EUBO=-409.427, ELBO=-443.412, ESS=1.294 (11s)\n",
      "epoch=81, EUBO=-408.998, ELBO=-442.707, ESS=1.306 (11s)\n",
      "epoch=82, EUBO=-408.076, ELBO=-441.789, ESS=1.301 (11s)\n",
      "epoch=83, EUBO=-407.747, ELBO=-441.314, ESS=1.305 (11s)\n",
      "epoch=84, EUBO=-407.322, ELBO=-440.751, ESS=1.306 (11s)\n",
      "epoch=85, EUBO=-406.496, ELBO=-439.811, ESS=1.308 (11s)\n",
      "epoch=86, EUBO=-406.011, ELBO=-439.168, ESS=1.312 (11s)\n",
      "epoch=87, EUBO=-405.543, ELBO=-438.516, ESS=1.304 (11s)\n",
      "epoch=88, EUBO=-405.053, ELBO=-437.919, ESS=1.316 (11s)\n",
      "epoch=89, EUBO=-404.523, ELBO=-437.228, ESS=1.315 (11s)\n",
      "epoch=90, EUBO=-404.152, ELBO=-436.705, ESS=1.314 (11s)\n",
      "epoch=91, EUBO=-403.499, ELBO=-436.013, ESS=1.313 (10s)\n",
      "epoch=92, EUBO=-403.209, ELBO=-435.434, ESS=1.322 (11s)\n",
      "epoch=93, EUBO=-402.656, ELBO=-434.904, ESS=1.323 (11s)\n",
      "epoch=94, EUBO=-402.271, ELBO=-434.435, ESS=1.318 (11s)\n",
      "epoch=95, EUBO=-401.824, ELBO=-433.955, ESS=1.331 (11s)\n",
      "epoch=96, EUBO=-401.306, ELBO=-433.333, ESS=1.323 (11s)\n",
      "epoch=97, EUBO=-400.944, ELBO=-432.881, ESS=1.329 (11s)\n",
      "epoch=98, EUBO=-400.545, ELBO=-432.284, ESS=1.332 (11s)\n",
      "epoch=99, EUBO=-400.113, ELBO=-431.854, ESS=1.322 (11s)\n",
      "epoch=100, EUBO=-399.419, ELBO=-431.091, ESS=1.329 (11s)\n",
      "epoch=101, EUBO=-399.312, ELBO=-430.822, ESS=1.336 (12s)\n",
      "epoch=102, EUBO=-399.205, ELBO=-430.571, ESS=1.340 (11s)\n",
      "epoch=103, EUBO=-398.726, ELBO=-430.035, ESS=1.340 (11s)\n",
      "epoch=104, EUBO=-398.399, ELBO=-429.672, ESS=1.341 (11s)\n",
      "epoch=105, EUBO=-397.980, ELBO=-429.186, ESS=1.340 (11s)\n",
      "epoch=106, EUBO=-397.749, ELBO=-428.885, ESS=1.346 (10s)\n",
      "epoch=107, EUBO=-397.174, ELBO=-428.390, ESS=1.345 (10s)\n",
      "epoch=108, EUBO=-397.302, ELBO=-428.118, ESS=1.346 (10s)\n",
      "epoch=109, EUBO=-396.704, ELBO=-427.793, ESS=1.353 (10s)\n",
      "epoch=110, EUBO=-396.509, ELBO=-427.420, ESS=1.347 (11s)\n",
      "epoch=111, EUBO=-396.246, ELBO=-427.212, ESS=1.352 (11s)\n",
      "epoch=112, EUBO=-395.970, ELBO=-426.793, ESS=1.354 (11s)\n",
      "epoch=113, EUBO=-395.802, ELBO=-426.530, ESS=1.356 (11s)\n",
      "epoch=114, EUBO=-395.377, ELBO=-426.193, ESS=1.358 (11s)\n",
      "epoch=115, EUBO=-395.073, ELBO=-425.738, ESS=1.359 (11s)\n",
      "epoch=116, EUBO=-395.154, ELBO=-425.638, ESS=1.353 (11s)\n",
      "epoch=117, EUBO=-394.944, ELBO=-425.572, ESS=1.357 (11s)\n",
      "epoch=118, EUBO=-394.647, ELBO=-425.139, ESS=1.364 (11s)\n",
      "epoch=119, EUBO=-394.642, ELBO=-425.038, ESS=1.362 (11s)\n",
      "epoch=120, EUBO=-394.419, ELBO=-424.855, ESS=1.362 (11s)\n",
      "epoch=121, EUBO=-394.308, ELBO=-424.740, ESS=1.358 (11s)\n",
      "epoch=122, EUBO=-394.132, ELBO=-424.523, ESS=1.359 (11s)\n",
      "epoch=123, EUBO=-393.772, ELBO=-424.256, ESS=1.360 (11s)\n",
      "epoch=124, EUBO=-393.665, ELBO=-424.087, ESS=1.364 (11s)\n",
      "epoch=125, EUBO=-393.621, ELBO=-424.002, ESS=1.364 (10s)\n",
      "epoch=126, EUBO=-393.327, ELBO=-423.513, ESS=1.364 (11s)\n",
      "epoch=127, EUBO=-393.134, ELBO=-423.290, ESS=1.362 (11s)\n",
      "epoch=128, EUBO=-393.190, ELBO=-423.307, ESS=1.367 (11s)\n",
      "epoch=129, EUBO=-392.957, ELBO=-423.172, ESS=1.370 (11s)\n",
      "epoch=130, EUBO=-392.650, ELBO=-422.930, ESS=1.369 (11s)\n",
      "epoch=131, EUBO=-392.777, ELBO=-422.733, ESS=1.371 (11s)\n",
      "epoch=132, EUBO=-392.719, ELBO=-422.706, ESS=1.377 (11s)\n",
      "epoch=133, EUBO=-392.200, ELBO=-422.362, ESS=1.372 (11s)\n",
      "epoch=134, EUBO=-392.276, ELBO=-422.352, ESS=1.371 (11s)\n",
      "epoch=135, EUBO=-392.181, ELBO=-422.094, ESS=1.379 (11s)\n",
      "epoch=136, EUBO=-391.681, ELBO=-421.866, ESS=1.377 (11s)\n",
      "epoch=137, EUBO=-391.507, ELBO=-421.638, ESS=1.370 (11s)\n",
      "epoch=138, EUBO=-391.458, ELBO=-421.535, ESS=1.370 (11s)\n",
      "epoch=139, EUBO=-391.424, ELBO=-421.402, ESS=1.375 (11s)\n",
      "epoch=140, EUBO=-391.299, ELBO=-421.339, ESS=1.375 (11s)\n",
      "epoch=141, EUBO=-391.433, ELBO=-421.122, ESS=1.381 (10s)\n",
      "epoch=142, EUBO=-391.136, ELBO=-420.938, ESS=1.378 (11s)\n",
      "epoch=143, EUBO=-390.995, ELBO=-420.856, ESS=1.391 (11s)\n",
      "epoch=144, EUBO=-390.691, ELBO=-420.589, ESS=1.375 (11s)\n",
      "epoch=145, EUBO=-390.416, ELBO=-420.321, ESS=1.379 (11s)\n",
      "epoch=146, EUBO=-390.571, ELBO=-420.371, ESS=1.387 (10s)\n",
      "epoch=147, EUBO=-390.327, ELBO=-420.088, ESS=1.376 (10s)\n",
      "epoch=148, EUBO=-390.200, ELBO=-420.079, ESS=1.378 (10s)\n",
      "epoch=149, EUBO=-389.923, ELBO=-419.770, ESS=1.383 (12s)\n",
      "epoch=150, EUBO=-389.871, ELBO=-419.524, ESS=1.382 (13s)\n",
      "epoch=151, EUBO=-389.602, ELBO=-419.366, ESS=1.381 (13s)\n",
      "epoch=152, EUBO=-389.622, ELBO=-419.386, ESS=1.384 (12s)\n",
      "epoch=153, EUBO=-389.421, ELBO=-419.088, ESS=1.387 (11s)\n",
      "epoch=154, EUBO=-389.386, ELBO=-419.076, ESS=1.384 (13s)\n",
      "epoch=155, EUBO=-389.014, ELBO=-418.817, ESS=1.384 (12s)\n",
      "epoch=156, EUBO=-389.221, ELBO=-418.732, ESS=1.385 (12s)\n",
      "epoch=157, EUBO=-388.928, ELBO=-418.543, ESS=1.382 (12s)\n",
      "epoch=158, EUBO=-388.958, ELBO=-418.420, ESS=1.390 (13s)\n",
      "epoch=159, EUBO=-388.644, ELBO=-418.324, ESS=1.380 (11s)\n",
      "epoch=160, EUBO=-388.435, ELBO=-418.041, ESS=1.390 (11s)\n",
      "epoch=161, EUBO=-388.338, ELBO=-417.919, ESS=1.391 (11s)\n",
      "epoch=162, EUBO=-388.401, ELBO=-417.653, ESS=1.396 (11s)\n",
      "epoch=163, EUBO=-388.160, ELBO=-417.555, ESS=1.385 (11s)\n",
      "epoch=164, EUBO=-387.881, ELBO=-417.321, ESS=1.392 (11s)\n",
      "epoch=165, EUBO=-387.834, ELBO=-417.349, ESS=1.387 (11s)\n",
      "epoch=166, EUBO=-387.629, ELBO=-417.047, ESS=1.398 (11s)\n",
      "epoch=167, EUBO=-387.444, ELBO=-416.842, ESS=1.398 (11s)\n",
      "epoch=168, EUBO=-387.255, ELBO=-416.615, ESS=1.400 (11s)\n",
      "epoch=169, EUBO=-387.181, ELBO=-416.564, ESS=1.392 (11s)\n",
      "epoch=170, EUBO=-386.912, ELBO=-416.316, ESS=1.395 (11s)\n",
      "epoch=171, EUBO=-386.809, ELBO=-416.184, ESS=1.387 (11s)\n",
      "epoch=172, EUBO=-386.887, ELBO=-416.034, ESS=1.389 (11s)\n",
      "epoch=173, EUBO=-386.739, ELBO=-415.861, ESS=1.394 (10s)\n",
      "epoch=174, EUBO=-386.468, ELBO=-415.713, ESS=1.387 (11s)\n",
      "epoch=175, EUBO=-386.384, ELBO=-415.477, ESS=1.398 (11s)\n",
      "epoch=176, EUBO=-386.370, ELBO=-415.322, ESS=1.397 (11s)\n",
      "epoch=177, EUBO=-386.066, ELBO=-415.261, ESS=1.399 (11s)\n",
      "epoch=178, EUBO=-385.866, ELBO=-415.016, ESS=1.399 (11s)\n",
      "epoch=179, EUBO=-385.825, ELBO=-414.760, ESS=1.396 (11s)\n",
      "epoch=180, EUBO=-385.728, ELBO=-414.800, ESS=1.401 (11s)\n",
      "epoch=181, EUBO=-385.645, ELBO=-414.508, ESS=1.393 (11s)\n",
      "epoch=182, EUBO=-385.193, ELBO=-414.153, ESS=1.394 (10s)\n",
      "epoch=183, EUBO=-385.162, ELBO=-413.982, ESS=1.391 (10s)\n",
      "epoch=184, EUBO=-385.126, ELBO=-413.922, ESS=1.392 (11s)\n",
      "epoch=185, EUBO=-384.940, ELBO=-413.753, ESS=1.401 (11s)\n",
      "epoch=186, EUBO=-384.772, ELBO=-413.570, ESS=1.397 (11s)\n"
     ]
    }
   ],
   "source": [
    "EUBOs = []\n",
    "ELBOs = []\n",
    "ESSs = []\n",
    "\n",
    "flog = open('results/log-' + PATH + '.txt', 'w+')\n",
    "flog.write('EUBO\\tELBO\\tESS\\tKLs_eta_ex\\tKLs_eta_in\\tKLs_z_ex\\tKLs_z_in\\n')\n",
    "flog.close()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    time_start = time.time()\n",
    "    indices = torch.randperm(NUM_SEQS)\n",
    "    EUBO = 0.0\n",
    "    ELBO = 0.0\n",
    "    ESS = 0.0\n",
    "    KL_eta_ex = 0.0\n",
    "    KL_eta_in = 0.0\n",
    "    KL_z_ex = 0.0\n",
    "    KL_z_in = 0.0\n",
    "    for step in range(NUM_BATCHES):\n",
    "        optimizer.zero_grad()\n",
    "        batch_indices = indices[step*BATCH_SIZE : (step+1)*BATCH_SIZE]\n",
    "        obs = Xs[batch_indices]\n",
    "        obs = shuffler(obs).repeat(SAMPLE_SIZE, 1, 1, 1)\n",
    "        if CUDA:\n",
    "            obs =obs.cuda().to(gpu2)\n",
    "        eubo, elbo, ess, kl_eta_ex, kl_eta_in, kl_z_ex, kl_z_in = Eubo_ag_sis_initz_adapt(enc_eta, enc_z, obs, N, K, D, MCMC_SIZE, SAMPLE_SIZE, BATCH_SIZE)\n",
    "        ## gradient step\n",
    "        eubo.backward()\n",
    "        optimizer.step()\n",
    "        EUBO += eubo.item()\n",
    "        ELBO += elbo.item()\n",
    "        ESS += ess.item()\n",
    "        KL_eta_ex += kl_eta_ex.item()\n",
    "        KL_eta_in += kl_eta_in.item()\n",
    "        KL_z_ex += kl_z_ex.item()\n",
    "        KL_z_in += kl_z_in.item()\n",
    "    EUBOs.append(EUBO / NUM_BATCHES)\n",
    "    ELBOs.append(ELBO / NUM_BATCHES)\n",
    "    ESSs.append(ESS / NUM_BATCHES) \n",
    "    flog = open('results/log-' + PATH + '.txt', 'a+')\n",
    "    print('%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
    "            % (EUBO/NUM_BATCHES, ELBO/NUM_BATCHES, ESS/NUM_BATCHES, KL_eta_ex/NUM_BATCHES, KL_eta_in/NUM_BATCHES, KL_z_ex/NUM_BATCHES, KL_z_in/NUM_BATCHES), file=flog)\n",
    "    flog.close()\n",
    "    time_end = time.time()\n",
    "    print('epoch=%d, EUBO=%.3f, ELBO=%.3f, ESS=%.3f (%ds)'\n",
    "            % (epoch, EUBO/NUM_BATCHES, ELBO/NUM_BATCHES, ESS/NUM_BATCHES, time_end - time_start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(enc_eta.state_dict(), 'weights/enc-%s' + PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_single_batch(num_seqs, N, K, D, batch_size):\n",
    "    indices = torch.randperm(num_seqs)\n",
    "    batch_indices = indices[0*batch_size : (0+1)*batch_size]\n",
    "    obs = Xs[batch_indices]\n",
    "    states = STATES[batch_indices]\n",
    "    obs = shuffler(obs).repeat(SAMPLE_SIZE, 1, 1, 1)\n",
    "    if CUDA:\n",
    "        obs =obs.cuda().to(gpu2)\n",
    "    return obs\n",
    "\n",
    "def test(enc_eta, enc_z, obs, mcmc_size, sample_size, batch_size):\n",
    "    for m in range(mcmc_size):\n",
    "        if m == 0:\n",
    "            p_init_z = cat(enc_z.prior_pi)\n",
    "            states = p_init_z.sample((sample_size, batch_size, N,))\n",
    "        else:\n",
    "            ## update z -- cluster assignments\n",
    "            q_z, p_z = enc_z(obs, obs_sigma, obs_mu, sample_size, batch_size)\n",
    "            states = q_z['zs'].value ## S * B * N * K\n",
    "        ## update tau and mu -- global variables\n",
    "        local_vars = torch.cat((obs, states), -1)\n",
    "        q_eta, p_eta, q_nu = enc_eta(local_vars)\n",
    "    return q_eta, states\n",
    "\n",
    "obs = sample_single_batch(NUM_SEQS, N, K, D, batch_size=25)\n",
    "\n",
    "q_eta, states = test(enc_eta, enc_z, obs, 20, SAMPLE_SIZE, batch_size=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(obs, states, q_eta, batch_size):\n",
    "    colors = ['r', 'b', 'gold']\n",
    "    fig = plt.figure(figsize=(25,25))\n",
    "    xs = obs[0].cpu()\n",
    "    zs = states[0].cpu()\n",
    "    mu_means = q_eta['means'].dist.loc[0].cpu().data.numpy()\n",
    "    tau_means = (q_eta['precisions'].dist.concentration[0] / q_eta['precisions'].dist.rate[0]).cpu().data.numpy()\n",
    "    for b in range(batch_size):\n",
    "        ax = fig.add_subplot(int(batch_size / 5), 5, b+1)\n",
    "        x = xs[b].data.numpy()\n",
    "        z = zs[b].data.numpy()\n",
    "        mu = mu_means[b].reshape(K, D)\n",
    "        sigma2 = 1. / tau_means[b]\n",
    "        assignments = z.argmax(-1)\n",
    "        for k in range(K):\n",
    "            cov_k = np.diag(sigma2[k])\n",
    "            xk = x[np.where(assignments == k)]\n",
    "            ax.scatter(xk[:, 0], xk[:, 1], c=colors[k])\n",
    "            plot_cov_ellipse(cov=cov_k, pos=mu[k], nstd=2, ax=ax, alpha=0.2, color=colors[k])\n",
    "        ax.set_ylim([-15, 15])\n",
    "        ax.set_xlim([-15, 15])\n",
    "    plt.savefig('results/modes-' + PATH + '.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(obs, states, q_eta, batch_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
