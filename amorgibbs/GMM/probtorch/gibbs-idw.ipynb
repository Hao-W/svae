{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probtorch: 0.0+5a2c637 torch: 1.0.0 cuda: True\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from plots import *\n",
    "from kls import *\n",
    "from nats import *\n",
    "from utils import *\n",
    "from objectives import *\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.one_hot_categorical import OneHotCategorical as cat\n",
    "from torch.distributions.relaxed_categorical import RelaxedOneHotCategorical as rcat\n",
    "from torch.distributions.gamma import Gamma\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "sys.path.append('/home/hao/Research/probtorch/')\n",
    "import probtorch\n",
    "from probtorch.util import expand_inputs\n",
    "print('probtorch:', probtorch.__version__, \n",
    "      'torch:', torch.__version__, \n",
    "      'cuda:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "K = 3\n",
    "D = 2\n",
    "\n",
    "## Model Parameters\n",
    "NUM_SAMPLES = 10\n",
    "NUM_HIDDEN1 = 16\n",
    "NUM_HIDDEN2 = 4\n",
    "NUM_STATS = 3*K\n",
    "NUM_LATENTS =  K\n",
    "## Training Parameters\n",
    "SAMPLE_DIM = 0\n",
    "BATCH_DIM = 1\n",
    "BATCH_SIZE = 20\n",
    "NUM_EPOCHS = 2000\n",
    "LEARNING_RATE = 1e-4\n",
    "CUDA = torch.cuda.is_available()\n",
    "PATH = 'gibbs-nc-IWKL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = torch.from_numpy(np.load('gmm_dataset/obs.npy')).float()\n",
    "STATES = torch.from_numpy(np.load('gmm_dataset/states.npy')).float()\n",
    "OBS_MU = torch.from_numpy(np.load('gmm_dataset/obs_mu.npy')).float()\n",
    "OBS_SIGMA = torch.from_numpy(np.load('gmm_dataset/obs_sigma.npy')).float()\n",
    "Pi = torch.from_numpy(np.load('gmm_dataset/init.npy')).float()\n",
    "NUM_SEQS = Xs.shape[0]\n",
    "NUM_BATCHES = int((Xs.shape[0] / BATCH_SIZE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enc_eta(nn.Module):\n",
    "    def __init__(self, num_obs=D,\n",
    "                       num_hidden1=NUM_HIDDEN1,\n",
    "                       num_hidden2=NUM_HIDDEN2,\n",
    "                       num_stats=NUM_STATS,\n",
    "                       num_latents=NUM_LATENTS):\n",
    "        super(self.__class__, self).__init__()\n",
    "\n",
    "        self.mus_mu = nn.Sequential(\n",
    "            nn.Linear(num_stats, num_hidden1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(num_hidden1, int(0.5*num_hidden1)),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(int(0.5*num_hidden1), num_latents))\n",
    "        self.mus_log_sigma = nn.Sequential(\n",
    "            nn.Linear(num_stats, num_hidden1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(num_hidden1, int(0.5*num_hidden1)),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(int(0.5*num_hidden1), num_latents))\n",
    "        \n",
    "        self.tau_log_alpha = nn.Sequential(\n",
    "            nn.Linear(num_stats, num_hidden1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(num_hidden1, int(0.5*num_hidden1)),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(int(0.5*num_hidden1), num_latents))\n",
    "        \n",
    "        self.tau_log_beta = nn.Sequential(\n",
    "            nn.Linear(num_stats, num_hidden1),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(num_hidden1, int(0.5*num_hidden1)),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(int(0.5*num_hidden1), num_latents))\n",
    "        \n",
    "        self.prior_mu = torch.zeros((K, D))\n",
    "        self.prior_sigma = torch.ones((K, D)) * 6\n",
    "        self.prior_alpha = torch.ones((K, D)) * 3\n",
    "        self.prior_beta = torch.ones((K, D)) * 3\n",
    "        if CUDA:\n",
    "            self.prior_mu = self.prior_mu.cuda()\n",
    "            self.prior_sigma = self.prior_sigma.cuda()\n",
    "            self.prior_alpha = self.prior_alpha.cuda()\n",
    "            self.prior_beta = self.prior_beta.cuda()\n",
    "        \n",
    "    def forward(self, stat1, stat2, stat3):\n",
    "        q = probtorch.Trace()\n",
    "        stats_d1 = torch.cat((stat1, stat2[:, :, :, 0], stat3[:, :, :, 0]), -1) ## S * B * 3K\n",
    "        stats_d2 = torch.cat((stat1, stat2[:, :, :, 1], stat3[:, :, :, 1]), -1) ## S * B * 3K\n",
    "        ##\n",
    "        q_mu1 = self.mus_mu(stats_d1)\n",
    "        q_sigma1 = self.mus_log_sigma(stats_d1).exp()\n",
    "        q_mu2 = self.mus_mu(stats_d2)\n",
    "        q_sigma2 = self.mus_log_sigma(stats_d2).exp()        \n",
    "        q_mu = torch.cat((q_mu1.unsqueeze(-1), q_mu2.unsqueeze(-1)), -1)\n",
    "        q_sigma = torch.cat((q_sigma1.unsqueeze(-1), q_sigma2.unsqueeze(-1)), -1)\n",
    "        ##\n",
    "        q_alpha1 = self.tau_log_alpha(stats_d1).exp()\n",
    "        q_beta1 = self.tau_log_beta(stats_d1).exp()\n",
    "        q_alpha2 = self.tau_log_alpha(stats_d2).exp()\n",
    "        q_beta2 = self.tau_log_beta(stats_d2).exp()\n",
    "        ##\n",
    "        q_alpha = torch.cat((q_alpha1.unsqueeze(-1), q_alpha2.unsqueeze(-1)), -1)\n",
    "        q_beta = torch.cat((q_beta1.unsqueeze(-1), q_beta2.unsqueeze(-1)), -1)\n",
    "        precisions = Gamma(q_alpha, q_beta).sample()\n",
    "        q.gamma(q_alpha,\n",
    "                q_beta,\n",
    "                value=precisions,\n",
    "                name='precisions')\n",
    "        ##\n",
    "        means = Normal(q_mu, q_sigma).sample()\n",
    "        q.normal(q_mu, \n",
    "                 q_sigma, \n",
    "                 value=means, \n",
    "                 name='means')\n",
    "\n",
    "        p = probtorch.Trace()\n",
    "        p.normal(self.prior_mu, \n",
    "                 self.prior_sigma, \n",
    "                 value=q['means'], \n",
    "                 name='means')    \n",
    "        p.gamma(self.prior_alpha,\n",
    "                self.prior_beta,\n",
    "                value=q['precisions'],\n",
    "                name='precisions')   \n",
    "        \n",
    "        return q, p\n",
    "        \n",
    "def initialize():\n",
    "    enc_eta = Enc_eta()\n",
    "    if CUDA:\n",
    "        enc_eta.cuda()\n",
    "    optimizer =  torch.optim.Adam(list(enc_eta.parameters()),lr=LEARNING_RATE, betas=(0.9, 0.99))    \n",
    "    return enc_eta, optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Eubo_idw(q, p, obs, states, K, D):\n",
    "    ## for individual importance weight, S * B * K\n",
    "    log_q_mu = q['means'].log_prob.sum(-1)\n",
    "    log_q_tau = q['precisions'].log_prob.sum(-1)\n",
    "    log_p_mu = p['means'].log_prob.sum(-1)\n",
    "    log_p_tau = p['precisions'].log_prob.sum(-1)\n",
    "\n",
    "    obs_mu = q['means'].value.view(NUM_SAMPLES, BATCH_SIZE, K, D)\n",
    "    obs_sigma = 1. / (q['precisions'].value.view(NUM_SAMPLES, BATCH_SIZE, K, D)).sqrt()\n",
    "    ##\n",
    "    labels = states.argmax(-1)\n",
    "    labels_flat = labels.unsqueeze(-1).repeat(1, 1, 1, D)\n",
    "    obs_mu_expand = torch.gather(obs_mu, 2, labels_flat)\n",
    "    obs_sigma_expand = torch.gather(obs_sigma, 2, labels_flat)\n",
    "    log_obs = Normal(obs_mu_expand, obs_sigma_expand).log_prob(obs).sum(-1) ## S * B * N, need to group\n",
    "    log_obs = torch.cat([((labels==k).float() * log_obs).sum(-1).unsqueeze(-1) for k in range(K)], -1)\n",
    "    log_weights = log_obs + log_p_mu + log_p_tau - log_q_mu - log_q_tau\n",
    "    weights = F.softmax(log_weights, 0).detach()\n",
    "    eubo = (weights * log_weights).sum(0).sum(-1).mean()\n",
    "    elbo = log_weights.sum(-1).mean()\n",
    "    ess = (1. / (weights**2).sum(0)).mean(-1).mean()\n",
    "    return eubo, elbo, ess, weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_eta, optimizer = initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, EUBO=-329.198, ELBO=-1320.128, ESS=1.100, KL_ex=1168.127, KL_in=162.611 (5s)\n",
      "epoch=1, EUBO=-237.082, ELBO=-815.560, ESS=1.148, KL_ex=1154.097, KL_in=136.533 (5s)\n",
      "epoch=2, EUBO=-209.046, ELBO=-566.886, ESS=1.182, KL_ex=1162.868, KL_in=110.672 (4s)\n",
      "epoch=3, EUBO=-200.844, ELBO=-459.605, ESS=1.203, KL_ex=1177.872, KL_in=89.422 (5s)\n",
      "epoch=4, EUBO=-195.623, ELBO=-412.138, ESS=1.202, KL_ex=1192.492, KL_in=72.913 (5s)\n",
      "epoch=5, EUBO=-190.459, ELBO=-387.780, ESS=1.208, KL_ex=1212.306, KL_in=59.257 (4s)\n",
      "epoch=6, EUBO=-186.281, ELBO=-377.555, ESS=1.202, KL_ex=1238.811, KL_in=48.783 (6s)\n",
      "epoch=7, EUBO=-182.749, ELBO=-377.439, ESS=1.200, KL_ex=1272.324, KL_in=41.556 (5s)\n",
      "epoch=8, EUBO=-178.277, ELBO=-376.048, ESS=1.188, KL_ex=1302.991, KL_in=35.860 (3s)\n",
      "epoch=9, EUBO=-175.398, ELBO=-378.118, ESS=1.194, KL_ex=1330.346, KL_in=32.299 (4s)\n",
      "epoch=10, EUBO=-172.550, ELBO=-383.825, ESS=1.184, KL_ex=1359.052, KL_in=30.014 (5s)\n",
      "epoch=11, EUBO=-170.731, ELBO=-391.858, ESS=1.179, KL_ex=1378.071, KL_in=28.703 (4s)\n",
      "epoch=12, EUBO=-169.184, ELBO=-391.017, ESS=1.183, KL_ex=1388.375, KL_in=28.032 (3s)\n",
      "epoch=13, EUBO=-167.775, ELBO=-393.151, ESS=1.191, KL_ex=1394.624, KL_in=27.489 (3s)\n",
      "epoch=14, EUBO=-166.528, ELBO=-393.422, ESS=1.189, KL_ex=1399.343, KL_in=27.133 (3s)\n",
      "epoch=15, EUBO=-165.599, ELBO=-393.605, ESS=1.188, KL_ex=1395.229, KL_in=26.799 (3s)\n",
      "epoch=16, EUBO=-164.033, ELBO=-390.821, ESS=1.197, KL_ex=1389.567, KL_in=26.754 (5s)\n",
      "epoch=17, EUBO=-163.632, ELBO=-391.207, ESS=1.201, KL_ex=1383.129, KL_in=26.568 (4s)\n",
      "epoch=18, EUBO=-162.277, ELBO=-390.882, ESS=1.204, KL_ex=1380.829, KL_in=26.294 (4s)\n",
      "epoch=19, EUBO=-161.150, ELBO=-392.748, ESS=1.207, KL_ex=1376.510, KL_in=26.044 (4s)\n",
      "epoch=20, EUBO=-160.743, ELBO=-391.884, ESS=1.220, KL_ex=1373.909, KL_in=25.841 (5s)\n",
      "epoch=21, EUBO=-159.711, ELBO=-388.224, ESS=1.213, KL_ex=1358.808, KL_in=25.762 (5s)\n",
      "epoch=22, EUBO=-159.047, ELBO=-393.385, ESS=1.214, KL_ex=1365.865, KL_in=25.450 (5s)\n",
      "epoch=23, EUBO=-158.304, ELBO=-390.922, ESS=1.224, KL_ex=1365.124, KL_in=25.340 (4s)\n",
      "epoch=24, EUBO=-157.809, ELBO=-388.804, ESS=1.228, KL_ex=1362.730, KL_in=25.282 (5s)\n",
      "epoch=25, EUBO=-157.118, ELBO=-391.944, ESS=1.216, KL_ex=1354.125, KL_in=25.148 (5s)\n",
      "epoch=26, EUBO=-156.435, ELBO=-389.501, ESS=1.225, KL_ex=1351.052, KL_in=25.035 (5s)\n",
      "epoch=27, EUBO=-156.487, ELBO=-393.313, ESS=1.232, KL_ex=1354.929, KL_in=24.851 (5s)\n",
      "epoch=28, EUBO=-155.731, ELBO=-389.583, ESS=1.227, KL_ex=1342.930, KL_in=24.807 (4s)\n",
      "epoch=29, EUBO=-154.883, ELBO=-391.170, ESS=1.230, KL_ex=1341.046, KL_in=24.742 (3s)\n",
      "epoch=30, EUBO=-154.478, ELBO=-391.155, ESS=1.232, KL_ex=1343.454, KL_in=24.674 (4s)\n",
      "epoch=31, EUBO=-154.619, ELBO=-390.776, ESS=1.241, KL_ex=1328.636, KL_in=24.643 (5s)\n",
      "epoch=32, EUBO=-153.671, ELBO=-388.189, ESS=1.238, KL_ex=1335.342, KL_in=24.739 (5s)\n",
      "epoch=33, EUBO=-153.113, ELBO=-391.586, ESS=1.232, KL_ex=1341.151, KL_in=24.525 (5s)\n",
      "epoch=34, EUBO=-152.809, ELBO=-390.377, ESS=1.239, KL_ex=1334.712, KL_in=24.541 (4s)\n",
      "epoch=35, EUBO=-152.407, ELBO=-389.588, ESS=1.244, KL_ex=1328.277, KL_in=24.491 (5s)\n",
      "epoch=36, EUBO=-151.805, ELBO=-393.029, ESS=1.255, KL_ex=1320.918, KL_in=24.338 (5s)\n",
      "epoch=37, EUBO=-151.438, ELBO=-390.681, ESS=1.248, KL_ex=1315.237, KL_in=24.322 (5s)\n",
      "epoch=38, EUBO=-151.162, ELBO=-387.475, ESS=1.250, KL_ex=1300.883, KL_in=24.414 (5s)\n",
      "epoch=39, EUBO=-151.021, ELBO=-396.014, ESS=1.245, KL_ex=1318.898, KL_in=23.996 (5s)\n",
      "epoch=40, EUBO=-150.317, ELBO=-394.931, ESS=1.250, KL_ex=1302.740, KL_in=23.926 (5s)\n",
      "epoch=41, EUBO=-149.887, ELBO=-387.027, ESS=1.257, KL_ex=1282.733, KL_in=24.089 (4s)\n",
      "epoch=42, EUBO=-149.821, ELBO=-387.736, ESS=1.254, KL_ex=1274.464, KL_in=24.089 (4s)\n",
      "epoch=43, EUBO=-149.254, ELBO=-383.236, ESS=1.258, KL_ex=1254.150, KL_in=24.116 (5s)\n",
      "epoch=44, EUBO=-148.994, ELBO=-384.481, ESS=1.260, KL_ex=1244.634, KL_in=23.955 (5s)\n",
      "epoch=45, EUBO=-148.972, ELBO=-383.209, ESS=1.267, KL_ex=1222.929, KL_in=23.875 (5s)\n",
      "epoch=46, EUBO=-147.499, ELBO=-381.503, ESS=1.264, KL_ex=1220.918, KL_in=23.786 (4s)\n",
      "epoch=47, EUBO=-147.785, ELBO=-381.823, ESS=1.260, KL_ex=1212.151, KL_in=23.630 (4s)\n",
      "epoch=48, EUBO=-147.676, ELBO=-376.754, ESS=1.270, KL_ex=1196.302, KL_in=23.731 (5s)\n",
      "epoch=49, EUBO=-147.114, ELBO=-376.684, ESS=1.277, KL_ex=1177.712, KL_in=23.542 (5s)\n",
      "epoch=50, EUBO=-146.644, ELBO=-375.739, ESS=1.274, KL_ex=1174.217, KL_in=23.615 (5s)\n",
      "epoch=51, EUBO=-146.283, ELBO=-375.044, ESS=1.276, KL_ex=1168.570, KL_in=23.528 (5s)\n",
      "epoch=52, EUBO=-145.956, ELBO=-377.158, ESS=1.287, KL_ex=1159.363, KL_in=23.315 (5s)\n",
      "epoch=53, EUBO=-145.885, ELBO=-371.140, ESS=1.283, KL_ex=1133.733, KL_in=23.377 (5s)\n",
      "epoch=54, EUBO=-145.227, ELBO=-376.097, ESS=1.286, KL_ex=1132.052, KL_in=23.148 (5s)\n",
      "epoch=55, EUBO=-145.330, ELBO=-368.094, ESS=1.291, KL_ex=1119.329, KL_in=23.354 (5s)\n",
      "epoch=56, EUBO=-144.567, ELBO=-364.409, ESS=1.291, KL_ex=1106.429, KL_in=23.248 (3s)\n",
      "epoch=57, EUBO=-144.523, ELBO=-364.087, ESS=1.294, KL_ex=1092.308, KL_in=23.204 (4s)\n",
      "epoch=58, EUBO=-144.158, ELBO=-362.904, ESS=1.292, KL_ex=1084.264, KL_in=23.041 (3s)\n",
      "epoch=59, EUBO=-143.907, ELBO=-364.907, ESS=1.302, KL_ex=1081.217, KL_in=22.919 (3s)\n",
      "epoch=60, EUBO=-143.775, ELBO=-360.872, ESS=1.299, KL_ex=1066.250, KL_in=22.876 (4s)\n",
      "epoch=61, EUBO=-142.679, ELBO=-356.217, ESS=1.300, KL_ex=1049.440, KL_in=22.979 (5s)\n",
      "epoch=62, EUBO=-143.158, ELBO=-356.298, ESS=1.300, KL_ex=1041.441, KL_in=22.905 (4s)\n",
      "epoch=63, EUBO=-142.382, ELBO=-352.271, ESS=1.308, KL_ex=1045.086, KL_in=23.041 (5s)\n",
      "epoch=64, EUBO=-143.119, ELBO=-351.416, ESS=1.309, KL_ex=1027.376, KL_in=22.944 (5s)\n",
      "epoch=65, EUBO=-142.895, ELBO=-357.894, ESS=1.313, KL_ex=1025.725, KL_in=22.683 (5s)\n",
      "epoch=66, EUBO=-142.803, ELBO=-355.352, ESS=1.307, KL_ex=1012.814, KL_in=22.673 (4s)\n",
      "epoch=67, EUBO=-142.472, ELBO=-355.422, ESS=1.322, KL_ex=1007.639, KL_in=22.556 (5s)\n",
      "epoch=68, EUBO=-141.889, ELBO=-345.886, ESS=1.319, KL_ex=1001.776, KL_in=22.696 (5s)\n",
      "epoch=69, EUBO=-141.889, ELBO=-346.635, ESS=1.319, KL_ex=988.293, KL_in=22.672 (4s)\n",
      "epoch=70, EUBO=-141.828, ELBO=-343.929, ESS=1.320, KL_ex=989.442, KL_in=22.757 (5s)\n",
      "epoch=71, EUBO=-141.548, ELBO=-344.610, ESS=1.322, KL_ex=990.434, KL_in=22.619 (5s)\n",
      "epoch=72, EUBO=-141.323, ELBO=-342.827, ESS=1.325, KL_ex=976.679, KL_in=22.565 (5s)\n",
      "epoch=73, EUBO=-141.191, ELBO=-349.149, ESS=1.320, KL_ex=964.867, KL_in=22.336 (5s)\n",
      "epoch=74, EUBO=-141.601, ELBO=-336.336, ESS=1.327, KL_ex=949.878, KL_in=22.529 (5s)\n",
      "epoch=75, EUBO=-140.819, ELBO=-337.629, ESS=1.332, KL_ex=955.343, KL_in=22.461 (5s)\n",
      "epoch=76, EUBO=-140.489, ELBO=-331.955, ESS=1.331, KL_ex=938.392, KL_in=22.627 (4s)\n",
      "epoch=77, EUBO=-140.727, ELBO=-333.826, ESS=1.332, KL_ex=935.118, KL_in=22.553 (3s)\n",
      "epoch=78, EUBO=-140.232, ELBO=-333.981, ESS=1.341, KL_ex=924.095, KL_in=22.426 (3s)\n",
      "epoch=79, EUBO=-140.250, ELBO=-332.374, ESS=1.339, KL_ex=922.395, KL_in=22.341 (3s)\n",
      "epoch=80, EUBO=-140.216, ELBO=-334.105, ESS=1.343, KL_ex=913.758, KL_in=22.202 (3s)\n",
      "epoch=81, EUBO=-139.582, ELBO=-333.700, ESS=1.340, KL_ex=919.469, KL_in=22.113 (3s)\n",
      "epoch=82, EUBO=-139.820, ELBO=-329.632, ESS=1.353, KL_ex=894.383, KL_in=22.216 (3s)\n",
      "epoch=83, EUBO=-139.476, ELBO=-326.908, ESS=1.351, KL_ex=892.646, KL_in=22.165 (4s)\n",
      "epoch=84, EUBO=-139.749, ELBO=-324.747, ESS=1.354, KL_ex=873.148, KL_in=22.201 (5s)\n",
      "epoch=85, EUBO=-139.246, ELBO=-326.246, ESS=1.353, KL_ex=870.578, KL_in=22.038 (5s)\n",
      "epoch=86, EUBO=-138.938, ELBO=-324.417, ESS=1.358, KL_ex=866.558, KL_in=21.940 (5s)\n",
      "epoch=87, EUBO=-138.481, ELBO=-328.475, ESS=1.357, KL_ex=864.203, KL_in=21.781 (5s)\n",
      "epoch=88, EUBO=-138.514, ELBO=-326.668, ESS=1.360, KL_ex=855.106, KL_in=21.757 (5s)\n",
      "epoch=89, EUBO=-138.376, ELBO=-324.218, ESS=1.368, KL_ex=844.157, KL_in=21.803 (5s)\n",
      "epoch=90, EUBO=-138.098, ELBO=-325.060, ESS=1.371, KL_ex=827.254, KL_in=21.597 (5s)\n",
      "epoch=91, EUBO=-137.927, ELBO=-320.327, ESS=1.381, KL_ex=825.900, KL_in=21.636 (5s)\n",
      "epoch=92, EUBO=-137.990, ELBO=-322.799, ESS=1.380, KL_ex=825.626, KL_in=21.448 (5s)\n",
      "epoch=93, EUBO=-137.444, ELBO=-318.497, ESS=1.374, KL_ex=819.116, KL_in=21.547 (5s)\n",
      "epoch=94, EUBO=-137.657, ELBO=-320.595, ESS=1.388, KL_ex=812.032, KL_in=21.420 (5s)\n",
      "epoch=95, EUBO=-137.164, ELBO=-318.438, ESS=1.389, KL_ex=807.334, KL_in=21.250 (5s)\n",
      "epoch=96, EUBO=-137.203, ELBO=-321.622, ESS=1.391, KL_ex=797.566, KL_in=21.096 (5s)\n",
      "epoch=97, EUBO=-136.404, ELBO=-318.649, ESS=1.390, KL_ex=782.504, KL_in=20.991 (5s)\n",
      "epoch=98, EUBO=-136.218, ELBO=-322.329, ESS=1.391, KL_ex=781.925, KL_in=20.788 (5s)\n",
      "epoch=99, EUBO=-136.491, ELBO=-319.806, ESS=1.399, KL_ex=773.331, KL_in=20.839 (5s)\n",
      "epoch=100, EUBO=-136.280, ELBO=-324.192, ESS=1.404, KL_ex=776.199, KL_in=20.665 (5s)\n",
      "epoch=101, EUBO=-136.249, ELBO=-315.886, ESS=1.407, KL_ex=760.427, KL_in=20.905 (5s)\n",
      "epoch=102, EUBO=-136.042, ELBO=-312.797, ESS=1.416, KL_ex=750.104, KL_in=20.727 (5s)\n",
      "epoch=103, EUBO=-135.513, ELBO=-314.536, ESS=1.412, KL_ex=742.981, KL_in=20.533 (5s)\n",
      "epoch=104, EUBO=-134.739, ELBO=-315.072, ESS=1.425, KL_ex=745.917, KL_in=20.421 (5s)\n",
      "epoch=105, EUBO=-135.176, ELBO=-309.158, ESS=1.428, KL_ex=732.638, KL_in=20.490 (5s)\n",
      "epoch=106, EUBO=-135.095, ELBO=-307.017, ESS=1.435, KL_ex=716.520, KL_in=20.538 (5s)\n",
      "epoch=107, EUBO=-134.833, ELBO=-307.266, ESS=1.427, KL_ex=720.930, KL_in=20.387 (4s)\n",
      "epoch=108, EUBO=-134.445, ELBO=-307.661, ESS=1.436, KL_ex=709.614, KL_in=20.303 (3s)\n",
      "epoch=109, EUBO=-134.186, ELBO=-308.028, ESS=1.439, KL_ex=697.951, KL_in=20.225 (4s)\n",
      "epoch=110, EUBO=-134.665, ELBO=-305.819, ESS=1.449, KL_ex=685.668, KL_in=20.218 (5s)\n",
      "epoch=111, EUBO=-134.196, ELBO=-299.654, ESS=1.449, KL_ex=683.559, KL_in=20.194 (5s)\n",
      "epoch=112, EUBO=-134.144, ELBO=-296.938, ESS=1.447, KL_ex=682.454, KL_in=20.158 (5s)\n",
      "epoch=113, EUBO=-133.930, ELBO=-297.960, ESS=1.452, KL_ex=670.405, KL_in=19.989 (5s)\n",
      "epoch=114, EUBO=-133.343, ELBO=-300.552, ESS=1.444, KL_ex=673.168, KL_in=19.910 (5s)\n",
      "epoch=115, EUBO=-133.314, ELBO=-304.349, ESS=1.446, KL_ex=672.275, KL_in=19.741 (5s)\n",
      "epoch=116, EUBO=-133.488, ELBO=-297.108, ESS=1.461, KL_ex=656.477, KL_in=19.853 (5s)\n",
      "epoch=117, EUBO=-132.983, ELBO=-297.479, ESS=1.460, KL_ex=651.054, KL_in=19.791 (4s)\n",
      "epoch=118, EUBO=-133.494, ELBO=-298.086, ESS=1.464, KL_ex=646.127, KL_in=19.756 (5s)\n",
      "epoch=119, EUBO=-132.540, ELBO=-296.632, ESS=1.473, KL_ex=638.696, KL_in=19.680 (5s)\n",
      "epoch=120, EUBO=-132.789, ELBO=-292.103, ESS=1.466, KL_ex=630.372, KL_in=19.643 (5s)\n",
      "epoch=121, EUBO=-132.536, ELBO=-299.003, ESS=1.469, KL_ex=633.755, KL_in=19.311 (5s)\n",
      "epoch=122, EUBO=-132.312, ELBO=-295.102, ESS=1.467, KL_ex=626.987, KL_in=19.441 (5s)\n",
      "epoch=123, EUBO=-132.411, ELBO=-295.818, ESS=1.468, KL_ex=631.986, KL_in=19.305 (5s)\n",
      "epoch=124, EUBO=-132.277, ELBO=-294.131, ESS=1.483, KL_ex=618.138, KL_in=19.286 (4s)\n",
      "epoch=125, EUBO=-132.311, ELBO=-294.452, ESS=1.480, KL_ex=615.162, KL_in=19.223 (5s)\n",
      "epoch=126, EUBO=-132.113, ELBO=-291.244, ESS=1.475, KL_ex=616.772, KL_in=19.204 (4s)\n",
      "epoch=127, EUBO=-131.684, ELBO=-285.238, ESS=1.484, KL_ex=599.832, KL_in=19.250 (4s)\n",
      "epoch=128, EUBO=-131.606, ELBO=-289.414, ESS=1.479, KL_ex=605.868, KL_in=19.061 (4s)\n",
      "epoch=129, EUBO=-131.532, ELBO=-284.020, ESS=1.493, KL_ex=594.235, KL_in=19.098 (5s)\n",
      "epoch=130, EUBO=-131.791, ELBO=-288.889, ESS=1.489, KL_ex=588.414, KL_in=19.007 (5s)\n",
      "epoch=131, EUBO=-131.477, ELBO=-286.979, ESS=1.488, KL_ex=584.996, KL_in=19.019 (4s)\n",
      "epoch=132, EUBO=-131.572, ELBO=-282.359, ESS=1.496, KL_ex=583.782, KL_in=19.094 (5s)\n",
      "epoch=133, EUBO=-131.376, ELBO=-284.564, ESS=1.487, KL_ex=581.773, KL_in=19.045 (5s)\n",
      "epoch=134, EUBO=-131.073, ELBO=-288.079, ESS=1.485, KL_ex=580.705, KL_in=18.933 (5s)\n",
      "epoch=135, EUBO=-131.170, ELBO=-284.154, ESS=1.496, KL_ex=572.626, KL_in=19.001 (5s)\n",
      "epoch=136, EUBO=-131.084, ELBO=-289.276, ESS=1.496, KL_ex=577.799, KL_in=18.837 (5s)\n",
      "epoch=137, EUBO=-131.272, ELBO=-290.483, ESS=1.501, KL_ex=577.977, KL_in=18.749 (5s)\n",
      "epoch=138, EUBO=-131.528, ELBO=-286.466, ESS=1.501, KL_ex=570.504, KL_in=18.841 (5s)\n",
      "epoch=139, EUBO=-130.876, ELBO=-281.211, ESS=1.501, KL_ex=564.277, KL_in=18.857 (3s)\n",
      "epoch=140, EUBO=-131.215, ELBO=-282.295, ESS=1.501, KL_ex=565.942, KL_in=18.723 (4s)\n",
      "epoch=141, EUBO=-131.174, ELBO=-284.188, ESS=1.496, KL_ex=563.652, KL_in=18.746 (5s)\n",
      "epoch=142, EUBO=-131.171, ELBO=-282.555, ESS=1.506, KL_ex=558.485, KL_in=18.728 (5s)\n",
      "epoch=143, EUBO=-131.322, ELBO=-281.073, ESS=1.498, KL_ex=558.174, KL_in=18.689 (3s)\n",
      "epoch=144, EUBO=-130.977, ELBO=-280.693, ESS=1.502, KL_ex=555.489, KL_in=18.672 (3s)\n",
      "epoch=145, EUBO=-130.797, ELBO=-277.601, ESS=1.510, KL_ex=547.667, KL_in=18.699 (3s)\n",
      "epoch=146, EUBO=-130.669, ELBO=-275.405, ESS=1.508, KL_ex=546.172, KL_in=18.717 (4s)\n",
      "epoch=147, EUBO=-130.758, ELBO=-279.907, ESS=1.498, KL_ex=543.290, KL_in=18.545 (4s)\n",
      "epoch=148, EUBO=-130.545, ELBO=-279.619, ESS=1.497, KL_ex=546.698, KL_in=18.555 (5s)\n",
      "epoch=149, EUBO=-130.562, ELBO=-282.721, ESS=1.506, KL_ex=540.176, KL_in=18.400 (5s)\n",
      "epoch=150, EUBO=-130.916, ELBO=-282.187, ESS=1.506, KL_ex=538.616, KL_in=18.487 (5s)\n",
      "epoch=151, EUBO=-130.556, ELBO=-278.297, ESS=1.509, KL_ex=541.212, KL_in=18.473 (5s)\n",
      "epoch=152, EUBO=-130.571, ELBO=-283.090, ESS=1.503, KL_ex=540.780, KL_in=18.339 (6s)\n",
      "epoch=153, EUBO=-130.448, ELBO=-277.888, ESS=1.509, KL_ex=536.251, KL_in=18.457 (5s)\n",
      "epoch=154, EUBO=-130.497, ELBO=-276.157, ESS=1.505, KL_ex=533.935, KL_in=18.527 (4s)\n",
      "epoch=155, EUBO=-130.380, ELBO=-276.755, ESS=1.525, KL_ex=531.234, KL_in=18.413 (5s)\n",
      "epoch=156, EUBO=-130.363, ELBO=-277.828, ESS=1.516, KL_ex=527.829, KL_in=18.406 (5s)\n",
      "epoch=157, EUBO=-130.241, ELBO=-271.952, ESS=1.512, KL_ex=528.663, KL_in=18.502 (6s)\n",
      "epoch=158, EUBO=-130.269, ELBO=-276.435, ESS=1.502, KL_ex=526.435, KL_in=18.247 (5s)\n",
      "epoch=159, EUBO=-129.714, ELBO=-277.670, ESS=1.502, KL_ex=529.871, KL_in=18.144 (5s)\n",
      "epoch=160, EUBO=-130.144, ELBO=-272.194, ESS=1.527, KL_ex=524.033, KL_in=18.232 (5s)\n",
      "epoch=161, EUBO=-130.026, ELBO=-273.107, ESS=1.515, KL_ex=522.407, KL_in=18.235 (4s)\n",
      "epoch=162, EUBO=-130.159, ELBO=-276.467, ESS=1.517, KL_ex=521.270, KL_in=18.125 (4s)\n",
      "epoch=163, EUBO=-129.963, ELBO=-278.013, ESS=1.505, KL_ex=519.129, KL_in=18.139 (5s)\n",
      "epoch=164, EUBO=-129.592, ELBO=-280.459, ESS=1.513, KL_ex=522.789, KL_in=17.942 (4s)\n",
      "epoch=165, EUBO=-129.720, ELBO=-272.936, ESS=1.523, KL_ex=517.609, KL_in=18.103 (4s)\n",
      "epoch=166, EUBO=-130.229, ELBO=-270.125, ESS=1.516, KL_ex=515.810, KL_in=18.284 (5s)\n",
      "epoch=167, EUBO=-129.911, ELBO=-274.864, ESS=1.517, KL_ex=517.034, KL_in=18.062 (4s)\n",
      "epoch=168, EUBO=-129.555, ELBO=-272.016, ESS=1.514, KL_ex=516.273, KL_in=18.091 (4s)\n",
      "epoch=169, EUBO=-129.786, ELBO=-274.009, ESS=1.513, KL_ex=515.424, KL_in=18.039 (4s)\n",
      "epoch=170, EUBO=-129.586, ELBO=-271.818, ESS=1.521, KL_ex=512.243, KL_in=18.052 (3s)\n",
      "epoch=171, EUBO=-129.544, ELBO=-270.440, ESS=1.524, KL_ex=505.884, KL_in=18.082 (3s)\n",
      "epoch=172, EUBO=-129.556, ELBO=-271.672, ESS=1.520, KL_ex=509.904, KL_in=18.038 (5s)\n",
      "epoch=173, EUBO=-129.364, ELBO=-274.387, ESS=1.521, KL_ex=507.058, KL_in=17.946 (5s)\n",
      "epoch=174, EUBO=-129.553, ELBO=-272.814, ESS=1.527, KL_ex=506.964, KL_in=17.968 (5s)\n",
      "epoch=175, EUBO=-129.340, ELBO=-273.571, ESS=1.522, KL_ex=504.569, KL_in=17.831 (5s)\n",
      "epoch=176, EUBO=-129.396, ELBO=-277.555, ESS=1.523, KL_ex=504.049, KL_in=17.743 (4s)\n",
      "epoch=177, EUBO=-129.369, ELBO=-273.711, ESS=1.524, KL_ex=498.171, KL_in=17.791 (5s)\n",
      "epoch=178, EUBO=-129.666, ELBO=-273.516, ESS=1.515, KL_ex=498.379, KL_in=17.836 (5s)\n",
      "epoch=179, EUBO=-129.445, ELBO=-272.595, ESS=1.520, KL_ex=498.662, KL_in=17.824 (5s)\n",
      "epoch=180, EUBO=-129.169, ELBO=-274.950, ESS=1.515, KL_ex=501.513, KL_in=17.768 (5s)\n",
      "epoch=181, EUBO=-129.434, ELBO=-275.818, ESS=1.518, KL_ex=498.554, KL_in=17.728 (5s)\n",
      "epoch=182, EUBO=-129.141, ELBO=-270.016, ESS=1.538, KL_ex=498.105, KL_in=17.797 (5s)\n",
      "epoch=183, EUBO=-129.401, ELBO=-273.792, ESS=1.533, KL_ex=494.562, KL_in=17.747 (5s)\n",
      "epoch=184, EUBO=-129.256, ELBO=-270.260, ESS=1.520, KL_ex=497.438, KL_in=17.800 (5s)\n",
      "epoch=185, EUBO=-129.282, ELBO=-269.870, ESS=1.528, KL_ex=495.245, KL_in=17.752 (5s)\n",
      "epoch=186, EUBO=-129.213, ELBO=-272.835, ESS=1.527, KL_ex=495.037, KL_in=17.745 (5s)\n",
      "epoch=187, EUBO=-129.068, ELBO=-274.269, ESS=1.524, KL_ex=494.495, KL_in=17.681 (5s)\n",
      "epoch=188, EUBO=-128.936, ELBO=-280.589, ESS=1.527, KL_ex=494.474, KL_in=17.545 (5s)\n",
      "epoch=189, EUBO=-129.018, ELBO=-275.671, ESS=1.524, KL_ex=493.321, KL_in=17.629 (5s)\n",
      "epoch=190, EUBO=-129.120, ELBO=-268.146, ESS=1.535, KL_ex=489.968, KL_in=17.703 (5s)\n",
      "epoch=191, EUBO=-129.136, ELBO=-266.842, ESS=1.539, KL_ex=493.657, KL_in=17.761 (5s)\n",
      "epoch=192, EUBO=-128.993, ELBO=-271.154, ESS=1.532, KL_ex=494.738, KL_in=17.674 (4s)\n",
      "epoch=193, EUBO=-128.597, ELBO=-271.943, ESS=1.535, KL_ex=494.950, KL_in=17.498 (5s)\n",
      "epoch=194, EUBO=-128.969, ELBO=-273.384, ESS=1.527, KL_ex=491.303, KL_in=17.466 (5s)\n",
      "epoch=195, EUBO=-128.558, ELBO=-269.244, ESS=1.534, KL_ex=488.065, KL_in=17.576 (5s)\n",
      "epoch=196, EUBO=-128.901, ELBO=-272.477, ESS=1.533, KL_ex=489.702, KL_in=17.483 (5s)\n",
      "epoch=197, EUBO=-128.669, ELBO=-269.394, ESS=1.531, KL_ex=487.260, KL_in=17.617 (5s)\n",
      "epoch=198, EUBO=-128.289, ELBO=-272.583, ESS=1.538, KL_ex=487.944, KL_in=17.440 (4s)\n",
      "epoch=199, EUBO=-128.536, ELBO=-271.255, ESS=1.542, KL_ex=486.540, KL_in=17.464 (3s)\n",
      "epoch=200, EUBO=-128.270, ELBO=-267.958, ESS=1.536, KL_ex=485.574, KL_in=17.472 (5s)\n",
      "epoch=201, EUBO=-128.474, ELBO=-267.287, ESS=1.539, KL_ex=481.933, KL_in=17.501 (4s)\n",
      "epoch=202, EUBO=-128.322, ELBO=-268.955, ESS=1.543, KL_ex=482.836, KL_in=17.405 (5s)\n",
      "epoch=203, EUBO=-128.624, ELBO=-268.117, ESS=1.541, KL_ex=484.070, KL_in=17.408 (4s)\n",
      "epoch=204, EUBO=-128.443, ELBO=-268.389, ESS=1.529, KL_ex=480.901, KL_in=17.400 (4s)\n",
      "epoch=205, EUBO=-128.301, ELBO=-267.970, ESS=1.548, KL_ex=480.384, KL_in=17.347 (5s)\n",
      "epoch=206, EUBO=-128.352, ELBO=-270.906, ESS=1.543, KL_ex=483.767, KL_in=17.292 (5s)\n",
      "epoch=207, EUBO=-128.659, ELBO=-265.030, ESS=1.544, KL_ex=479.524, KL_in=17.430 (5s)\n",
      "epoch=208, EUBO=-128.598, ELBO=-263.446, ESS=1.548, KL_ex=480.066, KL_in=17.424 (5s)\n",
      "epoch=209, EUBO=-128.321, ELBO=-265.070, ESS=1.552, KL_ex=478.995, KL_in=17.371 (5s)\n",
      "epoch=210, EUBO=-128.257, ELBO=-270.389, ESS=1.550, KL_ex=486.208, KL_in=17.349 (5s)\n",
      "epoch=211, EUBO=-128.683, ELBO=-267.214, ESS=1.552, KL_ex=481.318, KL_in=17.312 (5s)\n",
      "epoch=212, EUBO=-128.015, ELBO=-267.969, ESS=1.549, KL_ex=476.449, KL_in=17.231 (4s)\n",
      "epoch=213, EUBO=-128.427, ELBO=-265.441, ESS=1.553, KL_ex=474.421, KL_in=17.347 (5s)\n",
      "epoch=214, EUBO=-128.037, ELBO=-265.435, ESS=1.553, KL_ex=474.154, KL_in=17.310 (5s)\n",
      "epoch=215, EUBO=-128.203, ELBO=-268.307, ESS=1.551, KL_ex=475.038, KL_in=17.288 (5s)\n",
      "epoch=216, EUBO=-128.028, ELBO=-271.032, ESS=1.544, KL_ex=477.230, KL_in=17.196 (5s)\n",
      "epoch=217, EUBO=-128.211, ELBO=-270.596, ESS=1.551, KL_ex=473.313, KL_in=17.201 (5s)\n",
      "epoch=218, EUBO=-128.007, ELBO=-263.579, ESS=1.539, KL_ex=473.469, KL_in=17.252 (4s)\n",
      "epoch=219, EUBO=-127.986, ELBO=-268.005, ESS=1.556, KL_ex=469.397, KL_in=17.144 (5s)\n",
      "epoch=220, EUBO=-128.359, ELBO=-262.660, ESS=1.549, KL_ex=470.549, KL_in=17.336 (5s)\n",
      "epoch=221, EUBO=-128.082, ELBO=-265.789, ESS=1.544, KL_ex=472.969, KL_in=17.172 (3s)\n",
      "epoch=222, EUBO=-127.479, ELBO=-268.384, ESS=1.543, KL_ex=475.144, KL_in=17.108 (3s)\n",
      "epoch=223, EUBO=-128.052, ELBO=-269.027, ESS=1.544, KL_ex=472.638, KL_in=17.138 (3s)\n",
      "epoch=224, EUBO=-127.959, ELBO=-268.466, ESS=1.552, KL_ex=470.600, KL_in=17.096 (3s)\n",
      "epoch=225, EUBO=-127.691, ELBO=-265.833, ESS=1.562, KL_ex=464.212, KL_in=17.128 (5s)\n",
      "epoch=226, EUBO=-127.928, ELBO=-262.146, ESS=1.546, KL_ex=466.061, KL_in=17.113 (5s)\n",
      "epoch=227, EUBO=-127.524, ELBO=-264.714, ESS=1.556, KL_ex=469.678, KL_in=17.096 (5s)\n",
      "epoch=228, EUBO=-127.919, ELBO=-266.005, ESS=1.557, KL_ex=467.074, KL_in=17.089 (5s)\n",
      "epoch=229, EUBO=-128.080, ELBO=-266.623, ESS=1.553, KL_ex=469.300, KL_in=17.122 (5s)\n",
      "epoch=230, EUBO=-127.478, ELBO=-269.959, ESS=1.561, KL_ex=468.389, KL_in=17.030 (4s)\n",
      "epoch=231, EUBO=-127.868, ELBO=-269.050, ESS=1.559, KL_ex=468.839, KL_in=17.043 (5s)\n",
      "epoch=232, EUBO=-127.947, ELBO=-269.785, ESS=1.563, KL_ex=470.021, KL_in=17.041 (5s)\n",
      "epoch=233, EUBO=-127.639, ELBO=-264.367, ESS=1.562, KL_ex=467.190, KL_in=17.027 (5s)\n",
      "epoch=234, EUBO=-127.547, ELBO=-258.380, ESS=1.563, KL_ex=467.199, KL_in=17.131 (5s)\n",
      "epoch=235, EUBO=-127.416, ELBO=-262.618, ESS=1.559, KL_ex=469.887, KL_in=17.012 (5s)\n",
      "epoch=236, EUBO=-127.566, ELBO=-264.922, ESS=1.550, KL_ex=464.212, KL_in=16.967 (5s)\n",
      "epoch=237, EUBO=-127.584, ELBO=-261.421, ESS=1.554, KL_ex=462.288, KL_in=17.098 (5s)\n",
      "epoch=238, EUBO=-127.591, ELBO=-265.518, ESS=1.556, KL_ex=467.639, KL_in=16.998 (5s)\n",
      "epoch=239, EUBO=-127.556, ELBO=-265.002, ESS=1.549, KL_ex=462.467, KL_in=17.015 (5s)\n",
      "epoch=240, EUBO=-127.563, ELBO=-263.244, ESS=1.570, KL_ex=460.878, KL_in=17.015 (4s)\n",
      "epoch=241, EUBO=-127.387, ELBO=-264.343, ESS=1.541, KL_ex=464.612, KL_in=17.072 (5s)\n",
      "epoch=242, EUBO=-127.457, ELBO=-263.620, ESS=1.564, KL_ex=461.098, KL_in=17.058 (5s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-d29e60055a17>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mkl_mu_in\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mkl_mu_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m## gradient step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0meubo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mEUBO\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0meubo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dev/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dev/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "EUBOs = []\n",
    "ELBOs = []\n",
    "ESSs = []\n",
    "KLs_mu_ex = []\n",
    "KLs_mu_in = []\n",
    "KLs_tau_ex = []\n",
    "KLs_tau_in = []\n",
    "\n",
    "flog = open('results/log-' + PATH + '.txt', 'w+')\n",
    "flog.write('EUBO\\tELBO\\tESS\\tKLs_mu_ex\\tKLs_mu_in\\tKLs_tau_ex\\tKLs_tau_in\\n')\n",
    "flog.close()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    time_start = time.time()\n",
    "    indices = torch.randperm(NUM_SEQS)\n",
    "    EUBO = 0.0\n",
    "    ELBO = 0.0\n",
    "    ESS = 0.0\n",
    "    KL_mu_ex = 0.0\n",
    "    KL_mu_in = 0.0\n",
    "    KL_tau_ex = 0.0\n",
    "    KL_tau_in = 0.0\n",
    "    for step in range(NUM_BATCHES):\n",
    "        optimizer.zero_grad()\n",
    "        batch_indices = indices[step*BATCH_SIZE : (step+1)*BATCH_SIZE]\n",
    "        obs = Xs[batch_indices]\n",
    "        states = STATES[batch_indices]\n",
    "        obs_mu_true = OBS_MU[batch_indices]\n",
    "        obs_sigma_true = OBS_SIGMA[batch_indices]\n",
    "        data = shuffler(torch.cat((obs, states), -1)).repeat(NUM_SAMPLES, 1, 1, 1)\n",
    "        if CUDA:\n",
    "            obs =data.cuda()[:, :, :, :2]\n",
    "            states = data.cuda()[:, :, :, 2:]\n",
    "            obs_mu_true = obs_mu_true.cuda()\n",
    "            obs_sigma_true = obs_sigma_true.cuda()\n",
    "        stat1, stat2, stat3 = data_to_stats(obs, states, N, K, D)\n",
    "        q, p = enc_eta(stat1, stat2, stat3)\n",
    "        eubo, elbo, ess, weights = Eubo_idw(q, p, obs, states, K, D)\n",
    "        ## KLs for mu given true sigma\n",
    "        post_mu, post_sigma = Post_mu(stat1, stat2, enc_eta.prior_mu, enc_eta.prior_sigma, obs_sigma_true, D)\n",
    "        q_mu_mu = q['means'].dist.loc\n",
    "        q_mu_sigma = q['means'].dist.scale\n",
    "        kl_mu_ex, kl_mu_in = kls_normals(q_mu_mu, q_mu_sigma, post_mu, post_sigma)\n",
    "        ##KLs for precision given true mu\n",
    "        post_alpha, post_beta = Post_tau(stat1, stat2, stat3, enc_eta.prior_alpha, enc_eta.prior_beta, obs_mu_true, D)\n",
    "        q_tau_alpha = q['precisions'].dist.concentration\n",
    "        q_tau_beta = q['precisions'].dist.rate\n",
    "        kl_tau_ex, kl_tau_in = kls_gammas(q_tau_alpha, q_tau_beta, post_alpha, post_beta)\n",
    "        kl_tau_ex = (weights * kl_tau_ex).sum(0).sum(-1).mean()\n",
    "        kl_tau_in = (weights * kl_tau_in).sum(0).sum(-1).mean()\n",
    "        kl_mu_ex = (weights * kl_mu_ex).sum(0).sum(-1).mean()\n",
    "        kl_mu_in = (weights * kl_mu_in).sum(0).sum(-1).mean()\n",
    "        ## gradient step\n",
    "        eubo.backward()\n",
    "        optimizer.step()\n",
    "        EUBO += eubo.item()\n",
    "        ELBO += elbo.item()\n",
    "        ESS += ess.item()\n",
    "        KL_mu_ex += kl_mu_ex.item()\n",
    "        KL_mu_in += kl_mu_in.item()\n",
    "        KL_tau_ex += kl_tau_ex.item()\n",
    "        KL_tau_in += kl_tau_in.item()\n",
    "    EUBOs.append(EUBO / NUM_BATCHES)\n",
    "    ELBOs.append(ELBO / NUM_BATCHES)\n",
    "    ESSs.append(ESS / NUM_BATCHES) \n",
    "\n",
    "    flog = open('results/log-' + PATH + '.txt', 'a+')\n",
    "#     flog.write(str(EUBO/NUM_BATCHES) + ', ' + str(ELBO/NUM_BATCHES) + ', ' + str(ESS/NUM_BATCHES) + ', ' + \n",
    "#            str(KL_mu_ex/NUM_BATCHES) + ', ' + str(KL_mu_in/NUM_BATCHES) + ', '+ str(KL_tau_ex/NUM_BATCHES) + ', ' + str(KL_tau_in/NUM_BATCHES) + '\\n')\n",
    "    time_end = time.time()\n",
    "    print('epoch=%d, EUBO=%.3f, ELBO=%.3f, ESS=%.3f, KL_ex=%.3f, KL_in=%.3f (%ds)'\n",
    "            % (epoch, EUBO/NUM_BATCHES, ELBO/NUM_BATCHES, ESS/NUM_BATCHES, \n",
    "               (KL_mu_ex+KL_tau_ex)/NUM_BATCHES, (KL_mu_in+KL_tau_in)/NUM_BATCHES, \n",
    "               time_end - time_start))\n",
    "    print('%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f\\t%.3f'\n",
    "            % (EUBO/NUM_BATCHES, ELBO/NUM_BATCHES, ESS/NUM_BATCHES, \n",
    "               KL_mu_ex/NUM_BATCHES, KL_mu_in/NUM_BATCHES, KL_tau_ex/NUM_BATCHES, KL_tau_in/NUM_BATCHES), file=flog)\n",
    "    flog.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(enc_eta.state_dict(), 'weights/enc-%s' + PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(EUBOs, ELBOs, ESSs, num_samples, num_epochs, lr):\n",
    "    fig = plt.figure(figsize=(15, 15))\n",
    "    fig.tight_layout()\n",
    "    ax1 = fig.add_subplot(3, 1, 1)\n",
    "    ax2 = fig.add_subplot(3, 1, 2)\n",
    "    ax3 = fig.add_subplot(3, 1, 3)\n",
    "    ax1.plot(EUBOs, 'r', label='EUBOs')\n",
    "    ax1.plot(ELBOs, 'b', label='ELBOs')\n",
    "\n",
    "    ax1.tick_params(labelsize=18)\n",
    "    \n",
    "    ax3.plot(np.array(ESSs) / num_samples, 'm', label='ESS')\n",
    "    ax1.set_title('epoch=%d, batch_size=%d, lr=%.1E, samples=%d' % (num_epochs, BATCH_SIZE, lr, num_samples), fontsize=18)\n",
    "    ax1.set_ylim([-200, -90])\n",
    "    ax1.legend()\n",
    "    ax3.legend()\n",
    "    ax3.tick_params(labelsize=18)\n",
    "    plt.savefig('train_' + PATH + '.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(EUBOs, ELBOs, ESSs, NUM_SAMPLES, NUM_EPOCHS, LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_single_batch(num_seqs, N, K, D, batch_size):\n",
    "    indices = torch.randperm(num_seqs)\n",
    "    batch_indices = indices[0*batch_size : (0+1)*batch_size]\n",
    "    obs = Xs[batch_indices]\n",
    "    states = STATES[batch_indices]\n",
    "    data = shuffler(torch.cat((obs, states), -1)).repeat(NUM_SAMPLES, 1, 1, 1)\n",
    "    if CUDA:\n",
    "        obs =data.cuda()[:, :, :, :2]\n",
    "        states = data.cuda()[:, :, :, 2:]\n",
    "    return obs, states\n",
    "\n",
    "obs, states = sample_single_batch(NUM_SEQS, N, K, D, batch_size=25)\n",
    "stat1, stat2, stat3 = data_to_stats(obs, states, N, K, D)\n",
    "q, p = enc_mu(stat1, stat2, stat3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_samples(obs, states, q, batch_size):\n",
    "    colors = ['r', 'b', 'gold']\n",
    "    fig = plt.figure(figsize=(25,25))\n",
    "    xs = obs[0].cpu()\n",
    "    zs = states[0].cpu()\n",
    "    means_mean = q['means'].dist.loc[0].cpu().data.numpy()\n",
    "\n",
    "    for b in range(batch_size):\n",
    "        ax = fig.add_subplot(int(batch_size / 5), 5, b+1)\n",
    "        x = xs[b].data.numpy()\n",
    "        z = zs[b].data.numpy()\n",
    "#         covs = np.zeros((K, D, D))\n",
    "        mu = means_mean[b].reshape(K, D)\n",
    "        assignments = z.argmax(-1)\n",
    "        for k in range(K):\n",
    "            cov_k = np.diag(np.ones(2))\n",
    "            xk = x[np.where(assignments == k)]\n",
    "            ax.scatter(xk[:, 0], xk[:, 1], c=colors[k])\n",
    "#             ax.scatter(x[:, 0], x[:, 1])\n",
    "            plot_cov_ellipse(cov=cov_k, pos=mu[k], nstd=2, ax=ax, alpha=0.2, color=colors[k])\n",
    "        ax.set_ylim([-10, 10])\n",
    "        ax.set_xlim([-10, 10])\n",
    "    plt.savefig('results/modes' + PATH + '.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_samples(obs, states, q, batch_size=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states[0,0].argmax(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
