{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from plots import *\n",
    "from kls import *\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.one_hot_categorical import OneHotCategorical as cat\n",
    "from torch.distributions.beta import Beta\n",
    "from torch.distributions.uniform import Uniform\n",
    "from torch import logsumexp\n",
    "import sys\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 160\n",
    "D = 2\n",
    "## Model Parameters\n",
    "NUM_SAMPLES = 10\n",
    "NUM_HIDDEN = 64\n",
    "NUM_LATENTS = 2\n",
    "NUM_OBS = D\n",
    "BATCH_SIZE = 20\n",
    "NUM_EPOCHS = 20000\n",
    "LEARNING_RATE = 1e-3\n",
    "CUDA = False\n",
    "PATH = 'circles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = torch.from_numpy(np.load('squares/obs.npy')).float()\n",
    "Xs = Xs.transpose(1,0)\n",
    "# mus_true = torch.from_numpy(np.load('circles/mus.npy')).float()\n",
    "# rads_true = torch.from_numpy(np.load('circles/rads.npy')).float()\n",
    "# num_seqs = Xs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_obs= D,\n",
    "                       num_hidden=NUM_HIDDEN,\n",
    "                       num_latents=NUM_LATENTS):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.enc_hidden = nn.Sequential(\n",
    "            nn.Linear(num_obs, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_hidden),\n",
    "            nn.ReLU())\n",
    "        self.local_mean = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        self.local_log_std = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        \n",
    "    def forward(self, obs, N, D, num_samples):\n",
    "        hidden = self.enc_hidden(obs)\n",
    "        q_mean = self.local_mean(hidden)\n",
    "        q_std = torch.exp(self.local_log_std(hidden))\n",
    "        return q_mean, q_std\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_obs=NUM_OBS,\n",
    "                       num_hidden=NUM_HIDDEN,\n",
    "                       num_latents=NUM_LATENTS):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dec_rad = nn.Sequential(\n",
    "            nn.Linear(num_latents, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, 2),\n",
    "            nn.ReLU())\n",
    "        self.dec_angle = nn.Sequential(\n",
    "            nn.Linear(num_latents, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, 2),\n",
    "            nn.ReLU())\n",
    "        self.x_sigma = torch.ones((N, D)) * 0.05\n",
    "    def forward(self, embed, obs, N, D):\n",
    "        x_mus = self.dec_rad(embed) \n",
    "#         angle = self.dec_angle(embed)\n",
    "#         xaxis = rad * torch.cos(angle)\n",
    "#         yaxis = rad * torch.sin(angle)\n",
    "#         x_mus = torch.cat((xaxis, yaxis), -1)\n",
    "        log_p_x = Normal(x_mus, self.x_sigma).log_prob(obs).sum(-1)\n",
    "        return x_mus, log_p_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 1e-2)     \n",
    "        \n",
    "def initialize():\n",
    "    enc = Encoder()\n",
    "    dec = Decoder()\n",
    "#     enc.apply(weights_init)\n",
    "    optimizer =  torch.optim.Adam(list(enc.parameters()) + list(dec.parameters()),lr=LEARNING_RATE, betas=(0.9, 0.99))    \n",
    "    return enc, dec, optimizer\n",
    "enc, dec, optimizer = initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparam(q_mus, q_sigma, N, D, num_samples):\n",
    "    eps = Normal(torch.zeros((num_samples, N, NUM_LATENTS)), torch.ones((num_samples, N, NUM_LATENTS))).sample()\n",
    "    ws = q_mus.unsqueeze(0).repeat(num_samples, 1, 1) + torch.mul(q_sigma.unsqueeze(0).repeat(num_samples, 1, 1), eps)\n",
    "    log_qs = Normal(q_mus, q_sigma).log_prob(ws).sum(-1) ## S * N\n",
    "    return ws, log_qs\n",
    "\n",
    "def oneshot(x, N, D, num_samples):\n",
    "    q_mean, q_sigma = enc(x, N, D, num_samples)\n",
    "    embed, log_q = reparam(q_mean, q_sigma, N, D, num_samples)\n",
    "    log_weights = torch.zeros((num_samples, N))\n",
    "    for s in range(num_samples):\n",
    "        log_pr = Normal(torch.zeros(N, NUM_LATENTS), torch.ones(N, NUM_LATENTS)).log_prob(embed[s]).sum(-1)\n",
    "        x_mus, log_p_x = dec(embed[s], x, N, D)\n",
    "        log_weights[s] = log_p_x + log_pr - log_q[s]\n",
    "    elbo = log_weights.mean(0).mean()\n",
    "    weights = torch.exp(log_weights - logsumexp(log_weights, 0)).detach()\n",
    "    eubo = torch.mul(weights, log_weights).sum(0).mean()\n",
    "    return eubo, elbo, x_mus, embed\n",
    "\n",
    "def shuffler(batch_Xs, N, D, batch_size):\n",
    "    indices = torch.cat([torch.randperm(N).unsqueeze(0) for b in range(batch_size)])\n",
    "    indices_Xs = indices.unsqueeze(-1).repeat(1, 1, D)\n",
    "    return torch.gather(batch_Xs, 1, indices_Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, EUBO=-1105.989, ELBO=-1127.224 (0s)\n",
      "epoch=1000, EUBO=-840.905, ELBO=-841.336 (6s)\n",
      "epoch=2000, EUBO=-840.827, ELBO=-841.232 (6s)\n",
      "epoch=3000, EUBO=-840.727, ELBO=-841.045 (6s)\n",
      "epoch=4000, EUBO=-840.478, ELBO=-841.109 (6s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-c80a48b37ed3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0meubo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melbo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_mus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moneshot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs_shuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_SAMPLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0melbo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mELBOs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melbo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dev/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    100\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m         \"\"\"\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dev/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ELBOs = []\n",
    "EUBOs = []\n",
    "num_batches = int((Xs.shape[0] / BATCH_SIZE))\n",
    "time_start = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    indices = torch.randperm(N)\n",
    "    Xs_shuffle = Xs[indices]\n",
    "    optimizer.zero_grad()\n",
    "    eubo, elbo, x_mus, embed = oneshot(Xs_shuffle, N, D, NUM_SAMPLES)\n",
    "    (-elbo).backward()\n",
    "    optimizer.step()\n",
    "    ELBOs.append(elbo.item())\n",
    "    EUBOs.append(eubo.item())\n",
    "    \n",
    "    if epoch%1000 == 0:\n",
    "        time_end = time.time()  \n",
    "        print('epoch=%d, EUBO=%.3f, ELBO=%.3f (%ds)' % (epoch, eubo, elbo, time_end - time_start))\n",
    "        time_start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_single_batch(num_seqs, N, D, batch_size):\n",
    "    indices = torch.randperm(num_seqs)\n",
    "    batch_indices = indices[0*batch_size : (0+1)*batch_size]\n",
    "    batch_Xs = Xs[batch_indices]\n",
    "    batch_mus = mus_true[batch_indices]\n",
    "    batch_Xs = shuffler(batch_Xs, N, D, batch_size)\n",
    "    return batch_Xs, batch_mus\n",
    "\n",
    "BATCH_SIZE = 20\n",
    "prior_mean = torch.zeros((BATCH_SIZE, N, D))\n",
    "prior_sigma = torch.ones((BATCH_SIZE, N, D))\n",
    "\n",
    "batch_Xs, batch_mus = sample_single_batch(num_seqs, N, D, BATCH_SIZE)\n",
    "elbo, q_mean, q_sigma, p_mean, p_sigma = oneshot(batch_Xs, batch_mus, N, D, NUM_SAMPLES, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 200, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3, 3)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsUAAAEzCAYAAADZ6H6BAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzt3X+MXWd95/HP15MbMg6ICcISeIhxlmUnLTHEYkTS9R+7Sdk6WyAdwgLN0m61VLIqLRLJRqPaSgSJNiizGlG2Wip1rQWxFdnUaeIMAW81BDkVKqpTHMbGcZOp2DYkuemKtGRoiQdyPX72j5k7uXN9zj2/z3N+vF8SAt+ZOfc59w5nPvc53+f7mHNOAAAAQJtt8z0AAAAAwDdCMQAAAFqPUAwAAIDWIxQDAACg9QjFAAAAaD1CMQAAAFovcyg2s8vM7C/N7LSZnTWze/IYGAAAAFAWy9qn2MxM0uXOuZ+aWUfSn0v6tHPuRB4DBAAAAIp2SdYDuPVU/dONf3Y2/sOOIAAAAKiNXGqKzWzMzE5J+pGkx5xzT+RxXAAAAKAMmWeKJck5tybpWjObkPSImV3jnHtq8HvM7ICkA5J0+eWXv/fqq6/O46kBoFRPPvnk3zvndvgeR5ne/OY3u927d/seBgCkEve6nUso7nPOrZjZn0m6SdJTQ187LOmwJE1PT7uTJ0/m+dQAUAoz+6HvMZRt9+7d4poNoK7iXrfz6D6xY2OGWGY2Lun9kp7JelwAAACgLHnMFL9V0v8yszGth+wHnXPfyOG4AAAAQCny6D7xfUl7cxgLAAAA4AU72gEAAKD1CMUAAABoPUIxAAAAWo9QDAAAgNYjFAMAAKD1ct28AwAAoC0WlrqaX1zWiyur2jkxrtn9U5rZO+l7WEiJUAwAAJDQwlJXh46e0WpvTZLUXVnVoaNnJIlgXFOUTwAAACQ0v7i8GYj7Vntrml9c9jQiZEUoBgAASOjFldVEj6P6CMUAAAAJ7ZwYT/Q4qo9QDAAAkNDs/imNd8a2PDbeGdPs/ilPI0JWLLQDAABIqL+Yju4TzUEoBgAASGFm7yQhuEEonwAAAEDrEYoBAADQeoRiAAAAtB6hGAAAAK1HKAYAAEDrEYoBAADQerRkAwAAqKiFpS69kEtCKAYAAKighaWuDh09o9XemiSpu7KqQ0fPSBLBuACUTwAAAFTQ/OLyZiDuW+2taX5x2dOImo1QDAAAUEEvrqwmehzZEIoBAAAqaOfEeKLHkQ2hGAAAoIJm909pvDO25bHxzphm9095GlGzsdAOAACggvqL6eg+UQ5CMQAAQEXN7J0kBJeE8gkAAAC0HqEYAAAArUf5BAAAABJp4k57hGIAAADE1tSd9iifAICWMbMrzexxM3vazM6a2ad9jwlAfTR1pz1migGgfc5LusM59z0ze4OkJ83sMefcX/keGIDqa+pOe8wUA0DLOOf+zjn3vY3//U+SnpZU33ueAErV1J32CMUA0GJmtlvSXklP+B0JgLpo6k57lE8AQEuZ2eslPSzpNufcPw597YCkA5K0a9cuD6MDUFVN3Wkvcyg2sysl/ZGkt0i6IOmwc+73sx4XAFAcM+toPRDf75w7Ovx159xhSYclaXp62pU8PAAV18Sd9vKYKWbBBgDUiJmZpC9Jeto593u+xwMAVZC5ppgFGwBQO/sk/aakG83s1MZ/ftX3oADAp1xrilmwAQDV55z7c0nmexwAUCW5dZ8YtWBj4+sHzOykmZ186aWX8npaAAAAILNcQnHUgg1pfdGGc27aOTe9Y8eOPJ4WAAAAyEXmUMyCDQAAANRdHjPFLNgAAABArWVeaMeCDQAAANQd2zwDAACg9QjFAAAAaD1CMQAAAFqPUAwAAIDWIxQDAACg9QjFAAAAaD1CMQAAAFqPUAwAAIDWy7x5BwAAqKeFpa7mF5f14sqqdk6Ma3b/lGb2TvoeFuAFoRgAgBZaWOrq0NEzWu2tSZK6K6s6dPSMJBGM0UqUTwAA0ELzi8ubgbhvtbem+cVlTyMC/CIUAwDQQi+urCZ6HGg6QjEAAC20c2I80eNA0xGKAQBoodn9UxrvjG15bLwzptn9U55GBPjFQjsAAFqov5iO7hPAOkIxAAApBbU0k+oTNGf2TlZ2bEDZCMUAAKQQ1NJs9k9OSyb11tzmY7Q5A+qBmmIAAFIIamnWu+A2A3Efbc6AemCmGACAFJK0LqPNGcrADoXZMFMMAEAKSVqX0eYMReuX83RXVuX0WunOwlLX99Bqg1AMAEAKQS3NOttMnTHb+tiY6ZWfn9dVB49p39xxQgoKkWSHwoWlrvbNHed3cgjlEwAApBDW0mzwsYntHf30Z+e1stqTVN2Fd9x2r7+4OxQGLRCt4u+kD4RiAABSCmtp1n9s39xxvXyut+Vr/dm7qgQQQlIz7JwYVzcgGA+X7oyaUW77+035BAAABYk7e+dTktvuqK64OxTW4XfSF2aKAQAoSNzZO58ISdlUpfQk7g6Fdfid9IVQDABAQWb3T20pTZCCZ+98qkJIqkqwTGphqavZh05v2axl9qHTkvyUnsTZobAOv5O+UD4BAEBBZvZO6r5b9mhyYlwmaXJiXPfdsqdSgS/ubfei1LmV2D1fP3vRZi29Nad7vn7W04ii1eF30hdmigEAKFCc2Tuf4t52L0qdF34NL6KMerwqqv476QuhGACAlvMZkqhpRlVQPgEAALwJq12uw8KvifFOosdRbYRiAADgje+a5izuvvld6mwb2sFwm+num9/laUTIgvIJAADgje+a5izqPHZcjFAMAC1jZl+W9EFJP3LOXeN7PFhX17Zkeajzwq86jx1bEYoBoH2+IumLkv7I8ziwga2W26PNH36qjppiAGgZ59y3Jf3Y9zjwGrZaboc692RuA2aKAQDwYHDG0IV8D23JmqXOPZnbgFAMALiImR2QdECSdu3a5Xk0zTNcLhEmTVsybs9XT/89CdpOW+LDT1XkEopZtAEAzeKcOyzpsCRNT0+HTWQipaAZw2Fp2pJVrTaZgB7vA1AdejK3QV41xV+RdFNOxwIAoNFGzQyapMmJcd13y57EAbJKtcnUz66L+gBUl57MbZDLTLFz7ttmtjuPYwEAimVmD0j615LebGYvSPqsc+5LfkfVLjsnxgNvpU9OjOs7B29MfdwqbZlM/ey6Ua/9Fds7ck66/cgpzS8ub5lJZ5a9fNQUA0DLOOdu9T2GtpvdP3XRLfU8ZgzDwnbet+fjBLaw+tmgx5scAMPek4nxjn7WuxBY6iJJsw+dVm/NbX5t9qHTkmjRV6TSWrKZ2QEzO2lmJ1966aWynhYAgMqZ2Tup+27Zo8mJ8UzlEsPK2DI5blnEmFngzw8/3vQyi7D3xEyBM+l3PHhatx05tRmI+3prTvd8/Wzh422z0maKWbQBAMBritgJrYxth+OWRay54D/1w483vcwi7D25/cipwO8Pe90k6eVzvULGiHWUTwAA0CBFbzsct255ckTddJrj1VnQezKqRRv8yKV8YmPRxl9ImjKzF8zst/M4LgAAqJaw+uThx+OWcsQ9XtMEvT5RJsY7BY0GUk6h2Dl3q3Purc65jnPubaxiBgCgmeKG3bh102XUQVfR8OsTVoPd19lmuvvmd5UzuApZWOpq39xxXXXwmPbNHS+01pzyCQAAEFuSuuU4pRxl1EFX1eDrE7TJh0lyWv9A0ZbXZFDZm9EQigEAQCJ51y0XXQddB23+cBCm7EWYhGIAAOBNE3sUpz0nPhxsVfYiTEIxAKAWmhie6ijP96Hs2+Nxx5Tl/Kp4TnVV1mY0fYRiAEDlETSqIe/3oWo9ivM4v1Hn1P86H+ziKWrnxzCl7WgHAEBaUUED5cj7fahaj+I8zi9s7P2A3dSd+4pQ1M6PYZgpBgBUXtXCU1vl/T6UfXs8Sh7nF3ZOY2a5zoq3pZyozDprZooBAJXX1g0eypCkD2ze70PYBhbnXj3vZQY1j/ML67sctn1zmg8U/TIPZp3zRSgGAFTeqA0eymzu3xT912z3wWO6/cipLeHq9iOntDvktcx7o43+7fHhndpePtfzEvLyOL+wW/7D21v3pflAEVbmcceDp/n9z4DyCQBA5YX1cJVUiQV4dbqVPbyYbHj+sv/voNeyiF66M3snNb+4rJXV3pbHfSy4y+v8wm7557VoLGx2ec05FqBmQCgGANRCUNDYN3d8ZJ1mGWE1bceCpGPL61yCZhnDBAXTImo8g2pwRz1epKJqWPP8QBFWtyz57d5Rd4RiAEBtjVoYVVYbtzRtxZKO7a6FM7r/xHMjZ3HjSlrDWsZixjGzwJrbMbPCn7tMeQXuoFZlg1iAmg41xQCA2hq1MKqsNm6jWnCF1TknGdvCUndLII76/ihJa1jLWMwYtggt7PG269cth31oYAFqOoRiAEBtjVoYVVYbt1EBJKwzQJKxzS8uXxSIo44zStBrZkP/3VfkRgmDwhahhT2O9WD8+Y+9J9eFj21HKAYA1Nao5v5ltXELays2aHhWN8nYRgXfNOcS9Jp94ePX6tm5D+gLH7+2tI0SBuXd1aJpwjqslL25RdNRUwwAqLWwOs2ytogdXkAVZ1Y3ydjCFlXZxnHSjjnoNStzo4Th55XYAjlIVP25r/esiQjFAIBGKjNo9YPJwlJXdzx4OrAWdnBWN8nYbrh6h7564rmLHv+X73hTo8JQmeGuTi300izkRDqEYgBAYxURtMICVX9GLygQB80Cxx3b48+8FPj4s/9Ah4E0iu5KknfgZovz8hCKAQCtkiW0jApUYf1/x8wy1XkSivJV5Mxr0O/H7EOndfejZ/WT1V6qkBxWPkOHifyx0A4A0Br90DK4rXGS7YRHBaqwkHrBuUxhq6wFg21R5IeMoN+P3prTymov1e+bxCLEMhGKAQCtkbV38ahAlTW8hnUYIBTlq8gPGXGCddL+0nSYKA/lEwCA1sg6SzjqVnaWbhdx6lyrtjCsTovVBhXZlWTU9suDks5K02GiHIRiAEAtpQllWeszRwWqLOE1qs61aqGorC20i1Dkh4yo7Zf7KH2pJkIxAKB20oayrLOEUYEqbXit22K6urcJy+tDRtAHs/tu2aO7Hz2rldVe4M9Q+lJdhGIAQC0MBpBtZhe1PosTyvKYJSxi1rZuHQbqFuKLEPbB7L5b9ujy110SGIqzdiKJ466FM3rgiee15pzGzHTrdVfq3pk9hT1fkxCKAQCVNxxAgnoBS/FCWdVKEaTydt/LS91CfBHSdCJZy9iJJMpdC2e2bPSy5tzmvwnG0eg+AQCovLAewMPqGsrq1mGAjhjpOpGYlKgdW1IPPPF8osexFTPFAIDKizMDXPVQFrUwsIoz2GGq2hGjTFGdSG4/ckrD9zOcdFGJT9TvRZIFpWF3UMIex1aEYgBA5YUFkDEzXXBuMyxI0r6546l3qysq5AXVn95+5JRO/vDHtb2tXacQX4SoTiS3HTkV+HODH/CiFowmXVA6FlBr3398UF3b6RWN8gkAQOWF3a7//Mfeo7+d+4C+c/BGSUq9W13Wne6iBJV/OEn3n3iu0NvpKE5UycvkiE1C+hu13Hbk1MjNZMLqlu/5+tnAjV5uve7KwOccfLzo3/U6Y6YYAFB5cW7XZ2kTVnSLsbDyj6Db6aiPUbPlYTPJN1y9I7KXcf/3Jez35uVzPb18br27xeDscf+uw6juE3Vvp1ckQjEAIHdF3J6Nul2fpU1Y0S3GRu101qY2Zm0S9kEuzqLR/kK9uDvkDYbae2f2jCzJoZ1eOEIxACBXvnY7y9ImrOgWY2ELr/J8jrxQb5qfoA9yt4fUGvcNLhiNu0OelM9W5W1HTTEAtJCZ3WRmy2b2AzM7mOexR92eLVKWNmFFtxib2TupT1y/Szb0eNU6ZlBvWryJ7Z3Qr12xvbOlLjmobnliPPjnk2xV3vZ2emGYKQaAljGzMUl/IOnfSHpB0nfN7FHn3F/lcXxft2eztAkro8XYvTN7NP32N1V6FpZ602ItLHX105+dD/369ksvueh1Hp5tHr4TI+W7VXmbEYoBoH3eJ+kHzrm/kSQz+2NJvyYpl1Ds8/ZsljZhZbQYq3obM+pNizW/uKzehfCewXF3ZOwfq0pblTdBLqHYzG6S9PuSxiT9T+fcXB7HBQAUYlLS4BZXL0i6bvAbzOyApAOStGvXrkQHr9uWxXVTZM1vkR9oqFWODr1xX2dCbTEyh+Kib8MBce0+eOyix56d+8Dm/16/5fR9rfYuBP785ZeO6ZVX1zabn08muGj3L/ZBf0w626TXX9bRyrle5B+CweOYSYM92E0KXKRzxfaOPvuhd6W6QC4sdXXP189utvYZ1n/O4dfiroUzmy1/wsaV1n/7+LVc7Is3XNoqDb2NzrnDkg5L0vT0dKK3mNuzxUm7iDFuIC3qA42vxZdV88bxjlZWg6+3fHD0L4+Z4kJvwwFxBAXi/uPPzn1AC0td/ecjpxQch9e98ur6xbq/G1CSPzajVgf3Liiwn+TwMYePM7wpUVgqeflcT7MPnY4cZ9C4Zx86rd5aeN7pf2Vw3Cd/+GN99cRzkeNKq78LVJv+UHrwgqTBLv9vk/Rink/QlJmsqs1upqn5TRJIi/pAQ63y+vvwyqvB9cQT4x3dfXO6yQ3kJ49QHHkbDvBtfnF5ZCAOE+eiHafnZJxjJj3OoN6aS/zHZX5xeWQgHtYf9//7yc/SDDGRNv2h9OS7kt5pZldJ6kr6dUn/3u+QqqfI2c20YTtNzW/SQFrEBxpqlcOvuVds72jpM7/iYUQYlkcojrwNJ2WrTwOyynLhjfrZNMcO+pmsfxyS/nzacec9Mxz2PCiOc+68mX1K0qLW14J82Tl31vOwKqeo2c0sYTtNze+oQFrWTDi9ccPfh5WQ8jWpencqmi6PPsWxbsM55w4756adc9M7duzI4WmB+LJceKN+Ns2xg34m6x+HpD+fdtxjFvQ5OF9t+kPpi3Pu/zjn/oVz7h3Ouc/5Hk8VFTW7maWPc1CPWdN6sN43dzywn3DY/58mtndK60lMb9zw9yHscXpGly+PULx5G87MLtX6bbhHczgukJvZ/VOpftnjXLSDLvZpjpn0OIM6Y5b4j8vs/il1xuIH3P64b73uyuhvzqhNfyhRXUlDTFxZwvbgZg7S1gW4YaEpLJA6p9I2WQnahGJwk4omWVjqat/ccV118NiWDypJPhgsLHV1x4Onc3t/wsaErTKXT3AbDlXw7NwHRnaf6F94i+g+MbgwJUv3ieHjFN19ov/9SbtP9H+O7hNouqI6MWQtJej//3Df3PGLjhNU3hG2eC5su+E44TzNbf0mLL6MOu84pTFRr1v/GGvDq603JL1TQeeP+MyFvOhFmp6edidPniz9eQEgKzN70jk37XscZWrzNbuIms6wHcmSzpxedfBY4AdSk/S3A+0owwSFamn9Q/B3Dt4Y+nN5jb9OFpa6uvvRsxe1Uxs+77Sv6aCwY6Q5Vl5jqru41212tAMAIEQRs5t5tT3LOuOcdia8be3VRrXdHD7vPOrQR33vqHKLsN8nOn/ERygGAKBkYWE7ycx01vKOtOE8SchqQveEqHaZg+edR5eNsGOMmQXOxgeVR9x+5JRuO3JKkxPjmtjeCSyRY0HzxQjFAABUQJzaz+GQ+ZH3TurxZ15KHTrTzITHDX5hYe3kD3+se2f2JHpOn5JszZxHHXrYMcLKU4JC++Diy842U2fMtvRIblvnj7gIxQAAVEBUWUJQyHz4yW6ptbwLS1298vOLd2ULCllhYe3+E89p+u1vqs2McdiHAOni886jNCbpMaJCe++C08R4R5e/7pJaz9iXgVAMAGiMMm7XF/UcUWUJZdXyhp1fWG1tWAecsPNxqteulUEzt1L4eedRh57kGKNCe99PVns69Vl2zYtCKAYANEIZraeKfI6osoQyFkyNOr+w2trtl14SeO6jwlqdFnmNmrnt9/8dPM8xM9163ZWllYiEhfZB1A/HQygGADRCnjOpYbOlRc7WRtWjlrFV8qjzSxrK+72Qg9rG1S2kBc3chs2crzmnr554TpJKC8aXdbaFhmLqh+PLY0c7AAC8y2smddT2ukXO1kbt+lbGVsmjzi/pDn8zeyf1iet3aXjfzKaEtKiuFA888XzhY+j/rg52l+iMmSbGO43fObAIzBQDABohr5nUUbOlRc/WjqolTbIAK23d86jzC7tN/8rPz2thqRt4/Htn9mj67W+qfVu2IFEfhMJ2pEsjyZ2L3prT5a+7hBriFAjFAIBGyGtb5lGzpV/4+LWFbP0cJigMRe1ClqXuedRrGLY1/Mpqb+Txm7C9c5CoBW5jZrksyhz1frIxR74onwAANEJU+UFco8oE8nqOOEaVcYwyaqY7StT5zeyd1PZLL55Pi3v8JgkqZxl0/T+7ItX7NyzqzkWQutVsVwUzxQCAxshjVjJqxrmsmc+0i/qyzh5GnR+zk+sGy1mGZ4z3veNNevYfVhO/f0Ezy1W6c9F0hGIAAAbksQFDHtKGz6LrnpMcvwnbPI/SP5fhYPq9534Sughv8P0bfH3eON7RK6+e39x5rj+z/MbxjlZWg7dprsrvalMQigEAGFKFOti04fON451Ct/UNmkk3STdcveOiMRXdN7oKwmb0w/Tfv+HXJyj4rvbWdFlnm8Y7Y97vXLQBNcUAABSsv8nDVQePad/c8Vh1pXFbsA3XHq+s9iS3vuNaEXXPM3sn9ZH3Tm5pteYkPfxkd8t5ZaltrpMkZSOdbbb5/kW1dOtbOdcrrY697ZgpBgCgQGlnTOPeGg9sy3XBafull2jpM8W05Xr8mZcu2pRjuF62LbXHcbZZ7nv9ZZdEvj5Bx2c2uByEYgAACpRlF7w4YchH+IzznGXswFcFcbZZ7lsZaGUXJ0yzaK5clE8AAFCgokOrj7ZccZ6zjB34qqDfxm7Mhvfuu9jw6zPKmJk+8t58Z4jTlPG0CaEYAIACFR1afYTPOM9ZZk9n32b2TurzH3vPyL7FQa/PFds7od+/5txFddpZpO173SaUTwAAUKAbrt6h+088t6UGN8/Q6qMtV9znbFMt7PBr8sbxjszWSybCXp/PfuhdI0svVntruuPB07r9yKnM72uWMp62IBQDAFCQhaWuHn6yuyUQm5T7bXEf4TPJcza9X3Ff0vdh1AYgfWtua9/iwZ9Loi0LH7OgfAIAgIIEzc45rXdvaAtu2482s3dS3zl4oyZjlNNkaWnHltDRCMUAABSE2bn29CvOKqhOO0ja352wOvAbrt7B4rsNlE8AAFCQtrQlG4UPBvEM1yRvM9ssnRi0c2I8VTlKUB34DVfv0MNPdhu/62BchGIAAAoS1MO2iW3JRon6YNDkeuOk5zZYk3zXwpnABZo3XL0j9fbZwzXP++aOs/huAKEYAICC5N0Zok4Bsj/W7sqqTArsvpF2t786yHJuoxZoPv7MS7kFWWbxtyIUAwBQoLw6Q9QpQA6P1UmbwXhyIMw3eaYySwu0UQs08wyylPdsxUI7AABqoE4L1sJC3eTEuL5z8MbNUNjkmcos5zbqZ/PsItGWXQfjIhQDAFADdQqQccfa5DZhWc5t1M/mGWTbtOtgHJRPAABQA3W61R13rL4WIpZRm53l3Eb9bN516m3adTAKoRgAgBqoYieLsHAZd6w+tqguqzY7y7lF/SxBthiEYgAAasBHgBwlTriMM9ayA16WBXBJDZ/bwlJX++aOB74mgx8w3jjekZm0cq7n/X1uE0IxAAA5KOOWfJVmCKPCZZXGOshXbfaoDxGStnxtZbW3+XiVu4w0DaEYAICM6tQuLS91Wvg3yFdtdlT3kOGvBX1fU3+XqoLuEwDQImb2UTM7a2YXzGza93iaok7t0vJSROeIfnnBVQePad/ccS0sdVMfK4yvNmSjPkRkadOG/BCKAaBdnpJ0i6Rv+x5Ik+Q9a1pGOMwq73DZn23vrqzK6bXZ9rzP3VcbslEfIrK0aUN+MpVPmNlHJd0t6Rckvc85dzKPQQEAiuGce1qSzMz3UBolz1vydSnFyHvhn88FcGWI6sgx/LVBvruMtEXWmuL+jMP/yGEsAADUUp7t0soMh1nlGS7rUqOcdkFlnA8RdJ/wK1MoZsYBAKrHzL4l6S0BX7rTOfe1mMc4IOmAJO3atSvH0TVTnrOmdQmHeavD5iRZZ/FHfYioareONqH7BAA0jHPu/Tkc47Ckw5I0PT3tMg+qBfIKNXUIh0Wo4uYkw6oyi3/Xwhk98MTzWnNOY2a69borde/MntKev6kiQ3EeMw4bx2HWAQCACHUIh0Wo2uYkQeLM4keVV2TdpOOuhTP66onnNv+95tzmvwnG2USG4jxmHDaOw6wDAHhmZh+W9N8l7ZB0zMxOOef2ex4WBtQhHBal6iUEUbP4UeUVw19Ps0nHA088H/o4oTgbyicAoEWcc49IesT3ODBa1cNhW0XN4keVVwR9Pex7w6y54HnFsMcRX6Y+xWb2YTN7QdIvaX3GYTGfYQEAAFRLVI/jqPKKPDbpGAtpbhD2OOLL2n2CGQcAANAao2bxo8orwr4e9L1hbr3uyi01xYOPIxt2tAMAAMhB1C5/QV8P+94w987s0W9cv2tzZnjMTL9x/S7qiXNATTEAAJ6l3RAC1RK1SHL462k36bh3Zg8huACEYgAAPKrLts6IJ2qRJIsoq4tQDACAR1XZEAKQ2n3XglAMAIBHbd3WGdXT9rsWLLQDAMCjsG4DTd/WGdUz6q5FGxCKAQDwKKpjAVCWtt+1IBQDAOBR1IYQQFnafteCmmIAADyjIwGqIGob66YjFAMAgEpoc+eDKojqs9x0hGIAAOBd2zsfVEWb71oQigEAgHf0a84Hs+3pEYoBAIB3be98kAdm27Oh+wQAAPCuip0PFpa62jd3XFcdPKZ9c8e1sNT1NpY42t5nOCtCMQAA8K5q/Zr7s67dlVU5vTbrWuVgzGx7NoRiAADgXdX6Nddx1rWKs+11Qk0xAACohCp1PqjjrOvs/inN/slp9S64zcc626w1fYazYqYYAABgSG1nXS3i3whFKAYAABiSpsbZ98K8+cVl9dbclsd6a67SJR9VQvkEAADAkKS7u1WhHVodSz6qhFAMAAAQIElzp86qAAAI/UlEQVSNcxU2H9k5Ma5uQACufMlHRVA+AQCAJ75vtyM/VZilrVpbu7phphgAAA+qcLsd+anCLG3Skg9sRSgGAMCDKtxuR35m909t+ZAj+ZmlrVJbu7ohFAMA4EEVbrcjP8zS1h+hGAAAD6pwux35Ypa23lhoBwCAByyKQl01dYEoM8UAAHjA7XbUUZMXiBKKAQDwhNvtqJsmLxAlFAMAUBELS11mjlFpTV4gSk0xAAAV0L8t3V1ZldNrt6WbUq+JZghbCNqEBaKEYgAAKmDUbWmgKpq8QJTyCQAAKqDJt6XRHE1eIEooBgCgAuhbjLpo6gJRyicAAKiAJt+WBuqAmWIAaBEzm5f0IUmvSvq/kv6jc27F76ggNfu2NFAHhGIAaJfHJB1yzp03s/8q6ZCk3/U8Jmxo6m1poA4ylU+Y2byZPWNm3zezR8xsIq+BAQDy55z7pnPu/MY/T0h6m8/xAEBVZK0pfkzSNc65d0v6a63POAAA6uGTkv7U9yAAoAoylU8457458M8Tkv5dtuEAALIys29JekvAl+50zn1t43vulHRe0v0hxzgg6YAk7dq1q6CRAkB15FlT/ElJR8K+yAUWAMrhnHv/qK+b2W9J+qCkX3bOuZBjHJZ0WJKmp6cDvwcAmiQyFOcx4yBxgQWAKjCzm7S+sO5fOefO+R4PAFRFZCjOY8YBAFAZX5T0OkmPmZkknXDO/Y7fIQGAf5nKJ5hxAIB6cc79c99jAIAqytp94ouS3qD1GYdTZvaHOYwJAAAAKFXW7hPMOAAAkMLCUpfd64AKYUc7AABKtrDU1aGjZ7TaW5MkdVdWdejoGUkiGAOeZC2fAAAACc0vLm8G4r7V3prmF5c9jQgAoRgAgJK9uLKa6HEAxSMUAwBQsp0T44keB1A8QjEAACWb3T+l8c7YlsfGO2Oa3T/laUQAWGgHAEDJ+ovp6D4BVAehGAAAD2b2ThKCgQqhfAIAAACtRygGAABA6xGKAQAA0HqEYgAAALQeoRgAAACtRygGAABA6xGKAQAA0HqEYgAAALQeoRgAAACtRygGAABA6xGKAQAA0HqEYgAAALQeoRgAAACtRygGAABA6xGKAQAA0HqEYgAAALQeoRgAAACtd4nvAQAAADTJwlJX84vLenFlVTsnxjW7f0ozeyd9DwsRCMUAAAA5WVjq6tDRM1rtrUmSuiurOnT0jCQRjCuO8gkAAICczC8ubwbivtXemuYXlz2NCHERigEAAHLy4spqosdRHYRiAACAnOycGE/0OKqDUAwAAJCT2f1TGu+MbXlsvDOm2f1TnkaEuFhoBwAAkJP+Yjq6T9QPoRgAACBHM3snCcE1RPkEAAAAWo9QDAAAgNYjFAMAAKD1CMUAAABovUyh2Mz+i5l938xOmdk3zWxnXgMDAOSP6zYABMs6UzzvnHu3c+5aSd+Q9JkcxgQAKA7XbQAIkCkUO+f+ceCfl0ty2YYDACgS120ACJa5T7GZfU7Sf5D0E0k3ZB4RAKBQXLcB4GLm3OhJAjP7lqS3BHzpTufc1wa+75Cky5xznw05zgFJBzb+eY2kp1KNuHreLOnvfQ8iB005D4lzqaqmnMuUc+4NvgcxSh7X7aFr9pSk5SLGmlJTfpfSaOu5t/W8pfaee57n/Xbn3I6ob4oMxXGZ2dslHXPOXRPje08656ZzeWLPmnIuTTkPiXOpqqacS1POQ0p23a6SJr0HSbX13Nt63lJ7z93HeWftPvHOgX/eLOmZbMMBABSJ6zYABMtaUzxnZlOSLkj6oaTfyT4kAECBuG4DQIBModg595GUP3o4y/NWTFPOpSnnIXEuVdWUc6n1eWS4bldJrd+DjNp67m09b6m95176eedWUwwAAADUFds8AwAAoPW8heKmbDVqZvNm9szGuTxiZhO+x5SWmX3UzM6a2QUzq+VKVzO7ycyWzewHZnbQ93jSMrMvm9mPzKzWrQvN7Eoze9zMnt743fq07zGlZWaXmdlfmtnpjXO5x/eY2qxJ196kmnCtTqIp1/WkmvJ3ICmffzd8zhQ3ZavRxyRd45x7t6S/lnTI83iyeErSLZK+7XsgaZjZmKQ/kPRvJf2ipFvN7Bf9jiq1r0i6yfcgcnBe0h3OuV+QdL2k/1Tj9+Tnkm50zr1H0rWSbjKz6z2Pqc2adO1NqtbX6iQadl1P6itqxt+BpLz93fAWipuy1ahz7pvOufMb/zwh6W0+x5OFc+5p51yVGvQn9T5JP3DO/Y1z7lVJfyzp1zyPKRXn3Lcl/dj3OLJyzv2dc+57G//7nyQ9LWnS76jScet+uvHPzsZ/anndaoImXXuTasC1OonGXNeTasrfgaR8/t3wWlNsZp8zs+clfUL1nSke9ElJf+p7EC02Ken5gX+/oJoGsCYys92S9kp6wu9I0jOzMTM7JelHkh5zztX2XBqGa29zcV1vsbL/bmTtUzxS1Fajzrk7Jd25sdXopyQFbhHtW5wtU83sTq1P+d9f5tiSirv9a01ZwGPM5FWAmb1e0sOSbhu6S1Qrzrk1Sddu1K8+YmbXOOdaVe9XpiZde5Nq+LU6Ca7rLeXj70ahodg59/6Y3/q/JR1TRUNx1HmY2W9J+qCkX3YV73GX4D2poxckXTnw77dJetHTWLDBzDpav7Dd75w76ns8eXDOrZjZn2m93o9QXJAmXXuTavi1Ogmu6y3k6++Gz+4Tjdhq1MxukvS7km52zp3zPZ6W+66kd5rZVWZ2qaRfl/So5zG1mpmZpC9Jeto593u+x5OFme3odzgws3FJ71dNr1tNwLW3Nbiut4zPvxs+a4rnzOwpM/u+pF+RVNdWTV+U9AZJj220l/tD3wNKy8w+bGYvSPolScfMbNH3mJLYWHTzKUmLWi/Mf9A5d9bvqNIxswck/YWkKTN7wcx+2/eYUton6Tcl3bjx/49TZvarvgeV0lslPb5xzfqu1muKv+F5TG3WmGtvUnW/VifRpOt6Ug36O5CUt78b7GgHAACA1mNHOwAAALQeoRgAAACtRygGAABA6xGKAQAA0HqEYgAAALQeoRgAAACtRygGAABA6xGKAQAA0Hr/H4x+Z++7BP6VAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax1 = fig.add_subplot(1,2,1)\n",
    "ax2 = fig.add_subplot(1,2,2) \n",
    "ax1.scatter(x_mus[:,0].data.numpy(), x_mus[:,1].data.numpy())\n",
    "ax2.scatter(embed[0, :, 0].data.numpy(), embed[0, :, 1].data.numpy())\n",
    "ax1.set_xlim([-3,3])\n",
    "ax1.set_ylim([-3,3])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(EUBOs, ELBOs, ESSs, KLs_eta_ex, KLs_eta_in, KL_z_ex, KL_z_in):\n",
    "    fout = open('results/logs-' + PATH +'.txt', 'w+')\n",
    "    fout.write('EUBOs, ELBOs, ESSs, KLs_eta_ex, KLs_eta_in, KL_z_ex, KL_z_in\\n')\n",
    "    for i in range(len(EUBOs)):\n",
    "        fout.write(str(EUBOs[i]) + ', ' + str(ELBOs[i]) + ', ' + str(ESSs[i]) \n",
    "                   + str(KLs_eta_ex[i]) + str(KLs_eta_in[i]) + str(KLs_z_ex[i]) + str(KLs_z_in[i]) + '\\n')\n",
    "    fout.close()\n",
    "save_results(EUBOs, ELBOs, ESSs, KLs_eta_ex, KLs_eta_in, KLs_z_ex, KLs_z_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(EUBOs, ELBOs, ESSs, KLs_eta_ex, KLs_eta_in, KLs_z_ex, KLs_z_in, num_samples):\n",
    "    fig = plt.figure(figsize=(20, 30))\n",
    "    ax1 = fig.add_subplot(3, 1, 1)\n",
    "    ax1.plot(EUBOs, 'r', label='EUBOs')\n",
    "    ax1.plot(ELBOs, 'b', label='ELBOs')\n",
    "    ax1.tick_params(labelsize=18)\n",
    "    ax1.set_ylim([-220, -130])\n",
    "    ax1.legend(fontsize=18)\n",
    "    ##\n",
    "    ax2 = fig.add_subplot(3, 1, 2)\n",
    "    ax2.plot(KLs_eta_ex, '#66b3ff', label='KLs_eta_ex')\n",
    "    ax2.plot(KLs_eta_in, '#ff9999', label='KLs_eta_in')\n",
    "    ax2.plot(KLs_z_ex, '#99ff99', label='KLs_z_ex')\n",
    "    ax2.plot(KLs_z_in, 'gold', label='KLs_z_in')   \n",
    "    ax2.plot(np.ones(len(KLs_z_in)) * 5, 'k', label='const=5.0')\n",
    "    ax2.legend(fontsize=18)\n",
    "    ax2.tick_params(labelsize=18)\n",
    "    ax2.set_ylim([-1, 30])\n",
    "    ##\n",
    "    ax3 = fig.add_subplot(3, 1, 3)\n",
    "    ax3.plot(np.array(ESSs) / num_samples, 'm', label='ESS')\n",
    "    ax3.tick_params(labelsize=18)\n",
    "    ax3.set_xlabel('epochs (%d gradient steps per epoch)'  % num_batches, size=18)\n",
    "    ax3.legend()\n",
    "    plt.savefig('results/train-' + PATH + '.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(EUBOs, ELBOs, ESSs, KLs_eta_ex, KLs_eta_in, KLs_z_ex, KLs_z_in, NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS = 50\n",
    "def reverse(X, z, mus_prev, precisions_prev, N, D, K, batch_size):\n",
    "    data = torch.cat((X, z), dim=-1).view(batch_size*N, -1)\n",
    "    q_mean, q_nu, q_alpha, q_beta, q_sigma, mus, precisions = enc_global(data, K, D, 1, batch_size)  \n",
    "    log_q_eta =  Normal(q_mean[0], q_sigma[0]).log_prob(mus_prev).sum(-1).sum(-1) + Gamma(q_alpha, q_beta).log_prob(precisions_prev).sum(-1).sum(-1)## B\n",
    "    return log_q_eta\n",
    "\n",
    "def test(x, Pi, N, K, D, num_samples, steps, batch_size):\n",
    "    log_increment_weights = torch.zeros((steps, num_samples, batch_size))\n",
    "    log_p_joints = torch.zeros((steps, num_samples, batch_size))\n",
    "    log_qf = torch.zeros((steps-1, num_samples, batch_size))\n",
    "    log_qr = torch.zeros((steps-1, num_samples, batch_size))\n",
    "    Z_samples = torch.zeros((num_samples, batch_size, N, K))\n",
    "    mus_prevs = torch.zeros((num_samples, batch_size, K, D))\n",
    "    precisions_prevs = torch.zeros((num_samples, batch_size, K, D))\n",
    "    \n",
    "    for m in range(steps):\n",
    "        if m == 0:\n",
    "            for l in range(num_samples):\n",
    "                q_mean, q_nu, q_alpha, q_beta, q_sigma, mus, precisions, log_q_eta = Init_step(x, N, D, K, batch_size)\n",
    "                mus_prevs[l] = mus\n",
    "                precisions_prevs[l] = precisions\n",
    "                zs_pi, z, log_q_z = E_step(x, mus, precisions, N, D, K, batch_size)\n",
    "                Z_samples[l] = z\n",
    "                labels = z.nonzero()\n",
    "                log_p_z = cat(Pi).log_prob(z).sum(-1)\n",
    "                sigmas = 1. / torch.sqrt(precisions)\n",
    "                log_p_x = Normal(mus[labels[:, 0], labels[:, -1], :].view(batch_size, N, D), sigmas[labels[:, 0], labels[:, -1], :].view(batch_size, N, D)).log_prob(x).sum(-1).sum(-1)\n",
    "                log_increment_weights[m, l] = log_p_x + log_p_z - log_q_z     \n",
    "                log_p_joints[m, l] = log_joints_gmm(x, z, Pi, mus, precisions, N, D, K, prior_mean, prior_nu, prior_alpha, prior_beta, batch_size)\n",
    "                \n",
    "        else:\n",
    "            for l in range(num_samples):\n",
    "                z_prev = Z_samples[l]\n",
    "                q_mean, q_nu, q_alpha, q_beta, q_sigma, mus, precisions, log_q_eta = M_step(x, z_prev, N, D, K, batch_size)\n",
    "                zs_pi, z, log_q_z = E_step(x, mus, precisions, N, D, K, batch_size)\n",
    "                Z_samples[l] = z\n",
    "                log_p_joints[m, l] = log_joints_gmm(x, z, Pi, mus, precisions, N, D, K, prior_mean, prior_nu, prior_alpha, prior_beta, batch_size)\n",
    "                log_qf[m-1, l] = log_q_eta + log_q_z\n",
    "                \n",
    "                mus_prev = mus_prevs[l]\n",
    "                precisions_prev = precisions_prevs[l]\n",
    "                \n",
    "                log_qr[m-1, l] = reverse(x, z, mus_prev, precisions_prev, N, D, K, batch_size)\n",
    "                log_p_joint = log_joints_gmm(x, z, Pi, mus, precisions, N, D, K, prior_mean, prior_nu, prior_alpha, prior_beta, batch_size)\n",
    "                log_increment_weights[m, l] = log_p_joint - log_q_z - log_q_eta\n",
    "                mus_prevs[l] = mus\n",
    "                precisions_prevs[l] = precisions\n",
    "    detail_balances = log_p_joints[1:] - log_p_joints[:-1] -log_qf + log_qr\n",
    "    increment_weights = torch.exp(log_increment_weights - logsumexp(log_increment_weights, 1).unsqueeze(1).repeat(1, num_samples, 1)).detach()\n",
    "    esses = (1./ (increment_weights ** 2).sum(1))                   \n",
    "    log_last_weights = log_increment_weights[-1] ## S * B\n",
    "    ## EUBO and ELBO\n",
    "    eubos = torch.mul(increment_weights, log_increment_weights).sum(1).mean(-1)\n",
    "    elbos = log_increment_weights.mean(1).mean(-1)     \n",
    "    return eubos, elbos, esses, log_increment_weights, detail_balances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_single_batch(num_seqs, N, K, D, batch_size):\n",
    "    indices = torch.randperm(num_seqs)\n",
    "    batch_indices = indices[0*batch_size : (0+1)*batch_size]\n",
    "    batch_Xs = Xs[batch_indices]\n",
    "    batch_Xs = shuffler(batch_Xs, N, K, D, batch_size)\n",
    "    return batch_Xs\n",
    "batch_Xs = sample_single_batch(num_seqs, N, K, D, BATCH_SIZE)\n",
    "eubo, elbo, ess, log_increment_weights, detail_balances = test(batch_Xs, Pi, N, K, D, NUM_SAMPLES, STEPS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detail_balances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(BATCH_SIZE):\n",
    "    log_weights = log_increment_weights[:, :, i]\n",
    "    ess_stepwise = ess[:, i].data.numpy()\n",
    "    fig = plt.figure(figsize=(20,6))\n",
    "    ax1 = fig.add_subplot(1,3,1)\n",
    "    ax2 = fig.add_subplot(1,3,2)\n",
    "    ax3 = fig.add_subplot(1,3,3)\n",
    "    weights = torch.exp(log_weights - logsumexp(log_weights, -1)[:, None]).data.numpy()\n",
    "    db = detail_balances[:, :, i].mean(-1)\n",
    "    ax1.plot(db.data.numpy(), 'r-o')\n",
    "    ax1.set_ylim([-20, 20])\n",
    "    for s in range(NUM_SAMPLES):\n",
    "        ax2.plot(ess_stepwise, 'b-o')\n",
    "        ax3.plot(weights[:, s], 'g-o')\n",
    "        ax2.set_ylim([1.0, 7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test(x, num_seqs, Pi, N, K, D, steps, batch_size):\n",
    "    LLs = [] \n",
    "    for m in range(steps):\n",
    "        if m == 0:\n",
    "            mus, precisions, log_p_eta = inti_global(K, D, prior_mean, prior_nu, prior_alpha, prior_beta, batch_size)\n",
    "            zs_pi, z, log_q_z = E_step(x, mus, precisions, N, D, K, batch_size)\n",
    "        else:\n",
    "            q_mean, q_nu, q_alpha, q_beta, q_sigma, mus, precisions, log_q_eta = M_step(x, z, N, D, K, batch_size)\n",
    "            zs_pi, z, log_q_z = E_step(x, mus, precisions, N, D, K, batch_size)\n",
    "            labels = z.nonzero()\n",
    "            sigmas = 1. / torch.sqrt(precisions)\n",
    "            ll = Normal(mus[labels[:, 0], labels[:, -1], :].view(batch_size, N, D), sigmas[labels[:, 0], labels[:, -1], :].view(batch_size, N, D)).log_prob(x).sum(-1).sum(-1).mean()\n",
    "            LLs.append(ll.item())\n",
    "    E_precisions = q_alpha / q_beta\n",
    "    E_mus = q_mean\n",
    "    E_z = torch.argmax(zs_pi, dim=-1)\n",
    "\n",
    "    return z, mus, precisions, LLs, E_mus, E_precisions, E_z\n",
    "\n",
    "x,z_true = sample_single_batch(num_seqs, N, K, D, BATCH_SIZE)\n",
    "z, mus, precisions, LLs, E_mus, E_precisions, E_z = test(x, num_seqs, Pi, N, K, D, STEPS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_final_samples(Xs, Zs, mus, precisions, steps, batch_size):\n",
    "    colors = ['r', 'b', 'gold']\n",
    "    fig = plt.figure(figsize=(25,100))\n",
    "    for b in range(batch_size):\n",
    "        ax = fig.add_subplot(int(batch_size / 5), 5, b+1)\n",
    "        x = Xs[b].data.numpy()\n",
    "        z = Zs[b].data.numpy()\n",
    "        mu = mus[b].data.numpy()\n",
    "        precision = precisions[b].data.numpy()\n",
    "\n",
    "        covs = np.zeros((K, D, D))\n",
    "        assignments = z\n",
    "        for k in range(K):\n",
    "            covs[k] = np.diag(1. / precision[k])\n",
    "            xk = x[np.where(assignments == k)]\n",
    "            ax.scatter(xk[:, 0], xk[:, 1], c=colors[k])\n",
    "            plot_cov_ellipse(cov=covs[k], pos=mu[k], nstd=2, ax=ax, alpha=0.2, color=colors[k])\n",
    "        ax.set_ylim([-10, 10])\n",
    "        ax.set_xlim([-10, 10])\n",
    "    plt.savefig('results/modes' + PATH + '.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_final_samples(x, E_z, E_mus, E_precisions, STEPS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LLs[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LLs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(LLs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
