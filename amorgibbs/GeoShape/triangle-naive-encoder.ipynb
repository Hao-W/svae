{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from plots import *\n",
    "from kls import *\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.one_hot_categorical import OneHotCategorical as cat\n",
    "from torch.distributions.beta import Beta\n",
    "from torch.distributions.uniform import Uniform\n",
    "from torch import logsumexp\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 150\n",
    "D = 2\n",
    "## Model Parameters\n",
    "NUM_SAMPLES = 10\n",
    "NUM_HIDDEN = 64\n",
    "NUM_LATENTS = 1\n",
    "NUM_OBS = D\n",
    "BATCH_SIZE = 20\n",
    "NUM_EPOCHS = 6000\n",
    "LEARNING_RATE = 1e-3\n",
    "CUDA = False\n",
    "PATH = 'circles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = torch.from_numpy(np.load('triangles/obs.npy')).float()\n",
    "Xs = Xs.transpose(1,0)\n",
    "# mus_true = torch.from_numpy(np.load('circles/mus.npy')).float()\n",
    "# rads_true = torch.from_numpy(np.load('circles/rads.npy')).float()\n",
    "# num_seqs = Xs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_obs= D,\n",
    "                       num_hidden=NUM_HIDDEN,\n",
    "                       num_latents=NUM_LATENTS):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.enc_hidden = nn.Sequential(\n",
    "            nn.Linear(num_obs, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_hidden),\n",
    "            nn.ReLU())\n",
    "        self.local_log_alpha = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        self.local_log_beta = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        \n",
    "    def forward(self, obs, N, D, num_samples):\n",
    "        hidden = self.enc_hidden(obs)\n",
    "        q_alpha = torch.exp(self.local_log_alpha(hidden))\n",
    "        q_beta = torch.exp(self.local_log_beta(hidden))\n",
    "        embed = Beta(q_alpha, q_beta).sample((num_samples,)) ## S * N\n",
    "        log_q = Beta(q_alpha, q_beta).log_prob(embed)\n",
    "        return q_alpha, q_beta, embed, log_q\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_obs=NUM_OBS,\n",
    "                       num_hidden=NUM_HIDDEN,\n",
    "                       num_latents=NUM_LATENTS):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dec_angles = nn.Sequential(\n",
    "            nn.Linear(num_latents, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_obs))\n",
    "        self.dec_rad = nn.Sequential(\n",
    "            nn.Linear(num_latents, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, 1))\n",
    "        self.x_sigma = torch.ones((N, D)) * 0.01\n",
    "    def forward(self, embed, obs, N, D):\n",
    "#         angles = self.dec_angles(embed) \n",
    "        rads = self.dec_rad(embed)\n",
    "        xaxis = torch.cos(embed*2*math.pi) \n",
    "        yaxis = torch.sin(embed*2*math.pi)\n",
    "        angles = torch.cat((xaxis, yaxis), -1)\n",
    "        x_mus = angles * rads\n",
    "        log_p_x = Normal(x_mus, self.x_sigma).log_prob(obs).sum(-1)\n",
    "        return x_mus, log_p_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 1e-2)     \n",
    "        \n",
    "def initialize():\n",
    "    enc = Encoder()\n",
    "    dec = Decoder()\n",
    "#     enc.apply(weights_init)\n",
    "    opt1 =  torch.optim.Adam(list(enc.parameters()),lr=LEARNING_RATE, betas=(0.9, 0.99)) \n",
    "    opt2 = torch.optim.Adam(list(dec.parameters()),lr=LEARNING_RATE, betas=(0.9, 0.99))\n",
    "    return enc, dec, opt1, opt2\n",
    "enc, dec, opt1, opt2 = initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def reparam(q_mus, q_sigma, N, D, num_samples, batch_size):\n",
    "#     eps = Normal(torch.zeros((num_samples, batch_size, N, D)), torch.ones((num_samples, batch_size, N, D))).sample()\n",
    "#     ws = q_mus.unsqueeze(0).repeat(num_samples, 1, 1, 1) + torch.mul(q_sigma.unsqueeze(0).repeat(num_samples, 1, 1, 1), eps)\n",
    "#     log_qs = Normal(q_mus, q_sigma).log_prob(ws).sum(-1).sum(-1) ## S * B\n",
    "#     return ws, log_qs\n",
    "\n",
    "def oneshot(x, N, D, num_samples):\n",
    "    log_p_xs = torch.zeros((num_samples, N))\n",
    "    log_prs = torch.zeros((num_samples, N))\n",
    "    X_mus = torch.zeros((num_samples, N, D))\n",
    "    q_alpha, q_alpha, embed, log_q = enc(x, N, D, num_samples)\n",
    "#     angles = embed * 2*math.pi\n",
    "    angles = embed\n",
    "#     yaxis = torch.cos(angles) * 2.0\n",
    "#     xaxis = torch.sin(angles) * 2.0\n",
    "#     x_mus = torch.cat((xaxis, yaxis), dim=-1)\n",
    "#     x_sigma = torch.ones((N, D)) * 0.01\n",
    "    for s in range(num_samples):\n",
    "        x_mus, log_p_x = dec(angles[s], x, N, D)\n",
    "        X_mus[s] = x_mus\n",
    "        log_p_xs[s] = log_p_x\n",
    "        log_prs[s] = Uniform(torch.zeros(N), torch.ones(N)).log_prob(angles[s].squeeze(-1)) # S * N \n",
    "#     log_p_x = Normal(x_mus, x_sigma).log_prob(x).sum(-1) ## S * N\n",
    "#     log_pr = Uniform(torch.zeros(N), torch.ones(N)*2*math.pi).log_prob(embed.squeeze(-1)) # S * N \n",
    "    log_weights = (log_prs + log_p_xs - log_q.squeeze(-1))\n",
    "    weights = torch.exp(log_weights - logsumexp(log_weights, 0)).detach()\n",
    "    eubo = torch.mul(weights, log_weights).sum(0).mean()\n",
    "    elbo = log_weights.mean(0).mean()\n",
    "\n",
    "    return eubo, elbo, X_mus, embed\n",
    "\n",
    "def shuffler(batch_Xs, N, D, batch_size):\n",
    "    indices = torch.cat([torch.randperm(N).unsqueeze(0) for b in range(batch_size)])\n",
    "    indices_Xs = indices.unsqueeze(-1).repeat(1, 1, D)\n",
    "    return torch.gather(batch_Xs, 1, indices_Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, EUBO=-11881.228, ELBO=-17261.176 (0s)\n",
      "epoch=100, EUBO=-1944.137, ELBO=-3280.202 (1s)\n",
      "epoch=200, EUBO=-1546.848, ELBO=-2010.969 (1s)\n",
      "epoch=300, EUBO=-1276.764, ELBO=-1471.840 (1s)\n",
      "epoch=400, EUBO=-983.736, ELBO=-1152.461 (1s)\n",
      "epoch=500, EUBO=-750.193, ELBO=-881.276 (1s)\n",
      "epoch=600, EUBO=-614.751, ELBO=-739.449 (1s)\n",
      "epoch=700, EUBO=-538.876, ELBO=-688.547 (1s)\n",
      "epoch=800, EUBO=-508.602, ELBO=-669.781 (1s)\n",
      "epoch=900, EUBO=-486.240, ELBO=-613.390 (1s)\n",
      "epoch=1000, EUBO=-470.873, ELBO=-553.652 (1s)\n",
      "epoch=1100, EUBO=-435.325, ELBO=-495.731 (1s)\n",
      "epoch=1200, EUBO=-416.844, ELBO=-464.527 (1s)\n",
      "epoch=1300, EUBO=-395.769, ELBO=-440.053 (1s)\n",
      "epoch=1400, EUBO=-385.096, ELBO=-418.502 (1s)\n",
      "epoch=1500, EUBO=-404.066, ELBO=-475.590 (1s)\n",
      "epoch=1600, EUBO=-368.905, ELBO=-407.166 (1s)\n",
      "epoch=1700, EUBO=-362.647, ELBO=-390.310 (1s)\n",
      "epoch=1800, EUBO=-359.777, ELBO=-393.893 (1s)\n",
      "epoch=1900, EUBO=-352.624, ELBO=-389.652 (1s)\n",
      "epoch=2000, EUBO=-349.291, ELBO=-381.329 (1s)\n",
      "epoch=2100, EUBO=-345.691, ELBO=-380.109 (1s)\n",
      "epoch=2200, EUBO=-339.023, ELBO=-366.694 (1s)\n",
      "epoch=2300, EUBO=-374.045, ELBO=-406.325 (1s)\n",
      "epoch=2400, EUBO=-332.892, ELBO=-358.004 (1s)\n",
      "epoch=2500, EUBO=-330.232, ELBO=-351.630 (1s)\n",
      "epoch=2600, EUBO=-328.274, ELBO=-354.009 (1s)\n",
      "epoch=2700, EUBO=-326.539, ELBO=-348.862 (1s)\n",
      "epoch=2800, EUBO=-324.641, ELBO=-344.307 (1s)\n",
      "epoch=2900, EUBO=-322.987, ELBO=-341.153 (1s)\n",
      "epoch=3000, EUBO=-322.218, ELBO=-341.752 (1s)\n",
      "epoch=3100, EUBO=-321.943, ELBO=-342.629 (1s)\n",
      "epoch=3200, EUBO=-320.789, ELBO=-338.760 (1s)\n",
      "epoch=3300, EUBO=-319.567, ELBO=-338.631 (1s)\n",
      "epoch=3400, EUBO=-341.640, ELBO=-397.948 (1s)\n",
      "epoch=3500, EUBO=-320.199, ELBO=-342.288 (1s)\n",
      "epoch=3600, EUBO=-317.688, ELBO=-340.850 (1s)\n",
      "epoch=3700, EUBO=-320.440, ELBO=-357.720 (1s)\n",
      "epoch=3800, EUBO=-316.651, ELBO=-339.446 (1s)\n",
      "epoch=3900, EUBO=-316.630, ELBO=-345.912 (1s)\n",
      "epoch=4000, EUBO=-318.777, ELBO=-348.590 (1s)\n",
      "epoch=4100, EUBO=-315.725, ELBO=-337.610 (1s)\n",
      "epoch=4200, EUBO=-315.151, ELBO=-333.105 (1s)\n",
      "epoch=4300, EUBO=-316.480, ELBO=-337.571 (1s)\n",
      "epoch=4400, EUBO=-315.344, ELBO=-336.651 (1s)\n",
      "epoch=4500, EUBO=-314.053, ELBO=-331.478 (1s)\n",
      "epoch=4600, EUBO=-315.202, ELBO=-343.623 (1s)\n",
      "epoch=4700, EUBO=-313.626, ELBO=-332.507 (1s)\n",
      "epoch=4800, EUBO=-313.293, ELBO=-332.771 (1s)\n",
      "epoch=4900, EUBO=-313.475, ELBO=-333.058 (1s)\n",
      "epoch=5000, EUBO=-312.430, ELBO=-327.581 (1s)\n",
      "epoch=5100, EUBO=-312.656, ELBO=-331.631 (1s)\n",
      "epoch=5200, EUBO=-312.625, ELBO=-334.171 (1s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-89e764954ed4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mopt1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mopt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0meubo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melbo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_mus\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moneshot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXs_shuffle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_SAMPLES\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0melbo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mopt2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-8b6f68f60df0>\u001b[0m in \u001b[0;36moneshot\u001b[0;34m(x, N, D, num_samples)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m#     log_p_x = Normal(x_mus, x_sigma).log_prob(x).sum(-1) ## S * N\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#     log_pr = Uniform(torch.zeros(N), torch.ones(N)*2*math.pi).log_prob(embed.squeeze(-1)) # S * N\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mlog_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlog_prs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlog_p_xs\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlog_q\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m     \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_weights\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlogsumexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0meubo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "ELBOs = []\n",
    "EUBOs = []\n",
    "num_batches = int((Xs.shape[0] / BATCH_SIZE))\n",
    "time_start = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    indices = torch.randperm(N)\n",
    "    Xs_shuffle = Xs[indices]\n",
    "    opt1.zero_grad()\n",
    "    eubo, elbo, X_mus, embed = oneshot(Xs_shuffle, N, D, NUM_SAMPLES)\n",
    "    eubo.backward()\n",
    "    opt1.step()\n",
    "    opt2.zero_grad()\n",
    "    eubo, elbo, X_mus, embed = oneshot(Xs_shuffle, N, D, NUM_SAMPLES)\n",
    "    (-elbo).backward()\n",
    "    opt2.step()\n",
    "    \n",
    "    ELBOs.append(elbo.item())\n",
    "    EUBOs.append(eubo.item())\n",
    "    \n",
    "    if epoch%100 == 0:\n",
    "        time_end = time.time()  \n",
    "        print('epoch=%d, EUBO=%.3f, ELBO=%.3f (%ds)' % (epoch, eubo, elbo, time_end - time_start))\n",
    "        time_start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for s in range(NUM_SAMPLES):\n",
    "#     fig = plt.figure(figsize=(12,5))\n",
    "#     ax1 = fig.add_subplot(1,2,1)\n",
    "#     ax2 = fig.add_subplot(1,2,2)   \n",
    "#     ax1.scatter(X_mus[s, :, 0].data.numpy(), X_mus[s, :, 1].data.numpy())\n",
    "#     ax2.scatter(Xs_shuffle[:, 0].data.numpy(), Xs_shuffle[:, 1].data.numpy())\n",
    "#     ax1.set_xlim([-3,3])\n",
    "#     ax1.set_ylim([-3,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_samples = Uniform(torch.zeros(1), torch.ones(1)).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-3, 3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = torch.linspace(0, 1.0, 100)[:, None]\n",
    "b = torch.randn((100, 2))\n",
    "rads = dec.dec_rad(a)\n",
    "xaxis = torch.cos(a*2*math.pi) \n",
    "yaxis = torch.sin(a*2*math.pi)\n",
    "angles = torch.cat((xaxis, yaxis), -1)\n",
    "x_mus = angles * rads\n",
    "\n",
    "fig = plt.figure(figsize=(12,5))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "ax1.scatter(x_mus[:, 0].data.numpy(), x_mus[:, 1].data.numpy())\n",
    "ax1.set_xlim([-3,3])\n",
    "ax1.set_ylim([-3,3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(EUBOs, ELBOs, ESSs, KLs_eta_ex, KLs_eta_in, KL_z_ex, KL_z_in):\n",
    "    fout = open('results/logs-' + PATH +'.txt', 'w+')\n",
    "    fout.write('EUBOs, ELBOs, ESSs, KLs_eta_ex, KLs_eta_in, KL_z_ex, KL_z_in\\n')\n",
    "    for i in range(len(EUBOs)):\n",
    "        fout.write(str(EUBOs[i]) + ', ' + str(ELBOs[i]) + ', ' + str(ESSs[i]) \n",
    "                   + str(KLs_eta_ex[i]) + str(KLs_eta_in[i]) + str(KLs_z_ex[i]) + str(KLs_z_in[i]) + '\\n')\n",
    "    fout.close()\n",
    "save_results(EUBOs, ELBOs, ESSs, KLs_eta_ex, KLs_eta_in, KLs_z_ex, KLs_z_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(EUBOs, ELBOs, ESSs, KLs_eta_ex, KLs_eta_in, KLs_z_ex, KLs_z_in, num_samples):\n",
    "    fig = plt.figure(figsize=(20, 30))\n",
    "    ax1 = fig.add_subplot(3, 1, 1)\n",
    "    ax1.plot(EUBOs, 'r', label='EUBOs')\n",
    "    ax1.plot(ELBOs, 'b', label='ELBOs')\n",
    "    ax1.tick_params(labelsize=18)\n",
    "    ax1.set_ylim([-220, -130])\n",
    "    ax1.legend(fontsize=18)\n",
    "    ##\n",
    "    ax2 = fig.add_subplot(3, 1, 2)\n",
    "    ax2.plot(KLs_eta_ex, '#66b3ff', label='KLs_eta_ex')\n",
    "    ax2.plot(KLs_eta_in, '#ff9999', label='KLs_eta_in')\n",
    "    ax2.plot(KLs_z_ex, '#99ff99', label='KLs_z_ex')\n",
    "    ax2.plot(KLs_z_in, 'gold', label='KLs_z_in')   \n",
    "    ax2.plot(np.ones(len(KLs_z_in)) * 5, 'k', label='const=5.0')\n",
    "    ax2.legend(fontsize=18)\n",
    "    ax2.tick_params(labelsize=18)\n",
    "    ax2.set_ylim([-1, 30])\n",
    "    ##\n",
    "    ax3 = fig.add_subplot(3, 1, 3)\n",
    "    ax3.plot(np.array(ESSs) / num_samples, 'm', label='ESS')\n",
    "    ax3.tick_params(labelsize=18)\n",
    "    ax3.set_xlabel('epochs (%d gradient steps per epoch)'  % num_batches, size=18)\n",
    "    ax3.legend()\n",
    "    plt.savefig('results/train-' + PATH + '.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(EUBOs, ELBOs, ESSs, KLs_eta_ex, KLs_eta_in, KLs_z_ex, KLs_z_in, NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS = 50\n",
    "def reverse(X, z, mus_prev, precisions_prev, N, D, K, batch_size):\n",
    "    data = torch.cat((X, z), dim=-1).view(batch_size*N, -1)\n",
    "    q_mean, q_nu, q_alpha, q_beta, q_sigma, mus, precisions = enc_global(data, K, D, 1, batch_size)  \n",
    "    log_q_eta =  Normal(q_mean[0], q_sigma[0]).log_prob(mus_prev).sum(-1).sum(-1) + Gamma(q_alpha, q_beta).log_prob(precisions_prev).sum(-1).sum(-1)## B\n",
    "    return log_q_eta\n",
    "\n",
    "def test(x, Pi, N, K, D, num_samples, steps, batch_size):\n",
    "    log_increment_weights = torch.zeros((steps, num_samples, batch_size))\n",
    "    log_p_joints = torch.zeros((steps, num_samples, batch_size))\n",
    "    log_qf = torch.zeros((steps-1, num_samples, batch_size))\n",
    "    log_qr = torch.zeros((steps-1, num_samples, batch_size))\n",
    "    Z_samples = torch.zeros((num_samples, batch_size, N, K))\n",
    "    mus_prevs = torch.zeros((num_samples, batch_size, K, D))\n",
    "    precisions_prevs = torch.zeros((num_samples, batch_size, K, D))\n",
    "    \n",
    "    for m in range(steps):\n",
    "        if m == 0:\n",
    "            for l in range(num_samples):\n",
    "                q_mean, q_nu, q_alpha, q_beta, q_sigma, mus, precisions, log_q_eta = Init_step(x, N, D, K, batch_size)\n",
    "                mus_prevs[l] = mus\n",
    "                precisions_prevs[l] = precisions\n",
    "                zs_pi, z, log_q_z = E_step(x, mus, precisions, N, D, K, batch_size)\n",
    "                Z_samples[l] = z\n",
    "                labels = z.nonzero()\n",
    "                log_p_z = cat(Pi).log_prob(z).sum(-1)\n",
    "                sigmas = 1. / torch.sqrt(precisions)\n",
    "                log_p_x = Normal(mus[labels[:, 0], labels[:, -1], :].view(batch_size, N, D), sigmas[labels[:, 0], labels[:, -1], :].view(batch_size, N, D)).log_prob(x).sum(-1).sum(-1)\n",
    "                log_increment_weights[m, l] = log_p_x + log_p_z - log_q_z     \n",
    "                log_p_joints[m, l] = log_joints_gmm(x, z, Pi, mus, precisions, N, D, K, prior_mean, prior_nu, prior_alpha, prior_beta, batch_size)\n",
    "                \n",
    "        else:\n",
    "            for l in range(num_samples):\n",
    "                z_prev = Z_samples[l]\n",
    "                q_mean, q_nu, q_alpha, q_beta, q_sigma, mus, precisions, log_q_eta = M_step(x, z_prev, N, D, K, batch_size)\n",
    "                zs_pi, z, log_q_z = E_step(x, mus, precisions, N, D, K, batch_size)\n",
    "                Z_samples[l] = z\n",
    "                log_p_joints[m, l] = log_joints_gmm(x, z, Pi, mus, precisions, N, D, K, prior_mean, prior_nu, prior_alpha, prior_beta, batch_size)\n",
    "                log_qf[m-1, l] = log_q_eta + log_q_z\n",
    "                \n",
    "                mus_prev = mus_prevs[l]\n",
    "                precisions_prev = precisions_prevs[l]\n",
    "                \n",
    "                log_qr[m-1, l] = reverse(x, z, mus_prev, precisions_prev, N, D, K, batch_size)\n",
    "                log_p_joint = log_joints_gmm(x, z, Pi, mus, precisions, N, D, K, prior_mean, prior_nu, prior_alpha, prior_beta, batch_size)\n",
    "                log_increment_weights[m, l] = log_p_joint - log_q_z - log_q_eta\n",
    "                mus_prevs[l] = mus\n",
    "                precisions_prevs[l] = precisions\n",
    "    detail_balances = log_p_joints[1:] - log_p_joints[:-1] -log_qf + log_qr\n",
    "    increment_weights = torch.exp(log_increment_weights - logsumexp(log_increment_weights, 1).unsqueeze(1).repeat(1, num_samples, 1)).detach()\n",
    "    esses = (1./ (increment_weights ** 2).sum(1))                   \n",
    "    log_last_weights = log_increment_weights[-1] ## S * B\n",
    "    ## EUBO and ELBO\n",
    "    eubos = torch.mul(increment_weights, log_increment_weights).sum(1).mean(-1)\n",
    "    elbos = log_increment_weights.mean(1).mean(-1)     \n",
    "    return eubos, elbos, esses, log_increment_weights, detail_balances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_single_batch(num_seqs, N, K, D, batch_size):\n",
    "    indices = torch.randperm(num_seqs)\n",
    "    batch_indices = indices[0*batch_size : (0+1)*batch_size]\n",
    "    batch_Xs = Xs[batch_indices]\n",
    "    batch_Xs = shuffler(batch_Xs, N, K, D, batch_size)\n",
    "    return batch_Xs\n",
    "batch_Xs = sample_single_batch(num_seqs, N, K, D, BATCH_SIZE)\n",
    "eubo, elbo, ess, log_increment_weights, detail_balances = test(batch_Xs, Pi, N, K, D, NUM_SAMPLES, STEPS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detail_balances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(BATCH_SIZE):\n",
    "    log_weights = log_increment_weights[:, :, i]\n",
    "    ess_stepwise = ess[:, i].data.numpy()\n",
    "    fig = plt.figure(figsize=(20,6))\n",
    "    ax1 = fig.add_subplot(1,3,1)\n",
    "    ax2 = fig.add_subplot(1,3,2)\n",
    "    ax3 = fig.add_subplot(1,3,3)\n",
    "    weights = torch.exp(log_weights - logsumexp(log_weights, -1)[:, None]).data.numpy()\n",
    "    db = detail_balances[:, :, i].mean(-1)\n",
    "    ax1.plot(db.data.numpy(), 'r-o')\n",
    "    ax1.set_ylim([-20, 20])\n",
    "    for s in range(NUM_SAMPLES):\n",
    "        ax2.plot(ess_stepwise, 'b-o')\n",
    "        ax3.plot(weights[:, s], 'g-o')\n",
    "        ax2.set_ylim([1.0, 7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test(x, num_seqs, Pi, N, K, D, steps, batch_size):\n",
    "    LLs = [] \n",
    "    for m in range(steps):\n",
    "        if m == 0:\n",
    "            mus, precisions, log_p_eta = inti_global(K, D, prior_mean, prior_nu, prior_alpha, prior_beta, batch_size)\n",
    "            zs_pi, z, log_q_z = E_step(x, mus, precisions, N, D, K, batch_size)\n",
    "        else:\n",
    "            q_mean, q_nu, q_alpha, q_beta, q_sigma, mus, precisions, log_q_eta = M_step(x, z, N, D, K, batch_size)\n",
    "            zs_pi, z, log_q_z = E_step(x, mus, precisions, N, D, K, batch_size)\n",
    "            labels = z.nonzero()\n",
    "            sigmas = 1. / torch.sqrt(precisions)\n",
    "            ll = Normal(mus[labels[:, 0], labels[:, -1], :].view(batch_size, N, D), sigmas[labels[:, 0], labels[:, -1], :].view(batch_size, N, D)).log_prob(x).sum(-1).sum(-1).mean()\n",
    "            LLs.append(ll.item())\n",
    "    E_precisions = q_alpha / q_beta\n",
    "    E_mus = q_mean\n",
    "    E_z = torch.argmax(zs_pi, dim=-1)\n",
    "\n",
    "    return z, mus, precisions, LLs, E_mus, E_precisions, E_z\n",
    "\n",
    "x,z_true = sample_single_batch(num_seqs, N, K, D, BATCH_SIZE)\n",
    "z, mus, precisions, LLs, E_mus, E_precisions, E_z = test(x, num_seqs, Pi, N, K, D, STEPS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_final_samples(Xs, Zs, mus, precisions, steps, batch_size):\n",
    "    colors = ['r', 'b', 'gold']\n",
    "    fig = plt.figure(figsize=(25,100))\n",
    "    for b in range(batch_size):\n",
    "        ax = fig.add_subplot(int(batch_size / 5), 5, b+1)\n",
    "        x = Xs[b].data.numpy()\n",
    "        z = Zs[b].data.numpy()\n",
    "        mu = mus[b].data.numpy()\n",
    "        precision = precisions[b].data.numpy()\n",
    "\n",
    "        covs = np.zeros((K, D, D))\n",
    "        assignments = z\n",
    "        for k in range(K):\n",
    "            covs[k] = np.diag(1. / precision[k])\n",
    "            xk = x[np.where(assignments == k)]\n",
    "            ax.scatter(xk[:, 0], xk[:, 1], c=colors[k])\n",
    "            plot_cov_ellipse(cov=covs[k], pos=mu[k], nstd=2, ax=ax, alpha=0.2, color=colors[k])\n",
    "        ax.set_ylim([-10, 10])\n",
    "        ax.set_xlim([-10, 10])\n",
    "    plt.savefig('results/modes' + PATH + '.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_final_samples(x, E_z, E_mus, E_precisions, STEPS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LLs[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LLs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(LLs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
