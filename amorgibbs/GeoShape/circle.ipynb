{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from plots import *\n",
    "from kls import *\n",
    "from torch._six import inf\n",
    "from torch.distributions.normal import Normal\n",
    "from torch.distributions.multivariate_normal import MultivariateNormal as mvn\n",
    "from torch.distributions.one_hot_categorical import OneHotCategorical as cat\n",
    "from torch.distributions.gamma import Gamma\n",
    "from torch import logsumexp\n",
    "import sys\n",
    "import time\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "D = 2\n",
    "## Model Parameters\n",
    "NUM_SAMPLES = 10\n",
    "NUM_HIDDEN = 64\n",
    "NUM_LATENTS = 2\n",
    "NUM_OBS = D\n",
    "BATCH_SIZE = 20\n",
    "NUM_EPOCHS = 1000\n",
    "LEARNING_RATE = 1e-3\n",
    "CUDA = False\n",
    "PATH = 'circles'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = torch.from_numpy(np.load('circles/obs.npy')).float()\n",
    "Xs = Xs.transpose(1,2)\n",
    "mus_true = torch.from_numpy(np.load('circles/mus.npy')).float()\n",
    "rads_true = torch.from_numpy(np.load('circles/rads.npy')).float()\n",
    "num_seqs = Xs.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, num_obs=D+D,\n",
    "#                        num_stats=NUM_STATS,\n",
    "#                        num_hidden=NUM_HIDDEN,\n",
    "#                        num_latents=NUM_LATENTS):\n",
    "#         super(self.__class__, self).__init__()\n",
    "#         self.enc_stats = nn.Sequential(\n",
    "#             nn.Linear(num_obs, num_hidden),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(num_hidden, num_stats))\n",
    "#         self.enc_hidden = nn.Sequential(\n",
    "#             nn.Linear(num_stats, num_hidden),\n",
    "#             nn.ReLU())\n",
    "#         self.mus_mean = nn.Sequential(\n",
    "#             nn.Linear(num_hidden, num_latents))\n",
    "#         self.mus_log_sigma = nn.Sequential(\n",
    "#             nn.Linear(num_hidden, num_latents))\n",
    "        \n",
    "#     def forward(self, obs, K, D, num_samples, batch_size):\n",
    "#         stats = self.enc_stats(obs).view(batch_size, N, -1).sum(1)\n",
    "#         hidden = self.enc_hidden(stats)\n",
    "#         q_mean = self.mus_mean(hidden).view(-1, K, D)\n",
    "#         q_sigma = torch.exp(self.mus_log_sigma(hidden2).view(-1, K, D))\n",
    "#         mus = Normal(q_mean, q_sigma).sample((num_samples,))  \n",
    "#         return q_mean, q_sigma, mus\n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_obs=D+D,\n",
    "                       num_hidden=NUM_HIDDEN,\n",
    "                       num_latents=NUM_LATENTS):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.enc_hidden = nn.Sequential(\n",
    "            nn.Linear(num_obs, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_hidden),\n",
    "            nn.ReLU())\n",
    "        self.local_mean = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        self.local_log_sigma = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        \n",
    "    def forward(self, obs, N, D, batch_size):\n",
    "        hidden = self.enc_hidden(obs)\n",
    "        q_mean = self.local_mean(hidden).view(batch_size, N, D)\n",
    "        q_sigma = torch.exp(self.local_log_sigma(hidden).view(batch_size, N, D))\n",
    "        return q_mean, q_sigma\n",
    "    \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, num_obs=NUM_OBS,\n",
    "                       num_hidden=NUM_HIDDEN,\n",
    "                       num_latents=NUM_LATENTS+D):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.dec_hidden = nn.Sequential(\n",
    "            nn.Linear(num_latents, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_hidden),\n",
    "            nn.ReLU())\n",
    "        self.obs_mean = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_obs))\n",
    "        self.obs_log_std = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_obs))\n",
    "\n",
    "    def forward(self, embed, obs, N, D, batch_size):\n",
    "        hidden = self.dec_hidden(embed)           \n",
    "        p_mean = self.obs_mean(hidden).view(batch_size, N, D)\n",
    "        p_sigma = torch.exp(self.obs_log_std(hidden)).view(batch_size, N, D)\n",
    "        ll_recon = Normal(p_mean, p_sigma).log_prob(obs).sum(-1).sum(-1)  \n",
    "        return p_mean, p_sigma, ll_recon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Linear') != -1:\n",
    "        m.weight.data.normal_(0.0, 1e-2)     \n",
    "        \n",
    "def initialize():\n",
    "    enc = Encoder()\n",
    "    dec = Decoder()\n",
    "    optimizer =  torch.optim.Adam(list(enc.parameters()) + list(dec.parameters()),lr=LEARNING_RATE, betas=(0.9, 0.99))    \n",
    "    return enc, dec, optimizer\n",
    "enc, dec, optimizer = initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "prior_mean = torch.zeros((BATCH_SIZE, N, D))\n",
    "prior_sigma = torch.ones((BATCH_SIZE, N, D))\n",
    "\n",
    "def reparam(q_mus, q_sigma, N, D, num_samples, batch_size):\n",
    "    eps = Normal(torch.zeros((num_samples, batch_size, N, D)), torch.ones((num_samples, batch_size, N, D))).sample()\n",
    "    ws = q_mus.unsqueeze(0).repeat(num_samples, 1, 1, 1) + torch.mul(q_sigma.unsqueeze(0).repeat(num_samples, 1, 1, 1), eps)\n",
    "    log_qs = Normal(q_mus, q_sigma).log_prob(ws).sum(-1).sum(-1) ## S * B\n",
    "    return ws, log_qs\n",
    "    \n",
    "    \n",
    "def oneshot(x, mus, N, D, num_samples, batch_size):\n",
    "    mus_expand = mus.unsqueeze(1).repeat(1, N, 1)\n",
    "    data = torch.cat((x, mus_expand), dim=-1).view(batch_size*N, -1)\n",
    "    q_mean, q_sigma = enc(data, N, D, batch_size)\n",
    "    ws, log_qs = reparam(q_mean, q_sigma, N, D, num_samples, batch_size)\n",
    "    log_p_joints = torch.zeros((num_samples, batch_size))\n",
    "    for s in range(num_samples):\n",
    "        log_pr = Normal(prior_mean, prior_sigma).log_prob(ws[s]).sum(-1).sum(-1)\n",
    "        embed = torch.cat((ws[s], mus_expand), -1).view(batch_size*N, -1)\n",
    "        p_mean, p_sigma, ll_recon = dec(embed, x, N, D, batch_size)\n",
    "        log_p_joints[s] = ll_recon + log_pr\n",
    "    log_weights = log_p_joints - log_qs\n",
    "    elbo = log_weights.mean(0).mean()\n",
    "    return elbo\n",
    "\n",
    "def shuffler(batch_Xs, N, D, batch_size):\n",
    "    indices = torch.cat([torch.randperm(N).unsqueeze(0) for b in range(batch_size)])\n",
    "    indices_Xs = indices.unsqueeze(-1).repeat(1, 1, D)\n",
    "    return torch.gather(batch_Xs, 1, indices_Xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, ELBO=-149.244 (0s)\n",
      "epoch=10, ELBO=-102.336 (0s)\n",
      "epoch=20, ELBO=-82.893 (0s)\n",
      "epoch=30, ELBO=-79.308 (0s)\n",
      "epoch=40, ELBO=-78.629 (0s)\n",
      "epoch=50, ELBO=-78.433 (0s)\n",
      "epoch=60, ELBO=-78.416 (0s)\n",
      "epoch=70, ELBO=-77.308 (0s)\n",
      "epoch=80, ELBO=-77.809 (0s)\n",
      "epoch=90, ELBO=-78.428 (0s)\n",
      "epoch=100, ELBO=-77.703 (0s)\n",
      "epoch=110, ELBO=-77.314 (0s)\n",
      "epoch=120, ELBO=-78.328 (0s)\n",
      "epoch=130, ELBO=-76.796 (0s)\n",
      "epoch=140, ELBO=-76.985 (0s)\n",
      "epoch=150, ELBO=-76.584 (0s)\n",
      "epoch=160, ELBO=-76.940 (0s)\n",
      "epoch=170, ELBO=-76.727 (0s)\n",
      "epoch=180, ELBO=-76.340 (0s)\n",
      "epoch=190, ELBO=-76.495 (0s)\n",
      "epoch=200, ELBO=-76.982 (0s)\n",
      "epoch=210, ELBO=-77.038 (0s)\n",
      "epoch=220, ELBO=-76.372 (0s)\n"
     ]
    }
   ],
   "source": [
    "ELBOs = []\n",
    "\n",
    "num_batches = int((Xs.shape[0] / BATCH_SIZE))\n",
    "time_start = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    indices = torch.randperm(num_seqs)\n",
    "    ELBO = 0.0\n",
    "    for step in range(num_batches):\n",
    "        optimizer.zero_grad()\n",
    "        batch_indices = indices[step*BATCH_SIZE : (step+1)*BATCH_SIZE]\n",
    "        batch_Xs = Xs[batch_indices]\n",
    "        batch_mus = mus_true[batch_indices]\n",
    "        batch_Xs = shuffler(batch_Xs, N, D, BATCH_SIZE)\n",
    "        elbo = oneshot(batch_Xs, batch_mus, N, D, NUM_SAMPLES, BATCH_SIZE)\n",
    "        loss = - elbo\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        ELBO += elbo.item() \n",
    "    ELBO /= num_batches\n",
    "    ELBOs.append(ELBO)\n",
    "    \n",
    "    if epoch%10 == 0:\n",
    "        time_end = time.time()  \n",
    "        print('epoch=%d, ELBO=%.3f (%ds)' % (epoch, ELBO, time_end - time_start))\n",
    "        time_start = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc_global' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-47df27e31788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_global\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'models/global-enc-'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_local\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'models/local-enc'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'enc_global' is not defined"
     ]
    }
   ],
   "source": [
    "torch.save(enc_global.state_dict(), 'models/global-enc-' + PATH)\n",
    "torch.save(enc_local.state_dict(), 'models/local-enc' + PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(EUBOs, ELBOs, ESSs, KLs_eta_ex, KLs_eta_in, KL_z_ex, KL_z_in):\n",
    "    fout = open('results/logs-' + PATH +'.txt', 'w+')\n",
    "    fout.write('EUBOs, ELBOs, ESSs, KLs_eta_ex, KLs_eta_in, KL_z_ex, KL_z_in\\n')\n",
    "    for i in range(len(EUBOs)):\n",
    "        fout.write(str(EUBOs[i]) + ', ' + str(ELBOs[i]) + ', ' + str(ESSs[i]) \n",
    "                   + str(KLs_eta_ex[i]) + str(KLs_eta_in[i]) + str(KLs_z_ex[i]) + str(KLs_z_in[i]) + '\\n')\n",
    "    fout.close()\n",
    "save_results(EUBOs, ELBOs, ESSs, KLs_eta_ex, KLs_eta_in, KLs_z_ex, KLs_z_in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_results(EUBOs, ELBOs, ESSs, KLs_eta_ex, KLs_eta_in, KLs_z_ex, KLs_z_in, num_samples):\n",
    "    fig = plt.figure(figsize=(20, 30))\n",
    "    ax1 = fig.add_subplot(3, 1, 1)\n",
    "    ax1.plot(EUBOs, 'r', label='EUBOs')\n",
    "    ax1.plot(ELBOs, 'b', label='ELBOs')\n",
    "    ax1.tick_params(labelsize=18)\n",
    "    ax1.set_ylim([-220, -130])\n",
    "    ax1.legend(fontsize=18)\n",
    "    ##\n",
    "    ax2 = fig.add_subplot(3, 1, 2)\n",
    "    ax2.plot(KLs_eta_ex, '#66b3ff', label='KLs_eta_ex')\n",
    "    ax2.plot(KLs_eta_in, '#ff9999', label='KLs_eta_in')\n",
    "    ax2.plot(KLs_z_ex, '#99ff99', label='KLs_z_ex')\n",
    "    ax2.plot(KLs_z_in, 'gold', label='KLs_z_in')   \n",
    "    ax2.plot(np.ones(len(KLs_z_in)) * 5, 'k', label='const=5.0')\n",
    "    ax2.legend(fontsize=18)\n",
    "    ax2.tick_params(labelsize=18)\n",
    "    ax2.set_ylim([-1, 30])\n",
    "    ##\n",
    "    ax3 = fig.add_subplot(3, 1, 3)\n",
    "    ax3.plot(np.array(ESSs) / num_samples, 'm', label='ESS')\n",
    "    ax3.tick_params(labelsize=18)\n",
    "    ax3.set_xlabel('epochs (%d gradient steps per epoch)'  % num_batches, size=18)\n",
    "    ax3.legend()\n",
    "    plt.savefig('results/train-' + PATH + '.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_results(EUBOs, ELBOs, ESSs, KLs_eta_ex, KLs_eta_in, KLs_z_ex, KLs_z_in, NUM_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "STEPS = 50\n",
    "def reverse(X, z, mus_prev, precisions_prev, N, D, K, batch_size):\n",
    "    data = torch.cat((X, z), dim=-1).view(batch_size*N, -1)\n",
    "    q_mean, q_nu, q_alpha, q_beta, q_sigma, mus, precisions = enc_global(data, K, D, 1, batch_size)  \n",
    "    log_q_eta =  Normal(q_mean[0], q_sigma[0]).log_prob(mus_prev).sum(-1).sum(-1) + Gamma(q_alpha, q_beta).log_prob(precisions_prev).sum(-1).sum(-1)## B\n",
    "    return log_q_eta\n",
    "\n",
    "def test(x, Pi, N, K, D, num_samples, steps, batch_size):\n",
    "    log_increment_weights = torch.zeros((steps, num_samples, batch_size))\n",
    "    log_p_joints = torch.zeros((steps, num_samples, batch_size))\n",
    "    log_qf = torch.zeros((steps-1, num_samples, batch_size))\n",
    "    log_qr = torch.zeros((steps-1, num_samples, batch_size))\n",
    "    Z_samples = torch.zeros((num_samples, batch_size, N, K))\n",
    "    mus_prevs = torch.zeros((num_samples, batch_size, K, D))\n",
    "    precisions_prevs = torch.zeros((num_samples, batch_size, K, D))\n",
    "    \n",
    "    for m in range(steps):\n",
    "        if m == 0:\n",
    "            for l in range(num_samples):\n",
    "                q_mean, q_nu, q_alpha, q_beta, q_sigma, mus, precisions, log_q_eta = Init_step(x, N, D, K, batch_size)\n",
    "                mus_prevs[l] = mus\n",
    "                precisions_prevs[l] = precisions\n",
    "                zs_pi, z, log_q_z = E_step(x, mus, precisions, N, D, K, batch_size)\n",
    "                Z_samples[l] = z\n",
    "                labels = z.nonzero()\n",
    "                log_p_z = cat(Pi).log_prob(z).sum(-1)\n",
    "                sigmas = 1. / torch.sqrt(precisions)\n",
    "                log_p_x = Normal(mus[labels[:, 0], labels[:, -1], :].view(batch_size, N, D), sigmas[labels[:, 0], labels[:, -1], :].view(batch_size, N, D)).log_prob(x).sum(-1).sum(-1)\n",
    "                log_increment_weights[m, l] = log_p_x + log_p_z - log_q_z     \n",
    "                log_p_joints[m, l] = log_joints_gmm(x, z, Pi, mus, precisions, N, D, K, prior_mean, prior_nu, prior_alpha, prior_beta, batch_size)\n",
    "                \n",
    "        else:\n",
    "            for l in range(num_samples):\n",
    "                z_prev = Z_samples[l]\n",
    "                q_mean, q_nu, q_alpha, q_beta, q_sigma, mus, precisions, log_q_eta = M_step(x, z_prev, N, D, K, batch_size)\n",
    "                zs_pi, z, log_q_z = E_step(x, mus, precisions, N, D, K, batch_size)\n",
    "                Z_samples[l] = z\n",
    "                log_p_joints[m, l] = log_joints_gmm(x, z, Pi, mus, precisions, N, D, K, prior_mean, prior_nu, prior_alpha, prior_beta, batch_size)\n",
    "                log_qf[m-1, l] = log_q_eta + log_q_z\n",
    "                \n",
    "                mus_prev = mus_prevs[l]\n",
    "                precisions_prev = precisions_prevs[l]\n",
    "                \n",
    "                log_qr[m-1, l] = reverse(x, z, mus_prev, precisions_prev, N, D, K, batch_size)\n",
    "                log_p_joint = log_joints_gmm(x, z, Pi, mus, precisions, N, D, K, prior_mean, prior_nu, prior_alpha, prior_beta, batch_size)\n",
    "                log_increment_weights[m, l] = log_p_joint - log_q_z - log_q_eta\n",
    "                mus_prevs[l] = mus\n",
    "                precisions_prevs[l] = precisions\n",
    "    detail_balances = log_p_joints[1:] - log_p_joints[:-1] -log_qf + log_qr\n",
    "    increment_weights = torch.exp(log_increment_weights - logsumexp(log_increment_weights, 1).unsqueeze(1).repeat(1, num_samples, 1)).detach()\n",
    "    esses = (1./ (increment_weights ** 2).sum(1))                   \n",
    "    log_last_weights = log_increment_weights[-1] ## S * B\n",
    "    ## EUBO and ELBO\n",
    "    eubos = torch.mul(increment_weights, log_increment_weights).sum(1).mean(-1)\n",
    "    elbos = log_increment_weights.mean(1).mean(-1)     \n",
    "    return eubos, elbos, esses, log_increment_weights, detail_balances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_single_batch(num_seqs, N, K, D, batch_size):\n",
    "    indices = torch.randperm(num_seqs)\n",
    "    batch_indices = indices[0*batch_size : (0+1)*batch_size]\n",
    "    batch_Xs = Xs[batch_indices]\n",
    "    batch_Xs = shuffler(batch_Xs, N, K, D, batch_size)\n",
    "    return batch_Xs\n",
    "batch_Xs = sample_single_batch(num_seqs, N, K, D, BATCH_SIZE)\n",
    "eubo, elbo, ess, log_increment_weights, detail_balances = test(batch_Xs, Pi, N, K, D, NUM_SAMPLES, STEPS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "detail_balances.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(BATCH_SIZE):\n",
    "    log_weights = log_increment_weights[:, :, i]\n",
    "    ess_stepwise = ess[:, i].data.numpy()\n",
    "    fig = plt.figure(figsize=(20,6))\n",
    "    ax1 = fig.add_subplot(1,3,1)\n",
    "    ax2 = fig.add_subplot(1,3,2)\n",
    "    ax3 = fig.add_subplot(1,3,3)\n",
    "    weights = torch.exp(log_weights - logsumexp(log_weights, -1)[:, None]).data.numpy()\n",
    "    db = detail_balances[:, :, i].mean(-1)\n",
    "    ax1.plot(db.data.numpy(), 'r-o')\n",
    "    ax1.set_ylim([-20, 20])\n",
    "    for s in range(NUM_SAMPLES):\n",
    "        ax2.plot(ess_stepwise, 'b-o')\n",
    "        ax3.plot(weights[:, s], 'g-o')\n",
    "        ax2.set_ylim([1.0, 7])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def test(x, num_seqs, Pi, N, K, D, steps, batch_size):\n",
    "    LLs = [] \n",
    "    for m in range(steps):\n",
    "        if m == 0:\n",
    "            mus, precisions, log_p_eta = inti_global(K, D, prior_mean, prior_nu, prior_alpha, prior_beta, batch_size)\n",
    "            zs_pi, z, log_q_z = E_step(x, mus, precisions, N, D, K, batch_size)\n",
    "        else:\n",
    "            q_mean, q_nu, q_alpha, q_beta, q_sigma, mus, precisions, log_q_eta = M_step(x, z, N, D, K, batch_size)\n",
    "            zs_pi, z, log_q_z = E_step(x, mus, precisions, N, D, K, batch_size)\n",
    "            labels = z.nonzero()\n",
    "            sigmas = 1. / torch.sqrt(precisions)\n",
    "            ll = Normal(mus[labels[:, 0], labels[:, -1], :].view(batch_size, N, D), sigmas[labels[:, 0], labels[:, -1], :].view(batch_size, N, D)).log_prob(x).sum(-1).sum(-1).mean()\n",
    "            LLs.append(ll.item())\n",
    "    E_precisions = q_alpha / q_beta\n",
    "    E_mus = q_mean\n",
    "    E_z = torch.argmax(zs_pi, dim=-1)\n",
    "\n",
    "    return z, mus, precisions, LLs, E_mus, E_precisions, E_z\n",
    "\n",
    "x,z_true = sample_single_batch(num_seqs, N, K, D, BATCH_SIZE)\n",
    "z, mus, precisions, LLs, E_mus, E_precisions, E_z = test(x, num_seqs, Pi, N, K, D, STEPS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_final_samples(Xs, Zs, mus, precisions, steps, batch_size):\n",
    "    colors = ['r', 'b', 'gold']\n",
    "    fig = plt.figure(figsize=(25,100))\n",
    "    for b in range(batch_size):\n",
    "        ax = fig.add_subplot(int(batch_size / 5), 5, b+1)\n",
    "        x = Xs[b].data.numpy()\n",
    "        z = Zs[b].data.numpy()\n",
    "        mu = mus[b].data.numpy()\n",
    "        precision = precisions[b].data.numpy()\n",
    "\n",
    "        covs = np.zeros((K, D, D))\n",
    "        assignments = z\n",
    "        for k in range(K):\n",
    "            covs[k] = np.diag(1. / precision[k])\n",
    "            xk = x[np.where(assignments == k)]\n",
    "            ax.scatter(xk[:, 0], xk[:, 1], c=colors[k])\n",
    "            plot_cov_ellipse(cov=covs[k], pos=mu[k], nstd=2, ax=ax, alpha=0.2, color=colors[k])\n",
    "        ax.set_ylim([-10, 10])\n",
    "        ax.set_xlim([-10, 10])\n",
    "    plt.savefig('results/modes' + PATH + '.svg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_final_samples(x, E_z, E_mus, E_precisions, STEPS, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LLs[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LLs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(LLs[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(LLs).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.exp(-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
