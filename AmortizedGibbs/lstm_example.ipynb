{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probtorch: 0.0+f9f5c9b torch: 0.4.1 cuda: True\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "import torch as to\n",
    "import torch.nn as nn\n",
    "from torch.nn import Parameter\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "import probtorch\n",
    "from probtorch.util import expand_inputs\n",
    "print('probtorch:', probtorch.__version__, \n",
    "      'torch:', to.__version__, \n",
    "      'cuda:', to.cuda.is_available())\n",
    "from util_data import *\n",
    "from amorgibbs_v import *\n",
    "from smc_v import *\n",
    "from util_plots import *\n",
    "from util_lstm import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = 4\n",
    "K = 10\n",
    "HIDDEN_DIM = K*K\n",
    "BATCH_SIZE = 20\n",
    "NUM_EPOCHS = 200\n",
    "TARGET_DIM = K*K\n",
    "DATASET_SIZE = BATCH_SIZE * 20\n",
    "\n",
    "CUDA = to.cuda.is_available()\n",
    "RESTORE = False\n",
    "MODEL_NAME = 'lstm'\n",
    "DATA_PATH = './data'\n",
    "WEIGHTS_PATH = './lstm_weights'\n",
    "\n",
    "LEARNING_RATE = 1e-3\n",
    "\n",
    "\n",
    "# Parameters for data generation\n",
    "boundary = 32\n",
    "noise_cov = np.eye(2)*0.5\n",
    "T_min = 30\n",
    "T_max = 50\n",
    "dt = 10\n",
    "init_v = init_velocity(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kl_dirichlet(alpha1, alpha2):\n",
    "    A = to.lgamma(alpha1.sum()) - to.lgamma(alpha2.sum())\n",
    "    B = (to.lgamma(alpha1) - to.lgamma(alpha2)).sum()\n",
    "    C = (to.mul(alpha1 - alpha2, to.digamma(alpha1) - to.digamma(alpha1.sum()))).sum()\n",
    "    kl = A - B + C\n",
    "    return kl\n",
    "\n",
    "def MKL(alphas_pred, alpha_true):\n",
    "    kkl = 0\n",
    "    for i in range(alphas_pred.size()[0]):\n",
    "        for k in range(K): \n",
    "            kkl += kl_dirichlet(alphas_pred[i, k*K:(k+1)*K], alpha_true[k*K:(k+1)*K])\n",
    "            #print(alphas_pred[i, k*K:(k+1)*K], alpha_true[k*K:(k+1)*K], 'asd')\n",
    "    return kkl/K\n",
    "\n",
    "alpha_true = to.ones(K*K) / K\n",
    "loss_fn = lambda alpha: MKL(alpha, alpha_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0] Train: Loss 9.3842e+01 (0s)\n",
      "[Epoch 1] Train: Loss 7.0244e+01 (0s)\n",
      "[Epoch 2] Train: Loss 5.6478e+01 (0s)\n",
      "[Epoch 3] Train: Loss 5.2409e+01 (0s)\n",
      "[Epoch 4] Train: Loss 5.0418e+01 (0s)\n",
      "[Epoch 5] Train: Loss 4.9678e+01 (0s)\n",
      "[Epoch 6] Train: Loss 4.8531e+01 (0s)\n",
      "[Epoch 7] Train: Loss 4.7808e+01 (0s)\n",
      "[Epoch 8] Train: Loss 4.7414e+01 (0s)\n",
      "[Epoch 9] Train: Loss 4.6987e+01 (0s)\n",
      "[Epoch 10] Train: Loss 4.6659e+01 (0s)\n",
      "[Epoch 11] Train: Loss 4.6386e+01 (0s)\n",
      "[Epoch 12] Train: Loss 4.5939e+01 (0s)\n",
      "[Epoch 13] Train: Loss 4.5810e+01 (0s)\n",
      "[Epoch 14] Train: Loss 4.5793e+01 (0s)\n",
      "[Epoch 15] Train: Loss 4.5501e+01 (0s)\n",
      "[Epoch 16] Train: Loss 4.5229e+01 (0s)\n",
      "[Epoch 17] Train: Loss 4.5221e+01 (0s)\n",
      "[Epoch 18] Train: Loss 4.5216e+01 (0s)\n",
      "[Epoch 19] Train: Loss 4.5213e+01 (0s)\n",
      "[Epoch 20] Train: Loss 4.5210e+01 (0s)\n",
      "[Epoch 21] Train: Loss 4.5207e+01 (0s)\n",
      "[Epoch 22] Train: Loss 4.5205e+01 (0s)\n",
      "[Epoch 23] Train: Loss 4.5203e+01 (0s)\n",
      "[Epoch 24] Train: Loss 4.5201e+01 (0s)\n",
      "[Epoch 25] Train: Loss 4.5199e+01 (0s)\n",
      "[Epoch 26] Train: Loss 4.5198e+01 (0s)\n",
      "[Epoch 27] Train: Loss 4.5197e+01 (0s)\n",
      "[Epoch 28] Train: Loss 4.5196e+01 (0s)\n",
      "[Epoch 29] Train: Loss 4.5195e+01 (0s)\n",
      "[Epoch 30] Train: Loss 4.5194e+01 (0s)\n",
      "[Epoch 31] Train: Loss 4.5193e+01 (0s)\n",
      "[Epoch 32] Train: Loss 4.5192e+01 (0s)\n",
      "[Epoch 33] Train: Loss 4.5191e+01 (0s)\n",
      "[Epoch 34] Train: Loss 4.5191e+01 (0s)\n",
      "[Epoch 35] Train: Loss 4.5190e+01 (0s)\n",
      "[Epoch 36] Train: Loss 4.5189e+01 (0s)\n",
      "[Epoch 37] Train: Loss 4.5189e+01 (0s)\n",
      "[Epoch 38] Train: Loss 4.5188e+01 (0s)\n",
      "[Epoch 39] Train: Loss 4.5188e+01 (0s)\n",
      "[Epoch 40] Train: Loss 4.5187e+01 (0s)\n",
      "[Epoch 41] Train: Loss 4.5187e+01 (0s)\n",
      "[Epoch 42] Train: Loss 4.5187e+01 (0s)\n",
      "[Epoch 43] Train: Loss 4.5186e+01 (0s)\n",
      "[Epoch 44] Train: Loss 4.5186e+01 (0s)\n",
      "[Epoch 45] Train: Loss 4.5186e+01 (0s)\n",
      "[Epoch 46] Train: Loss 4.5185e+01 (0s)\n",
      "[Epoch 47] Train: Loss 4.5185e+01 (0s)\n",
      "[Epoch 48] Train: Loss 4.5185e+01 (0s)\n",
      "[Epoch 49] Train: Loss 4.5184e+01 (0s)\n",
      "[Epoch 50] Train: Loss 4.5184e+01 (0s)\n",
      "[Epoch 51] Train: Loss 4.5184e+01 (0s)\n",
      "[Epoch 52] Train: Loss 4.5184e+01 (0s)\n",
      "[Epoch 53] Train: Loss 4.5184e+01 (0s)\n",
      "[Epoch 54] Train: Loss 4.5183e+01 (0s)\n",
      "[Epoch 55] Train: Loss 4.5183e+01 (0s)\n",
      "[Epoch 56] Train: Loss 4.5183e+01 (0s)\n",
      "[Epoch 57] Train: Loss 4.5183e+01 (0s)\n",
      "[Epoch 58] Train: Loss 4.5183e+01 (0s)\n",
      "[Epoch 59] Train: Loss 4.5182e+01 (0s)\n",
      "[Epoch 60] Train: Loss 4.5182e+01 (0s)\n",
      "[Epoch 61] Train: Loss 4.5182e+01 (0s)\n",
      "[Epoch 62] Train: Loss 4.5182e+01 (0s)\n",
      "[Epoch 63] Train: Loss 4.5182e+01 (0s)\n",
      "[Epoch 64] Train: Loss 4.5181e+01 (0s)\n",
      "[Epoch 65] Train: Loss 4.5150e+01 (0s)\n",
      "[Epoch 66] Train: Loss 4.4638e+01 (0s)\n",
      "[Epoch 67] Train: Loss 4.4617e+01 (0s)\n",
      "[Epoch 68] Train: Loss 4.4617e+01 (0s)\n",
      "[Epoch 69] Train: Loss 4.4616e+01 (0s)\n",
      "[Epoch 70] Train: Loss 4.4616e+01 (0s)\n",
      "[Epoch 71] Train: Loss 4.4616e+01 (0s)\n",
      "[Epoch 72] Train: Loss 4.4616e+01 (0s)\n",
      "[Epoch 73] Train: Loss 4.4616e+01 (0s)\n",
      "[Epoch 74] Train: Loss 4.4616e+01 (0s)\n",
      "[Epoch 75] Train: Loss 4.4616e+01 (0s)\n",
      "[Epoch 76] Train: Loss 4.4616e+01 (0s)\n",
      "[Epoch 77] Train: Loss 4.4616e+01 (0s)\n",
      "[Epoch 78] Train: Loss 4.4616e+01 (0s)\n",
      "[Epoch 79] Train: Loss 4.4615e+01 (0s)\n",
      "[Epoch 80] Train: Loss 4.4615e+01 (0s)\n",
      "[Epoch 81] Train: Loss 4.4615e+01 (0s)\n",
      "[Epoch 82] Train: Loss 4.4615e+01 (0s)\n",
      "[Epoch 83] Train: Loss 4.4615e+01 (0s)\n",
      "[Epoch 84] Train: Loss 4.4615e+01 (0s)\n",
      "[Epoch 85] Train: Loss 4.4615e+01 (0s)\n",
      "[Epoch 86] Train: Loss 4.4615e+01 (0s)\n",
      "[Epoch 87] Train: Loss 4.4615e+01 (0s)\n",
      "[Epoch 88] Train: Loss 4.4615e+01 (0s)\n",
      "[Epoch 89] Train: Loss 4.4615e+01 (0s)\n",
      "[Epoch 90] Train: Loss 4.4615e+01 (0s)\n",
      "[Epoch 91] Train: Loss 4.4615e+01 (0s)\n",
      "[Epoch 92] Train: Loss 4.4615e+01 (0s)\n",
      "[Epoch 93] Train: Loss 4.4615e+01 (0s)\n",
      "[Epoch 94] Train: Loss 4.4615e+01 (0s)\n",
      "[Epoch 95] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 96] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 97] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 98] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 99] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 100] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 101] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 102] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 103] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 104] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 105] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 106] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 107] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 108] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 109] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 110] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 111] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 112] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 113] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 114] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 115] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 116] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 117] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 118] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 119] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 120] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 121] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 122] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 123] Train: Loss 4.4614e+01 (0s)\n",
      "[Epoch 124] Train: Loss 4.4613e+01 (0s)\n",
      "[Epoch 125] Train: Loss 4.4613e+01 (0s)\n",
      "[Epoch 126] Train: Loss 4.4613e+01 (0s)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6bdd525a3b93>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtrain_start\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mtrain_end\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m#test_start = time.time()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/amortized/AmortizedGibbs/util_lstm.py\u001b[0m in \u001b[0;36mtrain_epoch\u001b[0;34m(batches, model, optimizer, loss_fn)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mfx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_pack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen_seq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mbatch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m     91\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \"\"\"\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     88\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     89\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import time\n",
    "import os\n",
    "from random import random\n",
    "\n",
    "Seq, Len = genSeq(T_min, T_max, dt, init_v, noise_cov, boundary, DATASET_SIZE)\n",
    "batches = PackSeq(Seq, Len, BATCH_SIZE) \n",
    "\n",
    "model = LSTM(INPUT_DIM, HIDDEN_DIM, BATCH_SIZE, TARGET_DIM)\n",
    "optimizer = to.optim.Adam(model.parameters(), )\n",
    "\n",
    "\n",
    "if not RESTORE:\n",
    "    mask = {}\n",
    "    for e in range(NUM_EPOCHS):\n",
    "        train_start = time.time()\n",
    "        train_loss = train_epoch(batches, model, optimizer, loss_fn)\n",
    "        train_end = time.time()\n",
    "        #test_start = time.time()\n",
    "        #test_loss = test(test_loader, model)\n",
    "        #test_end = time.time()\n",
    "        print('[Epoch %d] Train: Loss %.4e (%ds)' % (e, train_loss, train_end - train_start))\n",
    "\n",
    "    if not os.path.isdir(WEIGHTS_PATH):\n",
    "        os.mkdir(WEIGHTS_PATH)\n",
    "    to.save(model.state_dict(),\n",
    "               '%s/%s-%s-%s-enc.rar' % (WEIGHTS_PATH, MODEL_NAME, probtorch.__version__, to.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
