{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probtorch: 0.0+5a2c637 torch: 1.0.0 cuda: True\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%run ../../import_envs.py\n",
    "print('probtorch:', probtorch.__version__, \n",
    "      'torch:', torch.__version__, \n",
    "      'cuda:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run PARAMETERS.py\n",
    "NAME = 'APG'\n",
    "MCMC_STEPS = 5\n",
    "PATH = NAME + '-bmnist-%dsamples' % (SAMPLE_SIZE)\n",
    "CUDA = torch.cuda.is_available()\n",
    "DEVICE = torch.device('cuda:1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_operations import Init_models, Save_models\n",
    "models, optimizer = Init_models(FRAME_PIXELS=FRAME_PIXELS, \n",
    "                                DIGIT_PIXELS=DIGIT_PIXELS, \n",
    "                                HIDDEN_LIST=HIDDEN_LIST, \n",
    "                                CUDA=CUDA, \n",
    "                                device=DEVICE, \n",
    "                                lr=LEARNING_RATE, \n",
    "                                RESTORE=False, \n",
    "                                PATH=None)\n",
    "\n",
    "(enc_coor, enc_digit, dec_digit) = models\n",
    "\n",
    "from crop import *\n",
    "crop = Crop(digit_size=28, \n",
    "            frame_size=64, \n",
    "            CUDA=CUDA, \n",
    "            device=DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed  in (8s),  phi_loss: -3.945,  theta_loss: 15424.491,  ess: 1.030\n",
      "Completed  in (8s),  phi_loss: -18.779,  theta_loss: 4542.320,  ess: 1.059\n",
      "Completed  in (8s),  phi_loss: -32.533,  theta_loss: 4266.087,  ess: 1.059\n",
      "Completed  in (8s),  phi_loss: -37.118,  theta_loss: 4041.823,  ess: 1.061\n",
      "Completed  in (8s),  phi_loss: -46.896,  theta_loss: 3806.028,  ess: 1.066\n",
      "Completed  in (8s),  phi_loss: -47.991,  theta_loss: 3710.010,  ess: 1.064\n",
      "Completed  in (8s),  phi_loss: -53.119,  theta_loss: 3519.406,  ess: 1.065\n",
      "Completed  in (8s),  phi_loss: -55.561,  theta_loss: 3547.026,  ess: 1.072\n",
      "Completed  in (8s),  phi_loss: -55.085,  theta_loss: 3457.389,  ess: 1.067\n",
      "Completed  in (8s),  phi_loss: -58.928,  theta_loss: 3318.284,  ess: 1.070\n",
      "Completed  in (8s),  phi_loss: -58.852,  theta_loss: 3278.996,  ess: 1.074\n",
      "Completed  in (8s),  phi_loss: -62.035,  theta_loss: 3257.823,  ess: 1.082\n",
      "Completed  in (8s),  phi_loss: -60.504,  theta_loss: 3189.499,  ess: 1.088\n",
      "Completed  in (8s),  phi_loss: -60.755,  theta_loss: 3167.907,  ess: 1.085\n",
      "Completed  in (8s),  phi_loss: -63.315,  theta_loss: 3112.490,  ess: 1.084\n",
      "Completed  in (8s),  phi_loss: -64.080,  theta_loss: 3132.813,  ess: 1.090\n",
      "Completed  in (8s),  phi_loss: -64.885,  theta_loss: 3103.048,  ess: 1.089\n",
      "Completed  in (8s),  phi_loss: -65.756,  theta_loss: 3080.144,  ess: 1.078\n",
      "Completed  in (8s),  phi_loss: -63.891,  theta_loss: 3032.420,  ess: 1.087\n"
     ]
    }
   ],
   "source": [
    "from apg import APG\n",
    "train(models=models, \n",
    "      objective=APG, \n",
    "      optimizer=optimizer, \n",
    "      data=data, \n",
    "      tjs=tjs,\n",
    "      mcmc_steps=MCMC_STEPS,\n",
    "      mnist_mean=mnist_mean,\n",
    "      crop=crop,\n",
    "      Train_Params=Train_Params,\n",
    "      CUDA=CUDA,\n",
    "      device=DEVICE,\n",
    "      path=PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.functional import conv2d, normalize, softmax\n",
    "batch = data[:BATCH_SIZE]\n",
    "B, T, FP, _ = batch.shape\n",
    "batch_expand = batch.transpose(0,1).unsqueeze(1).repeat(1, SAMPLE_SIZE, 1, 1, 1).view(T, SAMPLE_SIZE*BATCH_SIZE, FP, FP)\n",
    "kernels = mnist_mean.repeat(BATCH_SIZE*SAMPLE_SIZE,1,1).unsqueeze(1)\n",
    "convolved = conv2d(batch_expand, kernels, groups=int(BATCH_SIZE*SAMPLE_SIZE))\n",
    "convolved = convolved.view(T, SAMPLE_SIZE, BATCH_SIZE, 37, 37).view(T, SAMPLE_SIZE, BATCH_SIZE, 37*37).permute(1,2,0,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convolved[0,0,0,:].max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "softmax(convolved, dim=-1).max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data[:BATCH_SIZE].cuda().to(DEVICE)\n",
    "mnist_mean = mnist_mean.cuda().to(DEVICE)\n",
    "q, convolved = enc_coor(SAMPLE_SIZE, batch, mnist_mean.repeat(SAMPLE_SIZE, BATCH_SIZE, 1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_img = convolved.cpu().view(10, SAMPLE_SIZE, BATCH_SIZE, 37,37).data.numpy()\n",
    "original_img = batch.cpu().data.numpy()\n",
    "\n",
    "import matplotlib.gridspec as gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "fs = 15\n",
    "num_cols = 10*2\n",
    "num_rows = BATCH_SIZE\n",
    "gs = gridspec.GridSpec(num_rows, num_cols)\n",
    "gs.update(left=0.0 , bottom=0.0, right=1.0, top=1.0, wspace=0.05, hspace=0.05)\n",
    "fig = plt.figure(figsize=(fs, fs * num_rows / num_cols))\n",
    "for i in range(num_rows):\n",
    "    for j in range(num_cols):\n",
    "        ax = fig.add_subplot(gs[i, j])\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        if j < 10:\n",
    "            ax.imshow(original_img[i, j], cmap='gray', vmin=0.0, vmax=1.0)\n",
    "        else:\n",
    "            ax.imshow(conv_img[j-10, 0, i], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
