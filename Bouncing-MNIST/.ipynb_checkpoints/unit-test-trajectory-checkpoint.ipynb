{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sim_moving import *\n",
    "from torch.distributions.normal import Normal\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append('/home/hao/Research/probtorch/')\n",
    "import probtorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tjs = 10000\n",
    "T = 20\n",
    "num_digits = 1\n",
    "step_length = 1.0\n",
    "frame_size = 64\n",
    "PATH = '/home/hao/Research/amortized/Moving-MNIST/data/'\n",
    "mnist_path = '/home/hao/Research/apg_data/mnist/'\n",
    "simulator = BouncingMNIST(mnist_path=mnist_path, path=PATH, timesteps=T, num_digits=num_digits, step_length=step_length)\n",
    "# simulator.sim_tjs(num_tjs)\n",
    "\n",
    "Vs = torch.from_numpy(np.load('./data/tjs_v.npy')).float()\n",
    "Xs = torch.from_numpy(np.load('./data/tjs_x.npy')).float()\n",
    "##\n",
    "NUM_HIDDEN = 16\n",
    "NUM_STEPS = 1000\n",
    "BATCH_SIZE = 10\n",
    "SAMPLE_SIZE = 10\n",
    "CUDA = torch.cuda.is_available()\n",
    "num_tjs = Vs.shape[0]\n",
    "num_batches = int(num_tjs /  BATCH_SIZE)\n",
    "latent_dim = 8\n",
    "delta_t = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Enc_tj(nn.Module):\n",
    "#     def __init__(self, input_dim, num_hidden, latent_dim):\n",
    "#         super(self.__class__, self).__init__()\n",
    "#         self.enc_w = nn.Sequential(\n",
    "#                             nn.Linear(input_dim, latent_dim))\n",
    "#         self.enc_alpha = nn.Sequential(\n",
    "#                             nn.Linear(input_dim, latent_dim))\n",
    "#         self.enc_t = nn.Sequential(\n",
    "#                             nn.Linear(input_dim, latent_dim))\n",
    "#     def forward(self, x, v):\n",
    "#         xv = torch.cat((x, v), -1)\n",
    "#         w = self.enc_w(xv)\n",
    "#         alpha = self.enc_alpha(xv)\n",
    "#         t = self.enc_t(xv)\n",
    "#         x_pred = (w * torch.sin(x * alpha + t)).sum(-1).unsqueeze(-1)\n",
    "#         min_ind = torch.argmin(alpha, dim=-1).unsqueeze(-1).detach()\n",
    "        \n",
    "# #         alpha_min = torch.gather(alpha, 2, min_ind).detach()\n",
    "# #         w_min = torch.gather(w, 2, min_ind).detach()\n",
    "# #         t_min = torch.gather(t, 2, min_ind).detach()\n",
    "#         v_pred = (alpha * w * torch.cos(x * alpha + t)).sum(-1).unsqueeze(-1).detach().sign() * v.abs()\n",
    "#         return x_pred, v_pred, alpha, w, t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enc_tj(nn.Module):\n",
    "    def __init__(self, input_dim, num_hidden, latent_dim, T, delta_t, CUDA):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.enc_w = nn.Sequential(\n",
    "                            nn.Linear(input_dim, latent_dim))\n",
    "        self.enc_alpha = nn.Sequential(\n",
    "                            nn.Linear(input_dim, latent_dim))\n",
    "        self.enc_t = nn.Sequential(\n",
    "                            nn.Linear(input_dim, latent_dim))\n",
    "        \n",
    "        \n",
    "        self.ts =  torch.arange(0, T*delta_t, delta_t).float()\n",
    "        if CUDA:\n",
    "            self.ts = self.ts.cuda()\n",
    "    def forward(self, x):\n",
    "        B, _ = x.shape\n",
    "        w = self.enc_w(x)\n",
    "        alpha = self.enc_alpha(x)\n",
    "        phase = self.enc_t(x)\n",
    "        x_pred = (w.unsqueeze(1).repeat(1, T, 1) * torch.sin(torch.bmm(self.ts.repeat(B, 1).unsqueeze(-1), alpha.unsqueeze(1)) + phase.unsqueeze(1).repeat(1, T, 1))).sum(-1)\n",
    "#         min_ind = torch.argmin(alpha, dim=-1).unsqueeze(-1).detach()\n",
    "        \n",
    "#         alpha_min = torch.gather(alpha, 2, min_ind).detach()\n",
    "#         w_min = torch.gather(w, 2, min_ind).detach()\n",
    "#         t_min = torch.gather(t, 2, min_ind).detach()\n",
    "#         v_pred = (alpha * w * torch.cos(x * alpha + t)).sum(-1).unsqueeze(-1).detach().sign() * v.abs()\n",
    "        return x_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Enc_tj(nn.Module):\n",
    "#     def __init__(self, num_hidden):\n",
    "#         super(self.__class__, self).__init__()\n",
    "#         self.enc_hidden = nn.Sequential(\n",
    "#                             nn.Linear(4, num_hidden),\n",
    "#                             nn.Linear(num_hidden, 2),\n",
    "#                             nn.Tanh())\n",
    "#     def forward(self, x, v):\n",
    "#         v_new = self.enc_hidden(torch.cat((x, v), -1))\n",
    "#         x_new = x + 0.2 * v_new\n",
    "#         return x_new, v_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Enc_tj(input_dim=T, num_hidden=NUM_HIDDEN, latent_dim=latent_dim, T=T, delta_t=delta_t, CUDA=CUDA)\n",
    "enc.cuda()\n",
    "optimizer =  torch.optim.Adam(list(enc.parameters()),lr=1e-5, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(NUM_STEPS):\n",
    "#     optimizer.zero_grad()\n",
    "#     indices = torch.randperm(num_tjs)\n",
    "#     LOSS = 0.0\n",
    "#     for b in range(num_batches):\n",
    "#         b_ind = indices[b*BATCH_SIZE:(b+1)*BATCH_SIZE]\n",
    "#         xb = Xs[b_ind].cuda()[:, :, 0].unsqueeze(-1)\n",
    "#         vb = Vs[b_ind].cuda()[:, :, 0].unsqueeze(-1)\n",
    "#         for t in range(T-1):\n",
    "#             if t == 0:\n",
    "#                 x_pred, v_pred = enc(xb[:, t, 0].unsqueeze(-1), vb[:, t, 0].unsqueeze(-1))\n",
    "#             else:\n",
    "#                 x_pred, v_pred = enc(x_pred, v_pred)\n",
    "#             loss = ((x_pred - xb[:, t+1, 0])**2).sum(-1).mean()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         LOSS += loss\n",
    "#     %time print('iter=%d, loss=%.6f' % (i, LOSS / num_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=0, loss=7.610917\n",
      "CPU times: user 352 µs, sys: 23 µs, total: 375 µs\n",
      "Wall time: 360 µs\n",
      "iter=1, loss=4.714545\n",
      "CPU times: user 232 µs, sys: 15 µs, total: 247 µs\n",
      "Wall time: 252 µs\n"
     ]
    }
   ],
   "source": [
    "for i in range(NUM_STEPS):\n",
    "    optimizer.zero_grad()\n",
    "    indices = torch.randperm(num_tjs)\n",
    "    LOSS = 0.0\n",
    "    for b in range(num_batches):\n",
    "        b_ind = indices[b*BATCH_SIZE:(b+1)*BATCH_SIZE]\n",
    "        xb = Xs[b_ind].cuda()[:, :, 0] ## B * T\n",
    "#         xb.requires_grad_(True)\n",
    "#         vb = Vs[b_ind].cuda()[:,:, 0]\n",
    "        x_pred = enc(xb)\n",
    "        loss = ((x_pred - xb)**2).sum(-1).mean()\n",
    "        loss.backward()\n",
    "#         v_pred = (torch.sign(xb.grad) * vb.abs()).detach()\n",
    "        optimizer.step()\n",
    "        LOSS += loss\n",
    "        \n",
    "    %time print('iter=%d, loss=%.6f' % (i, LOSS / num_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer.zero_grad()\n",
    "# indices = torch.randperm(num_tjs)\n",
    "# b_ind = indices[0*BATCH_SIZE:(0+1)*BATCH_SIZE]\n",
    "# xb = Xs[b_ind].cuda()[:, :-1, 0].unsqueeze(-1)\n",
    "# xb.requires_grad_(True)\n",
    "\n",
    "# vb = Vs[b_ind].cuda()[:,:-1, 0].unsqueeze(-1)\n",
    "# x_pred = enc(xb, vb)\n",
    "# loss = ((x_pred - xb)**2).sum(-1).mean()\n",
    "# loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 4\n",
    "optimizer.zero_grad()\n",
    "xb = Xs[s:(s+1)].cuda()[:,:-1, 0].unsqueeze(-1)\n",
    "# xb.requires_grad_(True)\n",
    "vb = Vs[s:(s+1)].cuda()[:, :-1, 0].unsqueeze(-1)\n",
    "x_pred, v_pred, alpha, w, t  = enc(xb, vb)\n",
    "# x_pred.backward()\n",
    "# v_pred = (torch.sign(xb.grad) * vb.abs()).detach()\n",
    "# x_pred = x_pred.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = 20\n",
    "XP = []\n",
    "VP = []\n",
    "xp = x_pred[0, 0, :].unsqueeze(0).unsqueeze(0)\n",
    "# vp = v_pred[0, -1, :].unsqueeze(0).unsqueeze(0)\n",
    "vp = vb[0, 0, :].unsqueeze(0).unsqueeze(0)\n",
    "VP.append(vp.unsqueeze(0))\n",
    "XP.append(xp.unsqueeze(0))\n",
    "\n",
    "for p in range(P):\n",
    "    xp, vp, _, _, _ = enc(xp,vp)\n",
    "    XP.append(xp.unsqueeze(0))\n",
    "    VP.append(vp.unsqueeze(0))\n",
    "VP = torch.cat(VP, 0).squeeze(1)\n",
    "XP = torch.cat(XP, 0).squeeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "space = torch.linspace(0, 20, 200)\n",
    "wave = (w.cpu()[0, 0, :] * torch.sin(space.unsqueeze(-1).repeat(1, 8) * alpha.cpu()[0, 0, :] + t.cpu()[0, 0, :])).sum(-1).unsqueeze(-1)\n",
    "grad = (alpha.cpu()[0, 0, :] * w.cpu()[0, 0, :] * torch.cos(space.unsqueeze(-1).repeat(1, 8) * alpha.cpu()[0, 0, :] + t.cpu()[0, 0, :])).sum(-1).unsqueeze(-1).detach().sign() * vb[0, 0, 0].cpu().abs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plt = np.arange(0, 0.2*18, 0.2)\n",
    "\n",
    "plt.plot(x_plt, xb[0, 1:, 0].cpu().data.numpy(), c='r', marker='o')\n",
    "plt.plot(np.arange(0, 0.2*19, 0.2), x_pred[0, :,0].cpu().data.numpy(), c='b', marker='o')\n",
    "# plt.plot(x_plt, XP[:, 0, 0].cpu().data.numpy(), c='m')\n",
    "plt.plot(space.data.numpy(), wave.data.numpy())\n",
    "plt.plot(space.data.numpy(), grad.data.numpy(), marker='o')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(xb[0, 1:, 0].cpu().data.numpy(), xb[0, 1:, 1].cpu().data.numpy(), c='r')\n",
    "# plt.scatter(x_pred[:, 0].cpu().data.numpy(), x_pred[:, 1].cpu().data.numpy(), c='b')\n",
    "# plt.scatter(XP[:, 0].cpu().data.numpy(), XP[:, 1].cpu().data.numpy(), c='m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(enc.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
