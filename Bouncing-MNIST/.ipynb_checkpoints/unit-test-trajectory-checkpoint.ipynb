{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sim_moving import *\n",
    "from torch.distributions.normal import Normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_tjs = 10000\n",
    "# T = 20\n",
    "# num_digits = 1\n",
    "# step_length = 0.2\n",
    "# frame_size = 64\n",
    "# PATH = '/home/hao/Research/amortized/Moving-MNIST/data/'\n",
    "# simulator = BouncingMNIST(path=PATH, timesteps=T, num_digits=num_digits, step_length=step_length)\n",
    "# simulator.sim_tjs(num_tjs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vs = torch.from_numpy(np.load('./data/tjs_v.npy')).float()\n",
    "Xs = torch.from_numpy(np.load('./data/tjs_x.npy')).float()\n",
    "##\n",
    "NUM_HIDDEN = 64\n",
    "NUM_STEPS = 1000\n",
    "BATCH_SIZE = 10\n",
    "SAMPLE_SIZE = 10\n",
    "num_tjs = Vs.shape[0]\n",
    "num_batches = int(num_tjs /  BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Enc_tj(nn.Module):\n",
    "    def __init__(self, num_hidden):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.enc_hidden = nn.Sequential(\n",
    "                            nn.Linear(4, num_hidden),\n",
    "                            nn.Linear(num_hidden, 2),\n",
    "                            nn.Tanh())\n",
    "    def forward(self, x, v):\n",
    "        v_new = self.enc_hidden(torch.cat((x, v), -1))\n",
    "        x_new = x + 0.2 * v_new\n",
    "        return x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Enc_tj(num_hidden=NUM_HIDDEN)\n",
    "enc.cuda()\n",
    "optimizer =  torch.optim.Adam(list(enc.parameters()),lr=1e-5, betas=(0.9, 0.99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter=0, loss=0.044105\n",
      "CPU times: user 566 µs, sys: 66 µs, total: 632 µs\n",
      "Wall time: 637 µs\n",
      "iter=1, loss=0.030871\n",
      "CPU times: user 895 µs, sys: 0 ns, total: 895 µs\n",
      "Wall time: 877 µs\n",
      "iter=2, loss=0.021365\n",
      "CPU times: user 188 µs, sys: 22 µs, total: 210 µs\n",
      "Wall time: 215 µs\n",
      "iter=3, loss=0.014743\n",
      "CPU times: user 611 µs, sys: 72 µs, total: 683 µs\n",
      "Wall time: 689 µs\n",
      "iter=4, loss=0.010452\n",
      "CPU times: user 206 µs, sys: 24 µs, total: 230 µs\n",
      "Wall time: 236 µs\n",
      "iter=5, loss=0.007918\n",
      "CPU times: user 1.28 ms, sys: 149 µs, total: 1.43 ms\n",
      "Wall time: 1.43 ms\n",
      "iter=6, loss=0.006642\n",
      "CPU times: user 225 µs, sys: 26 µs, total: 251 µs\n",
      "Wall time: 257 µs\n",
      "iter=7, loss=0.006203\n",
      "CPU times: user 183 µs, sys: 22 µs, total: 205 µs\n",
      "Wall time: 209 µs\n",
      "iter=8, loss=0.006170\n",
      "CPU times: user 1.05 ms, sys: 122 µs, total: 1.18 ms\n",
      "Wall time: 1.18 ms\n",
      "iter=9, loss=0.006165\n",
      "CPU times: user 1.26 ms, sys: 145 µs, total: 1.4 ms\n",
      "Wall time: 1.41 ms\n",
      "iter=10, loss=0.006164\n",
      "CPU times: user 1.18 ms, sys: 134 µs, total: 1.31 ms\n",
      "Wall time: 1.31 ms\n",
      "iter=11, loss=0.006166\n",
      "CPU times: user 0 ns, sys: 2.33 ms, total: 2.33 ms\n",
      "Wall time: 2.29 ms\n",
      "iter=12, loss=0.006162\n",
      "CPU times: user 167 µs, sys: 19 µs, total: 186 µs\n",
      "Wall time: 190 µs\n",
      "iter=13, loss=0.006164\n",
      "CPU times: user 336 µs, sys: 38 µs, total: 374 µs\n",
      "Wall time: 379 µs\n",
      "iter=14, loss=0.006167\n",
      "CPU times: user 163 µs, sys: 19 µs, total: 182 µs\n",
      "Wall time: 186 µs\n",
      "iter=15, loss=0.006162\n",
      "CPU times: user 212 µs, sys: 24 µs, total: 236 µs\n",
      "Wall time: 241 µs\n",
      "iter=16, loss=0.006163\n",
      "CPU times: user 427 µs, sys: 49 µs, total: 476 µs\n",
      "Wall time: 481 µs\n",
      "iter=17, loss=0.006163\n",
      "CPU times: user 221 µs, sys: 25 µs, total: 246 µs\n",
      "Wall time: 233 µs\n",
      "iter=18, loss=0.006164\n",
      "CPU times: user 455 µs, sys: 52 µs, total: 507 µs\n",
      "Wall time: 472 µs\n",
      "iter=19, loss=0.006162\n",
      "CPU times: user 237 µs, sys: 0 ns, total: 237 µs\n",
      "Wall time: 224 µs\n",
      "iter=20, loss=0.006164\n",
      "CPU times: user 150 µs, sys: 17 µs, total: 167 µs\n",
      "Wall time: 172 µs\n",
      "iter=21, loss=0.006162\n",
      "CPU times: user 1.13 ms, sys: 128 µs, total: 1.26 ms\n",
      "Wall time: 1.27 ms\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9e0b80f64b1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_pred\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mxb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mLOSS\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"print('iter=%d, loss=%.6f' % (i, LOSS / num_batches))\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/dev/lib/python3.6/site-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    105\u001b[0m                 \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(NUM_STEPS):\n",
    "    optimizer.zero_grad()\n",
    "    indices = torch.randperm(num_tjs)\n",
    "    LOSS = 0.0\n",
    "    for b in range(num_batches):\n",
    "        b_ind = indices[b*BATCH_SIZE:(b+1)*BATCH_SIZE]\n",
    "        xb = Xs[b_ind].cuda()\n",
    "        vb = Vs[b_ind].cuda()\n",
    "        x_pred = enc(xb[:,:-1, :], vb[:, :-1, :])\n",
    "        loss = ((x_pred - xb[:, 1:, :])**2).sum(-1).mean()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        LOSS += loss\n",
    "    %time print('iter=%d, loss=%.6f' % (i, LOSS / num_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xb = Xs[1*BATCH_SIZE:(b+1)*BATCH_SIZE].cuda()\n",
    "vb = Vs[1*BATCH_SIZE:(b+1)*BATCH_SIZE].cuda()\n",
    "x_pred = enc(xb[:,:-1, :], vb[:, :-1, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
