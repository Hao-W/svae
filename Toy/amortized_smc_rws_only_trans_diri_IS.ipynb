{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from gmm_dataset import *\n",
    "from util_hmm_variational_gibbs import *\n",
    "from util_plots import *\n",
    "from scipy.stats import invwishart, dirichlet\n",
    "from torch.distributions.dirichlet import Dirichlet\n",
    "sys.path.append('/home/hao/Research/probtorch/')\n",
    "from probtorch.util import expand_inputs\n",
    "import probtorch\n",
    "print('probtorch:', probtorch.__version__, \n",
    "      'torch:', torch.__version__, \n",
    "      'cuda:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 30\n",
    "K = 3\n",
    "D = 2\n",
    "num_particles_rws = 10\n",
    "mcmc_steps = 5\n",
    "num_particles_smc = 30\n",
    "# num_particles_enc = 50\n",
    "NUM_HIDDEN = 128\n",
    "NUM_LATENTS = K*K\n",
    "NUM_OBS = T * K\n",
    "# training parameters\n",
    "# NUM_SAMPLES = 1\n",
    "# BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "CUDA = False\n",
    "\n",
    "RESTORE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs, mu_true, cov_true, Zs_true, Pi_true, A_true = sampling_hmm(T, K, D)\n",
    "plot_samples(Xs.data.numpy(), mu_true.data.numpy(), cov_true.data.numpy())\n",
    "Y = Xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## return samples in order to compute the weights and \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_obs=NUM_OBS,\n",
    "                       num_hidden=NUM_HIDDEN,\n",
    "                       num_latents=NUM_LATENTS):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.enc_hidden = nn.Sequential(\n",
    "            nn.Linear(num_obs, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_hidden),\n",
    "            nn.ReLU())\n",
    "        self.latent_dir = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        \n",
    "    def forward(self, obs, num_particles=1):\n",
    "        A_samples = torch.zeros((K, K))\n",
    "        hidden = self.enc_hidden(obs)\n",
    "        latents_dirs = torch.exp(self.latent_dir(hidden).view(K, K))\n",
    "        for k in range(K):\n",
    "            A_samples[k] = Dirichlet(latents_dirs[k]).sample()\n",
    "        return latents_dirs, A_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    enc = Encoder()\n",
    "    if CUDA:\n",
    "        enc.cuda()\n",
    "    optimizer =  torch.optim.Adam(list(enc.parameters()),lr=LEARNING_RATE)    \n",
    "    return enc, optimizer\n",
    "\n",
    "alpha_init_0, alpha_trans_0, m_0, beta_0, nu_0, W_0 = pirors(Y, T, D, K)\n",
    "enc, optimizer = initialize()\n",
    "## initialization\n",
    "# cov_ks = torch.zeros((K, D, D))\n",
    "# mu_ks = torch.zeros((K, D))\n",
    "# for k in range(K):\n",
    "# ## sample mu_k and Sigma_k randomly\n",
    "#     cov_ks[k] = torch.from_numpy(invwishart.rvs(df=nu_0, scale=W_0.data.numpy())).float()\n",
    "#     mu_ks[k] = MultivariateNormal(loc=m_0, covariance_matrix=cov_ks[k] / beta_0).sample()\n",
    "# Pi = Dirichlet(alpha_init_0).sample()\n",
    "# A = initial_trans(alpha_trans_0, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KLs = []\n",
    "KL_ests = []\n",
    "log_p_conds = []\n",
    "log_qs = []\n",
    "ESSs = []\n",
    "# Zs = torch.from_numpy(Zs_true).float()\n",
    "# mu_ks = torch.from_numpy(mu_true).float()\n",
    "# cov_ks = torch.from_numpy(cov_true).float()\n",
    "# Pi = torch.from_numpy(Pi_true).float()\n",
    "Pi = Pi_true\n",
    "# A = A_true\n",
    "mu_ks = mu_true\n",
    "cov_ks = cov_true\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    time_start = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    log_weights_rws = torch.zeros(num_particles_rws)\n",
    "    log_qs = torch.zeros(num_particles_rws)\n",
    "    log_p_conds = torch.zeros(num_particles_rws)\n",
    "    kl = 0.0\n",
    "    for l in range(num_particles_rws):\n",
    "        # initialize A from prior\n",
    "        A_samples = initial_trans(alpha_trans_0, K)\n",
    "        # SMC to generate a weighted sample set for local states\n",
    "        Zs, log_weights, log_normalizer = smc_hmm(Pi, A_samples, mu_ks, cov_ks, Y, T, D, K, num_particles_smc)\n",
    "        # draw a sample from the sample set \n",
    "        Z_ret = resampling_smc(Zs, log_weights)\n",
    "        log_weight_rws = log_normalizer\n",
    "        for m in range(mcmc_steps):\n",
    "            A_prev = A_samples\n",
    "            # pair (Zs_max, normalizer) is p.w. w.r.t. q_smc\n",
    "            latents_dirs, A_samples = enc(Z_ret.contiguous().view(-1, T*K), 1)\n",
    "            print('=====epoch : %d, rws : %d, mcmc : %d =====' % (epoch, l, m))\n",
    "            print(A_samples)\n",
    "            log_p_joint_curr = log_joint(alpha_init_0, alpha_trans_0, nu_0, W_0, m_0, beta_0, Z_ret, Pi, A_samples, mu_ks, cov_ks, Y, T, D, K).detach().item()\n",
    "            log_p_joint_prev = log_joint(alpha_init_0, alpha_trans_0, nu_0, W_0, m_0, beta_0, Z_ret, Pi, A_prev, mu_ks, cov_ks, Y, T, D, K).detach().item() \n",
    "            \n",
    "            log_q_curr = log_q_hmm(latents_dirs, A_samples, K, num_particles=1).detach()\n",
    "            log_q_prev = log_q_hmm(latents_dirs, A_prev, K,num_particles=1).detach()\n",
    "            \n",
    "            Zs, log_weights, log_normalizer = csmc_hmm(Z_ret, Pi, A_samples, mu_ks, cov_ks, Y, T, D, K, num_particles_smc)\n",
    "            Z_ret = resampling_smc(Zs, log_weights)\n",
    "            \n",
    "            log_weight_rws += log_p_joint_curr - log_p_joint_prev - log_q_curr + log_q_prev\n",
    "              \n",
    "        log_weights_rws[l] = log_weight_rws.detach()     \n",
    "        \n",
    "        \n",
    "        log_p_cond = torch.zeros(K)\n",
    "        alpha_trans_hat = alpha_trans_0 + pairwise(Z_ret, T).sum(0)\n",
    "        kl += kl_dirichlets(alpha_trans_0, latents_dirs, Z_ret, T, K)\n",
    "#         for k in range(K):\n",
    "#             log_p_cond[k] = Dirichlet(alpha_trans_hat[k]).log_prob(A_samples[k])\n",
    "\n",
    "#         log_p_conds[l] = log_p_cond.sum(0)        \n",
    "        log_qs[l] = log_q_hmm(latents_dirs, A_samples, K, num_particles=1)\n",
    "#     print('rws : %d, log_q : %f' % (l, log_qs[l]))    \n",
    "    kl /= num_particles_rws\n",
    "        \n",
    "    log_weights_rws = log_weights_rws - log_sum_exp(log_weights_rws)\n",
    "    weights_rws = torch.exp(log_weights_rws)\n",
    "    ess = (1. / (weights_rws ** 2 ).sum()).item()\n",
    "    loss_infer = - torch.mul(weights_rws, log_qs).sum()\n",
    "#     kl_est = torch.mul(weights_rws, log_p_conds - log_qs).sum().detach().item()\n",
    "    loss_infer.backward()\n",
    "    KLs.append(kl.item())\n",
    "#     KL_ests.append(kl_est)\n",
    "    ESSs.append(ess)\n",
    "    optimizer.step()\n",
    "    time_end = time.time()\n",
    "    print('epoch : %d, KL : %f (%ds)' % (epoch, kl,time_end - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_weights_rws\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_kl_est(np.array(KLs), np.array(KL_ests))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_dicichlet_post = latents_dirs\n",
    "true_dirichlet_post = alpha_trans_0 + pairwise(Z_ret, T).sum(0)\n",
    "print('variational : ')\n",
    "print(learned_dicichlet_post)\n",
    "print('conjugate posterior :')\n",
    "print(true_dirichlet_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(ESSs) / num_particles_rws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions.categorical import Categorical\n",
    "Categorical(torch.Tensor([1/2, 1/2])).sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ESSs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
