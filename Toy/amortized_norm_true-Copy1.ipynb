{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "probtorch: 0.0+5a2c637 torch: 0.5.0a0+3bb8c5e cuda: True\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from gmm_dataset import *\n",
    "from util_data import *\n",
    "from util_hmm_variational_gibbs import *\n",
    "from util_plots import *\n",
    "from scipy.stats import invwishart, dirichlet\n",
    "from torch.distributions.dirichlet import Dirichlet\n",
    "sys.path.append('/home/hao/Research/probtorch/')\n",
    "from probtorch.util import expand_inputs\n",
    "import probtorch\n",
    "print('probtorch:', probtorch.__version__, \n",
    "      'torch:', torch.__version__, \n",
    "      'cuda:', torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 100\n",
    "K = 4\n",
    "D = 2\n",
    "num_particles_rws = 50\n",
    "mcmc_steps = 2\n",
    "num_particles_smc = 30\n",
    "NUM_HIDDEN = 128\n",
    "NUM_LATENTS = K*K\n",
    "NUM_OBS = 2 * K\n",
    "# training parameters\n",
    "NUM_SAMPLES = 1\n",
    "# BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 80\n",
    "LEARNING_RATE = 1e-3\n",
    "CUDA = False\n",
    "\n",
    "num_series = 1\n",
    "dt = 1 ## \\delta t\n",
    "Boundary = 50 ## unit box is amplified by this value, centered at origin\n",
    "signal_noise_ratio = 0.15 # the noise is multiplied by this param\n",
    "training_epochs = 50 \n",
    "D = 2\n",
    "## Model parameters\n",
    "K =  4## number of hidden states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## generate time series\n",
    "x0, y0, init_v, init_v_rand_dir = intialization(T, num_series, Boundary)\n",
    "mu_ks = np.zeros((num_series, K, D))\n",
    "final_covs = np.zeros((num_series, K, D, D))\n",
    "STATEs = np.zeros((num_series, T+1, 4))\n",
    "Disps = np.zeros((num_series, T, D))\n",
    "As_true = np.zeros((num_series, 4, 4))\n",
    "\n",
    "for s in range(num_series):\n",
    "    init_state = np.array([x0[s], y0[s], init_v_rand_dir[s, 0], init_v_rand_dir[s, 1]])\n",
    "    STATEs[s], Disps[s], As_true[s] = generate_data(T, dt, init_state, Boundary, signal_noise_ratio)\n",
    "\n",
    "Y = Disps[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-7.82429537, -6.22739126]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plt.plot(Y[:,0], Y[:,1], 'ro')\n",
    "init_v_rand_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## return samples in order to compute the weights and \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, num_obs=NUM_OBS,\n",
    "                       num_hidden=NUM_HIDDEN,\n",
    "                       num_latents=NUM_LATENTS):\n",
    "        super(self.__class__, self).__init__()\n",
    "        self.enc_hidden = nn.Sequential(\n",
    "            nn.Linear(num_obs, num_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden, num_hidden),\n",
    "            nn.ReLU())\n",
    "        self.latent_dir = nn.Sequential(\n",
    "            nn.Linear(num_hidden, num_latents))\n",
    "        \n",
    "    def forward(self, obs, num_particles):\n",
    "        A_samples = torch.zeros((K, K))\n",
    "        hidden = self.enc_hidden(obs)\n",
    "        latents_dirs = torch.exp(self.latent_dir(hidden)).sum(0).view(K, K)\n",
    "        latents_dirs_norm = latents_dirs / latents_dirs.sum() * (T-1)\n",
    "        for k in range(K):\n",
    "            A_samples[k] = Dirichlet(latents_dirs_norm[k]).sample()\n",
    "        return latents_dirs_norm, A_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize():\n",
    "    enc = Encoder()\n",
    "    if CUDA:\n",
    "        enc.cuda()\n",
    "    optimizer =  torch.optim.Adam(list(enc.parameters()),lr=LEARNING_RATE)    \n",
    "    return enc, optimizer\n",
    "\n",
    "alpha_init_0, alpha_trans_0, m_0, beta_0, nu_0, W_0 = pirors(Y, T, D, K)\n",
    "enc, optimizer = initialize()\n",
    "## initialization\n",
    "# cov_ks = torch.zeros((K, D, D))\n",
    "# mu_ks = torch.zeros((K, D))\n",
    "# for k in range(K):\n",
    "# ## sample mu_k and Sigma_k randomly\n",
    "#     cov_ks[k] = torch.from_numpy(invwishart.rvs(df=nu_0, scale=W_0.data.numpy())).float()\n",
    "#     mu_ks[k] = MultivariateNormal(loc=m_0, covariance_matrix=cov_ks[k] / beta_0).sample()\n",
    "# Pi = Dirichlet(alpha_init_0).sample()\n",
    "# A_samples = initial_trans(alpha_trans_0, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KLs = []\n",
    "KL_ests = []\n",
    "log_p_conds = []\n",
    "log_qs = []\n",
    "ESSs = []\n",
    "\n",
    "Pi = Pi_true\n",
    "mu_ks = mu_true\n",
    "cov_ks = cov_true\n",
    "# Z_ret = Zs_true\n",
    "# Z_ret_pairwise = torch.cat((Z_ret[:T-1].unsqueeze(0), Z_ret[1:].unsqueeze(0)), 0).transpose(0, 1).contiguous().view(T-1, 2*K)\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    time_start = time.time()\n",
    "    optimizer.zero_grad()\n",
    "    log_weights_rws = torch.zeros(num_particles_rws)\n",
    "    log_qs = torch.zeros(num_particles_rws)\n",
    "    kl = 0.0\n",
    "    for l in range(num_particles_rws):\n",
    "#         A_samples = initial_trans(alpha_trans_0, K)\n",
    "        A_samples = A_true\n",
    "        Zs, log_weights, log_normalizer = smc_hmm(Pi, A_samples, mu_ks, cov_ks, Y, T, D, K, num_particles_smc)\n",
    "        Z_ret = resampling_smc(Zs, log_weights)\n",
    "        log_weight_rws = log_normalizer\n",
    "        Z_ret_pairwise = torch.cat((Z_ret[:T-1].unsqueeze(0), Z_ret[1:].unsqueeze(0)), 0).transpose(0, 1).contiguous().view(T-1, 2*K)\n",
    "        for m in range(mcmc_steps):\n",
    "            A_prev = A_samples\n",
    "            latents_dirs, A_samples = enc(Z_ret_pairwise, 1)\n",
    "            log_p_joint_curr = log_joint(alpha_init_0, alpha_trans_0, nu_0, W_0, m_0, beta_0, Z_ret, Pi, A_samples, mu_ks, cov_ks, Y, T, D, K).detach().item()\n",
    "#             log_p_joint_prev = log_joint(alpha_init_0, alpha_trans_0, nu_0, W_0, m_0, beta_0, Z_ret, Pi, A_prev, mu_ks, cov_ks, Y, T, D, K).detach().item() \n",
    "            log_p_smc = log_joint_smc(Z_ret, Pi, A_samples, mu_ks, cov_ks, Y, T, D, K)\n",
    "            log_q_curr = log_q_hmm(latents_dirs, A_samples, K, num_particles=1).detach()\n",
    "            log_q_prev = log_q_hmm(latents_dirs, A_prev, K,num_particles=1).detach()\n",
    "            \n",
    "            Zs, log_weights, log_normalizer = csmc_hmm(Z_ret, Pi, A_samples, mu_ks, cov_ks, Y, T, D, K, num_particles_smc)\n",
    "            Z_ret = resampling_smc(Zs, log_weights)\n",
    "            Z_ret_pairwise = torch.cat((Z_ret[:T-1].unsqueeze(0), Z_ret[1:].unsqueeze(0)), 0).transpose(0, 1).contiguous().view(T-1, 2*K)\n",
    "\n",
    "            log_weight_rws += log_p_joint_curr - log_p_smc - log_q_curr + log_q_prev + log_normalizer\n",
    "              \n",
    "        log_weights_rws[l] = log_weight_rws.detach()     \n",
    "\n",
    "  \n",
    "        alpha_trans_hat = alpha_trans_0 + pairwise(Z_ret, T).sum(0)\n",
    "        kl += kl_dirichlets(alpha_trans_0, latents_dirs, Z_ret, T, K)      \n",
    "        log_qs[l] = log_q_hmm(latents_dirs, A_samples, K, num_particles=1)   \n",
    "    kl /= num_particles_rws\n",
    "        \n",
    "    log_weights_rws = log_weights_rws - log_sum_exp(log_weights_rws)\n",
    "    weights_rws = torch.exp(log_weights_rws)\n",
    "    ess = (1. / (weights_rws ** 2 ).sum()).item()\n",
    "    loss_infer = - torch.mul(weights_rws, log_qs).sum()\n",
    "#     kl_est = torch.mul(weights_rws, log_p_conds - log_qs).sum().detach().item()\n",
    "    loss_infer.backward()\n",
    "    KLs.append(kl.item())\n",
    "#     KL_ests.append(kl_est)\n",
    "    ESSs.append(ess)\n",
    "    optimizer.step()\n",
    "#     A_samples = A_samples.detach()\n",
    "    time_end = time.time()\n",
    "    print('epoch : %d, KL : %f, loss_infer : %f (%ds)' % (epoch, kl, loss_infer, time_end - time_start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_kl_est(np.array(KLs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_trans_0 + pairwise(Zs_true, T).sum(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learned_dicichlet_post = latents_dirs\n",
    "true_dirichlet_post = alpha_trans_0 + pairwise(Zs_true, T).sum(0)\n",
    "print('variational : ')\n",
    "print(learned_dicichlet_post)\n",
    "print('conjugate posterior :')\n",
    "print(true_dirichlet_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.array(ESSs) / num_particles_rws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
